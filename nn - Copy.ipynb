{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"E:\\Projects\\mnist_train.csv\")\n",
    "x_train = df_train.drop(columns=['5'])\n",
    "y_train = df_train['5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(r\"E:\\Projects\\mnist_test.csv\")\n",
    "x_test = df_test.drop(columns=['7'])\n",
    "y_test = df_test['7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_digit(image, label=None):\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if label is not None:\n",
    "        plt.title(f'Label: {label}')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 59197 \n",
    "image = x_train.iloc[index].values.reshape(28, 28)\n",
    "visualize_digit(image, y_train.iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:60000]\n",
    "y_train = y_train[:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train)  # Convert to CuPy array\n",
    "y_train = np.asarray(y_train)  # Convert to CuPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_x_train = x_train\n",
    "original_y_train = y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Data augmentation\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import scipy.ndimage\n",
    "\n",
    "def augment_data(images):\n",
    "    augmented_images = []\n",
    "    for img in images:\n",
    "        # Convert CuPy array to NumPy and reshape to 28x28\n",
    "        img_np = img.get().reshape(28, 28)\n",
    "        \n",
    "        # Generate augmentation parameters as Python floats\n",
    "        angle = float(cp.random.uniform(-10, 10))\n",
    "        shift_x = float(cp.random.uniform(-2, 2))\n",
    "        shift_y = float(cp.random.uniform(-2, 2))\n",
    "        \n",
    "        # Apply rotation and shifting using SciPy (which works with NumPy arrays)\n",
    "        rotated = scipy.ndimage.rotate(img_np, angle, reshape=False, mode='constant', cval=0)\n",
    "        shifted = scipy.ndimage.shift(rotated, shift=[shift_x, shift_y], mode='constant', cval=0)\n",
    "        \n",
    "        # Convert back to CuPy array and add noise\n",
    "        augmented = cp.asarray(shifted) + cp.random.normal(0, 0.02, (28, 28))\n",
    "        augmented = cp.clip(augmented, 0, 1)\n",
    "        \n",
    "        # Flatten to original shape (e.g., 784)\n",
    "        augmented_images.append(augmented.flatten())\n",
    "    \n",
    "    return cp.array(augmented_images)\n",
    "\n",
    "# Example usage:\n",
    "# x_train_augmented = augment_data(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = augment_data(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Beginning of Neural Network</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Initial layer\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer_dense:\n",
    "    def __init__(self , n_inputs , n_neurons):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "\n",
    "        self.weights = np.random.randn(n_inputs , n_neurons)*0.01\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        #calculating the dot product basically X.W + b\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "#dvalues = dl_dz\n",
    "    def backward(self , dvalues):\n",
    "        self.dvalues = dvalues\n",
    "        \n",
    "        #gradient on weight and bias parameter\n",
    "        self.dweights = np.dot(self.inputs.T , dvalues)\n",
    "        self.dbiases = np.sum(dvalues , axis=0 , keepdims=True)\n",
    "\n",
    "        #gradient on input values\n",
    "        self.dinputs = np.dot(dvalues , self.weights.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "ReLU activation \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relu:\n",
    "    def forward(self , inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.maximum(0 , inputs)\n",
    "\n",
    "    def backward(self , dvalues):\n",
    "        #making the copies\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # zero gradient where input values are negative\n",
    "        self.dinputs[self.inputs <=0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Softmax activation\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation_softmax:\n",
    "    def forward(self , inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs , axis = 1 ,keepdims=True))\n",
    "        # keepdims = True for broadcasting\n",
    "        probabilities = exp_values/(np.sum(exp_values , axis=1 , keepdims=True))\n",
    "\n",
    "        self.outputs = probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Loss Class\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common loss class\n",
    "class loss:\n",
    "    #calculates the data and regularization losses , given the ground truth and predicted values\n",
    "    def calculate(self , outputs , y):\n",
    "        sample_loss = self.forward(outputs , y)\n",
    "        data_loss = np.mean(sample_loss)\n",
    "\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Categorical Cross Entropy Loss\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_categorical_cross_entropy(loss):\n",
    "\n",
    "    def forward(self , y_pred , y_true ):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        #clipping the data to prevent division by 0 \n",
    "        # and preventing giant values\n",
    "\n",
    "        y_pred_clipped = np.clip(y_pred , 1e-7 , 1-1e-7)\n",
    "\n",
    "        #probabilities for target values - \n",
    "        #only if categorical labels\n",
    "        #applying advanced indexing\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1 #making it to multiply just along the rows\n",
    "            )\n",
    "\n",
    "        negative_log_likelihood = -np.log(correct_confidences)\n",
    "        return negative_log_likelihood\n",
    "    \n",
    "    #backward pass\n",
    "    def backward(self , dvalues , y_true):\n",
    "\n",
    "        #number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        #number of labels in every sample will be calculated using the 1st sample\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        #if labels are sparse , turning them into one hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        #calculating the gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "combined softmax activation and categorical cross entropy for last layer : Forward and Backward pass\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax and cross entropy loss are brought together\n",
    "# in the single following class for faster backward step\n",
    "\n",
    "class activation_softmax_loss_categorical_cross_entropy:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = activation_softmax()\n",
    "        self.loss = loss_categorical_cross_entropy()\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self , inputs , y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.outputs = self.activation.outputs\n",
    "        return self.loss.calculate(self.outputs , y_true)\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self , dvalues , y_true):\n",
    "        samples = len(dvalues)\n",
    "        #if lables are one-hot encoded,\n",
    "        #turn them to discrete values\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true , axis = 1)\n",
    "\n",
    "        #copying in order to modify safely\n",
    "        self.dinputs = dvalues.copy()\n",
    "        #calculate gradients\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        #normalize the gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Batch Normalization\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, size, momentum=0.9, epsilon=1e-5):\n",
    "        self.gamma = cp.ones((1, size))\n",
    "        self.beta = cp.zeros((1, size))\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.running_mean = cp.zeros((1, size))\n",
    "        self.running_var = cp.ones((1, size))\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        self.X = X  # Store input for backpropagation\n",
    "        if training:\n",
    "            batch_mean = cp.mean(X, axis=0, keepdims=True)\n",
    "            batch_var = cp.var(X, axis=0, keepdims=True)\n",
    "            self.X_normalized = (X - batch_mean) / cp.sqrt(batch_var + self.epsilon)\n",
    "            self.outputs = self.gamma * self.X_normalized + self.beta\n",
    "\n",
    "            # Update running stats\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n",
    "        else:\n",
    "            self.X_normalized = (X - self.running_mean) / cp.sqrt(self.running_var + self.epsilon)\n",
    "            self.outputs = self.gamma * self.X_normalized + self.beta\n",
    "        \n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self, dinputs):\n",
    "        batch_size = self.X.shape[0]\n",
    "\n",
    "        dgamma = cp.sum(dinputs * self.X_normalized, axis=0, keepdims=True)\n",
    "        dbeta = cp.sum(dinputs, axis=0, keepdims=True)\n",
    "\n",
    "        dX_normalized = dinputs * self.gamma\n",
    "        dvar = cp.sum(dX_normalized * (self.X - self.running_mean) * -0.5 * cp.power(self.running_var + self.epsilon, -1.5), axis=0, keepdims=True)\n",
    "        dmean = cp.sum(dX_normalized * -1 / cp.sqrt(self.running_var + self.epsilon), axis=0, keepdims=True) + dvar * cp.mean(-2 * (self.X - self.running_mean), axis=0, keepdims=True)\n",
    "\n",
    "        self.dinputs = dX_normalized / cp.sqrt(self.running_var + self.epsilon) + dvar * 2 * (self.X - self.running_mean) / batch_size + dmean / batch_size\n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "using optimizer as adam\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta1=0.9, beta2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        if hasattr(layer, 'weights'):  # Standard Dense Layer\n",
    "            if not hasattr(layer, 'weight_cache'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.weight_cache = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "                layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update weights\n",
    "            layer.weight_momentums = self.beta1 * layer.weight_momentums + (1 - self.beta1) * layer.dweights\n",
    "            weight_momentums_corrected = layer.weight_momentums / (1 - self.beta1 ** (self.iterations + 1))\n",
    "            layer.weight_cache = self.beta2 * layer.weight_cache + (1 - self.beta2) * layer.dweights**2\n",
    "            weight_cache_corrected = layer.weight_cache / (1 - self.beta2 ** (self.iterations + 1))\n",
    "            layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "\n",
    "        # Update biases\n",
    "            layer.bias_momentums = self.beta1 * layer.bias_momentums + (1 - self.beta1) * layer.dbiases\n",
    "            bias_momentums_corrected = layer.bias_momentums / (1 - self.beta1 ** (self.iterations + 1))\n",
    "            layer.bias_cache = self.beta2 * layer.bias_cache + (1 - self.beta2) * layer.dbiases**2\n",
    "            bias_cache_corrected = layer.bias_cache / (1 - self.beta2 ** (self.iterations + 1))\n",
    "            layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "        elif hasattr(layer, 'gamma'):  # BatchNorm Layer\n",
    "            if not hasattr(layer, 'gamma_cache'):\n",
    "                layer.gamma_momentums = np.zeros_like(layer.gamma)\n",
    "                layer.gamma_cache = np.zeros_like(layer.gamma)\n",
    "                layer.beta_momentums = np.zeros_like(layer.beta)\n",
    "                layer.beta_cache = np.zeros_like(layer.beta)\n",
    "\n",
    "        # Update gamma\n",
    "            layer.gamma_momentums = self.beta1 * layer.gamma_momentums + (1 - self.beta1) * layer.dgamma\n",
    "            gamma_momentums_corrected = layer.gamma_momentums / (1 - self.beta1 ** (self.iterations + 1))\n",
    "            layer.gamma_cache = self.beta2 * layer.gamma_cache + (1 - self.beta2) * layer.dgamma**2\n",
    "            gamma_cache_corrected = layer.gamma_cache / (1 - self.beta2 ** (self.iterations + 1))\n",
    "            layer.gamma += -self.current_learning_rate * gamma_momentums_corrected / (np.sqrt(gamma_cache_corrected) + self.epsilon)\n",
    "\n",
    "        # Update beta\n",
    "            layer.beta_momentums = self.beta1 * layer.beta_momentums + (1 - self.beta1) * layer.dbeta\n",
    "            beta_momentums_corrected = layer.beta_momentums / (1 - self.beta1 ** (self.iterations + 1))\n",
    "            layer.beta_cache = self.beta2 * layer.beta_cache + (1 - self.beta2) * layer.dbeta**2\n",
    "            beta_cache_corrected = layer.beta_cache / (1 - self.beta2 ** (self.iterations + 1))\n",
    "            layer.beta += -self.current_learning_rate * beta_momentums_corrected / (np.sqrt(beta_cache_corrected) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Vanilla SGD\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=0.01, decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - \\\n",
    "                             self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - \\\n",
    "                           self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Droput method\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, rate=0.3):\n",
    "\n",
    "        # Dropout rate\n",
    "        self.rate = rate\n",
    "        \n",
    "        # Mask to track which neurons are kept/dropped\n",
    "        self.mask = None\n",
    "        \n",
    "        # Binary scale for maintaining expected output during inference\n",
    "        self.scale = 1 / (1 - rate)\n",
    "        \n",
    "    def forward(self, inputs, training=True):\n",
    "\n",
    "        # Store inputs for backward pass\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # During training, randomly drop neurons\n",
    "        if training:\n",
    "            # Create a mask of same shape as inputs\n",
    "            self.mask = np.random.rand(*inputs.shape) > self.rate\n",
    "            \n",
    "            # Scale and apply mask\n",
    "            self.outputs = inputs * self.mask * self.scale\n",
    "        else:\n",
    "            # During inference, no dropout\n",
    "            self.outputs = inputs\n",
    "        \n",
    "        return self.outputs\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # During backward pass, apply the same mask\n",
    "        self.dinputs = dvalues * self.mask * self.scale\n",
    "        \n",
    "        return self.dinputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Training the neural network\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.090, loss: 2.326, lr: 0.01\n",
      "epoch: 1, acc: 0.268, loss: 2.463, lr: 0.01\n",
      "epoch: 2, acc: 0.298, loss: 2.761, lr: 0.009999000099990002\n",
      "epoch: 3, acc: 0.330, loss: 2.047, lr: 0.009998000399920017\n",
      "epoch: 4, acc: 0.406, loss: 1.700, lr: 0.009997000899730081\n",
      "epoch: 5, acc: 0.494, loss: 1.488, lr: 0.009996001599360257\n",
      "epoch: 6, acc: 0.529, loss: 1.420, lr: 0.009995002498750625\n",
      "epoch: 7, acc: 0.553, loss: 1.354, lr: 0.009994003597841295\n",
      "epoch: 8, acc: 0.581, loss: 1.237, lr: 0.009993004896572401\n",
      "epoch: 9, acc: 0.612, loss: 1.143, lr: 0.009992006394884094\n",
      "epoch: 10, acc: 0.617, loss: 1.117, lr: 0.009991008092716556\n",
      "epoch: 11, acc: 0.640, loss: 1.059, lr: 0.009990009990009992\n",
      "epoch: 12, acc: 0.670, loss: 0.987, lr: 0.009989012086704625\n",
      "epoch: 13, acc: 0.682, loss: 0.946, lr: 0.00998801438274071\n",
      "epoch: 14, acc: 0.691, loss: 0.918, lr: 0.009987016878058523\n",
      "epoch: 15, acc: 0.704, loss: 0.880, lr: 0.009986019572598362\n",
      "epoch: 16, acc: 0.720, loss: 0.835, lr: 0.009985022466300548\n",
      "epoch: 17, acc: 0.732, loss: 0.802, lr: 0.009984025559105431\n",
      "epoch: 18, acc: 0.741, loss: 0.779, lr: 0.00998302885095338\n",
      "epoch: 19, acc: 0.745, loss: 0.759, lr: 0.009982032341784786\n",
      "epoch: 20, acc: 0.755, loss: 0.734, lr: 0.009981036031540073\n",
      "epoch: 21, acc: 0.767, loss: 0.704, lr: 0.00998003992015968\n",
      "epoch: 22, acc: 0.774, loss: 0.682, lr: 0.009979044007584073\n",
      "epoch: 23, acc: 0.780, loss: 0.663, lr: 0.009978048293753742\n",
      "epoch: 24, acc: 0.787, loss: 0.646, lr: 0.0099770527786092\n",
      "epoch: 25, acc: 0.791, loss: 0.632, lr: 0.009976057462090984\n",
      "epoch: 26, acc: 0.799, loss: 0.611, lr: 0.009975062344139652\n",
      "epoch: 27, acc: 0.803, loss: 0.594, lr: 0.009974067424695792\n",
      "epoch: 28, acc: 0.808, loss: 0.577, lr: 0.009973072703700011\n",
      "epoch: 29, acc: 0.814, loss: 0.565, lr: 0.00997207818109294\n",
      "epoch: 30, acc: 0.818, loss: 0.555, lr: 0.009971083856815237\n",
      "epoch: 31, acc: 0.820, loss: 0.549, lr: 0.009970089730807579\n",
      "epoch: 32, acc: 0.825, loss: 0.527, lr: 0.009969095803010666\n",
      "epoch: 33, acc: 0.829, loss: 0.520, lr: 0.00996810207336523\n",
      "epoch: 34, acc: 0.833, loss: 0.507, lr: 0.00996710854181202\n",
      "epoch: 35, acc: 0.835, loss: 0.502, lr: 0.009966115208291807\n",
      "epoch: 36, acc: 0.841, loss: 0.488, lr: 0.009965122072745391\n",
      "epoch: 37, acc: 0.842, loss: 0.481, lr: 0.00996412913511359\n",
      "epoch: 38, acc: 0.846, loss: 0.471, lr: 0.009963136395337252\n",
      "epoch: 39, acc: 0.850, loss: 0.462, lr: 0.009962143853357242\n",
      "epoch: 40, acc: 0.852, loss: 0.452, lr: 0.009961151509114453\n",
      "epoch: 41, acc: 0.854, loss: 0.446, lr: 0.0099601593625498\n",
      "epoch: 42, acc: 0.857, loss: 0.440, lr: 0.009959167413604223\n",
      "epoch: 43, acc: 0.860, loss: 0.429, lr: 0.00995817566221868\n",
      "epoch: 44, acc: 0.862, loss: 0.422, lr: 0.009957184108334164\n",
      "epoch: 45, acc: 0.864, loss: 0.417, lr: 0.009956192751891677\n",
      "epoch: 46, acc: 0.867, loss: 0.408, lr: 0.009955201592832256\n",
      "epoch: 47, acc: 0.866, loss: 0.403, lr: 0.009954210631096956\n",
      "epoch: 48, acc: 0.871, loss: 0.393, lr: 0.009953219866626855\n",
      "epoch: 49, acc: 0.873, loss: 0.388, lr: 0.009952229299363059\n",
      "epoch: 50, acc: 0.876, loss: 0.382, lr: 0.009951238929246693\n",
      "epoch: 51, acc: 0.878, loss: 0.376, lr: 0.009950248756218907\n",
      "epoch: 52, acc: 0.880, loss: 0.368, lr: 0.009949258780220873\n",
      "epoch: 53, acc: 0.883, loss: 0.363, lr: 0.00994826900119379\n",
      "epoch: 54, acc: 0.884, loss: 0.357, lr: 0.009947279419078883\n",
      "epoch: 55, acc: 0.887, loss: 0.351, lr: 0.009946290033817386\n",
      "epoch: 56, acc: 0.888, loss: 0.346, lr: 0.009945300845350571\n",
      "epoch: 57, acc: 0.890, loss: 0.342, lr: 0.00994431185361973\n",
      "epoch: 58, acc: 0.891, loss: 0.335, lr: 0.009943323058566173\n",
      "epoch: 59, acc: 0.893, loss: 0.329, lr: 0.009942334460131238\n",
      "epoch: 60, acc: 0.893, loss: 0.324, lr: 0.009941346058256287\n",
      "epoch: 61, acc: 0.894, loss: 0.323, lr: 0.009940357852882704\n",
      "epoch: 62, acc: 0.898, loss: 0.315, lr: 0.009939369843951894\n",
      "epoch: 63, acc: 0.899, loss: 0.310, lr: 0.009938382031405287\n",
      "epoch: 64, acc: 0.900, loss: 0.306, lr: 0.009937394415184339\n",
      "epoch: 65, acc: 0.903, loss: 0.301, lr: 0.009936406995230525\n",
      "epoch: 66, acc: 0.902, loss: 0.298, lr: 0.009935419771485345\n",
      "epoch: 67, acc: 0.904, loss: 0.291, lr: 0.009934432743890324\n",
      "epoch: 68, acc: 0.907, loss: 0.287, lr: 0.009933445912387009\n",
      "epoch: 69, acc: 0.906, loss: 0.285, lr: 0.009932459276916964\n",
      "epoch: 70, acc: 0.908, loss: 0.280, lr: 0.009931472837421792\n",
      "epoch: 71, acc: 0.911, loss: 0.277, lr: 0.0099304865938431\n",
      "epoch: 72, acc: 0.913, loss: 0.268, lr: 0.00992950054612253\n",
      "epoch: 73, acc: 0.913, loss: 0.266, lr: 0.009928514694201748\n",
      "epoch: 74, acc: 0.914, loss: 0.263, lr: 0.009927529038022435\n",
      "epoch: 75, acc: 0.915, loss: 0.259, lr: 0.009926543577526304\n",
      "epoch: 76, acc: 0.917, loss: 0.256, lr: 0.009925558312655087\n",
      "epoch: 77, acc: 0.917, loss: 0.254, lr: 0.009924573243350535\n",
      "epoch: 78, acc: 0.917, loss: 0.249, lr: 0.009923588369554431\n",
      "epoch: 79, acc: 0.920, loss: 0.246, lr: 0.009922603691208573\n",
      "epoch: 80, acc: 0.921, loss: 0.242, lr: 0.009921619208254788\n",
      "epoch: 81, acc: 0.923, loss: 0.238, lr: 0.009920634920634922\n",
      "epoch: 82, acc: 0.923, loss: 0.238, lr: 0.009919650828290844\n",
      "epoch: 83, acc: 0.924, loss: 0.233, lr: 0.009918666931164452\n",
      "epoch: 84, acc: 0.925, loss: 0.227, lr: 0.009917683229197661\n",
      "epoch: 85, acc: 0.927, loss: 0.223, lr: 0.009916699722332408\n",
      "epoch: 86, acc: 0.927, loss: 0.221, lr: 0.00991571641051066\n",
      "epoch: 87, acc: 0.928, loss: 0.218, lr: 0.009914733293674401\n",
      "epoch: 88, acc: 0.930, loss: 0.214, lr: 0.00991375037176564\n",
      "epoch: 89, acc: 0.932, loss: 0.210, lr: 0.009912767644726409\n",
      "epoch: 90, acc: 0.931, loss: 0.207, lr: 0.009911785112498763\n",
      "epoch: 91, acc: 0.932, loss: 0.206, lr: 0.009910802775024779\n",
      "epoch: 92, acc: 0.935, loss: 0.198, lr: 0.009909820632246555\n",
      "epoch: 93, acc: 0.936, loss: 0.198, lr: 0.009908838684106221\n",
      "epoch: 94, acc: 0.935, loss: 0.196, lr: 0.009907856930545923\n",
      "epoch: 95, acc: 0.937, loss: 0.193, lr: 0.009906875371507827\n",
      "epoch: 96, acc: 0.939, loss: 0.187, lr: 0.009905894006934125\n",
      "epoch: 97, acc: 0.938, loss: 0.187, lr: 0.009904912836767036\n",
      "epoch: 98, acc: 0.940, loss: 0.183, lr: 0.009903931860948796\n",
      "epoch: 99, acc: 0.942, loss: 0.180, lr: 0.009902951079421667\n"
     ]
    }
   ],
   "source": [
    "# Creating the dense layer with 28*28 inputs\n",
    "dense1 = layer_dense(784, 784)\n",
    "batchnorm1 = BatchNormalization(784)  # BatchNorm after first dense\n",
    "activation1 = relu()\n",
    "dropout1 = Dropout(rate=0.3)\n",
    "\n",
    "dense2 = layer_dense(784, 784)\n",
    "batchnorm2 = BatchNormalization(784)  # BatchNorm after second dense\n",
    "activation2 = relu()\n",
    "dropout2 = Dropout(rate=0.3)\n",
    "\n",
    "dense3 = layer_dense(784, 10)\n",
    "\n",
    "# Softmax loss activation\n",
    "loss_activation = activation_softmax_loss_categorical_cross_entropy()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.001, decay=1e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    # Forward pass\n",
    "    dense1.forward(x_train)\n",
    "    batchnorm1.forward(dense1.outputs, training=True)  # Apply Batch Norm\n",
    "    activation1.forward(batchnorm1.outputs)\n",
    "    dropout1.forward(activation1.outputs, training=True)\n",
    "\n",
    "    dense2.forward(dropout1.outputs)\n",
    "    batchnorm2.forward(dense2.outputs, training=True)  # Apply Batch Norm\n",
    "    activation2.forward(batchnorm2.outputs)\n",
    "    dropout2.forward(activation2.outputs, training=True)\n",
    "\n",
    "    dense3.forward(dropout2.outputs)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_activation.forward(dense3.outputs, y_train)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    predictions = np.argmax(loss_activation.outputs, axis=1)\n",
    "    if len(y_train.shape) == 2:\n",
    "        y_train = np.argmax(y_train, axis=1)\n",
    "    accuracy = np.mean(predictions == y_train)\n",
    "\n",
    "    print(f'epoch: {epoch}, ' +\n",
    "          f'acc: {accuracy:.3f}, ' +\n",
    "          f'loss: {loss:.3f}, ' +\n",
    "          f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.outputs, y_train)\n",
    "    dense3.backward(loss_activation.dinputs)\n",
    "    dropout2.backward(dense3.dinputs)\n",
    "    activation2.backward(dropout2.dinputs)\n",
    "    batchnorm2.backward(activation2.dinputs)  # Backpropagate through BatchNorm\n",
    "    dense2.backward(batchnorm2.dinputs)\n",
    "\n",
    "    dropout1.backward(dense2.dinputs)\n",
    "    activation1.backward(dropout1.dinputs)\n",
    "    batchnorm1.backward(activation1.dinputs)  # Backpropagate through BatchNorm\n",
    "    dense1.backward(batchnorm1.dinputs)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "\n",
    "    optimizer.update_params(dense2)\n",
    "\n",
    "    optimizer.update_params(dense3)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    " Metric analysis\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 54.13%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 978    0    0    0    0    0    0    0    2    0]\n",
      " [   1 1027    0    3    0    0    0   11   93    0]\n",
      " [ 767    2   16    7    0    0    0   37  200    3]\n",
      " [  52    5    0  803    0    0    0   28   97   25]\n",
      " [ 221   38    0    0    1    0   17    1   30  674]\n",
      " [ 306    0    0   90    0    7    0    3  463   23]\n",
      " [ 759    5    0    0    0    0   48    0  145    1]\n",
      " [  89   15    0    2    0    0    0  895    6   20]\n",
      " [ 105   21    0    0    0    0    1    9  817   21]\n",
      " [ 139   26    0    3    0    0    0    9   12  820]]\n",
      "\n",
      "Per-class Accuracy:\n",
      "Digit 0: 99.80%\n",
      "Digit 1: 90.48%\n",
      "Digit 2: 1.55%\n",
      "Digit 3: 79.50%\n",
      "Digit 4: 0.10%\n",
      "Digit 5: 0.78%\n",
      "Digit 6: 5.01%\n",
      "Digit 7: 87.15%\n",
      "Digit 8: 83.88%\n",
      "Digit 9: 81.27%\n",
      "\n",
      "Total Misclassified Samples: 4587\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data (ensure it's in the right format)\n",
    "x_test = np.asarray(x_test)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "# Perform forward pass through the network\n",
    "def predict(X):\n",
    "    # Forward pass through the first layer and ReLU\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.outputs)\n",
    "    dropout1.forward(activation1.outputs, training=False)  # Inference mode\n",
    "\n",
    "    # Second layer and ReLU\n",
    "    dense2.forward(dropout1.outputs)\n",
    "    activation2.forward(dense2.outputs)\n",
    "    dropout2.forward(activation2.outputs, training=False)  # Inference mode\n",
    "\n",
    "    # Final layer with Softmax\n",
    "    dense3.forward(dropout2.outputs)\n",
    "    \n",
    "    # Get predictions (class with highest probability)\n",
    "    return np.argmax(dense3.outputs, axis=1)\n",
    "\n",
    "# Perform prediction\n",
    "predictions = predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Optional: Detailed classification report\n",
    "from collections import Counter\n",
    "\n",
    "# Confusion matrix\n",
    "def confusion_matrix(true, pred):\n",
    "    conf_matrix = np.zeros((10, 10), dtype=int)\n",
    "    for t, p in zip(true, pred):\n",
    "        conf_matrix[t, p] += 1\n",
    "    return conf_matrix\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_mat)\n",
    "\n",
    "# Per-class accuracy\n",
    "per_class_accuracy = conf_mat.diagonal() / conf_mat.sum(axis=1)\n",
    "print(\"\\nPer-class Accuracy:\")\n",
    "for digit, acc in enumerate(per_class_accuracy):\n",
    "    print(f\"Digit {digit}: {acc * 100:.2f}%\")\n",
    "\n",
    "# Misclassification analysis\n",
    "misclassified_indices = np.where(predictions != y_test)[0]\n",
    "print(f\"\\nTotal Misclassified Samples: {len(misclassified_indices)}\")\n",
    "\n",
    "# Optional: Visualize some misclassified samples\n",
    "def visualize_misclassifications(x_test, y_test, predictions, num_to_show=5):\n",
    "    misclassified_indices = np.where(predictions != y_test)[0]\n",
    "    \n",
    "    # Randomly select some misclassified samples\n",
    "    show_indices = np.random.choice(misclassified_indices, \n",
    "                                    min(num_to_show, len(misclassified_indices)), \n",
    "                                    replace=False)\n",
    "    \n",
    "    for idx in show_indices:\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')\n",
    "        plt.title(f\"True: {y_test[idx]}, Predicted: {predictions[idx]}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Uncomment to visualize misclassifications\n",
    "# visualize_misclassifications(x_test, y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
