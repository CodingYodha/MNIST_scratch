{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_digit(image, label=None):\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if label is not None:\n",
    "        plt.title(f'Label: {label}')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 76\n",
    "image = x_train[index].reshape(28, 28)\n",
    "visualize_digit(image, y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train)  # Convert to CuPy array\n",
    "y_train = np.asarray(y_train)  # Convert to CuPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_x_train = x_train\n",
    "original_y_train = y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Data augmentation\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cupy as np\n",
    "# import scipy.ndimage\n",
    "\n",
    "# def augment_data(images):\n",
    "#     augmented_images = []\n",
    "#     for img in images:\n",
    "#         # Convert CuPy array to NumPy and reshape to 28x28\n",
    "#         img_np = img.get().reshape(28, 28)\n",
    "        \n",
    "#         # Generate augmentation parameters as Python floats\n",
    "#         angle = float(np.random.uniform(-10, 10))\n",
    "#         shift_x = float(np.random.uniform(-2, 2))\n",
    "#         shift_y = float(np.random.uniform(-2, 2))\n",
    "        \n",
    "#         # Apply rotation and shifting using SciPy (which works with NumPy arrays)\n",
    "#         rotated = scipy.ndimage.rotate(img_np, angle, reshape=False, mode='constant', cval=0)\n",
    "#         shifted = scipy.ndimage.shift(rotated, shift=[shift_x, shift_y], mode='constant', cval=0)\n",
    "        \n",
    "#         # Convert back to CuPy array and add noise\n",
    "#         augmented = np.asarray(shifted) + np.random.normal(0, 0.02, (28, 28))\n",
    "#         augmented = np.clip(augmented, 0, 1)\n",
    "        \n",
    "#         # Flatten to original shape (e.g., 784)\n",
    "#         augmented_images.append(augmented.flatten())\n",
    "    \n",
    "#     return np.array(augmented_images)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Data manipulation\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For NumPy/CuPy arrays, use slicing instead of .iloc\n",
    "# x_train = x_train[:60000]  # First 5,000 entries\n",
    "# y_train = y_train[:60000]\n",
    "\n",
    "# # If you want a balanced subset, here's a CuPy/NumPy compatible version:\n",
    "# def get_balanced_subset(x, y, samples_per_class=500):\n",
    "#     balanced_x = []\n",
    "#     balanced_y = []\n",
    "#     for digit in range(10):\n",
    "#         # Find indices of current digit\n",
    "#         digit_indices = np.where(y == digit)[0]\n",
    "        \n",
    "#         # Select first 'samples_per_class' indices for this digit\n",
    "#         selected_indices = digit_indices[:samples_per_class]\n",
    "        \n",
    "#         # Append selected samples\n",
    "#         balanced_x.append(x[selected_indices])\n",
    "#         balanced_y.append(y[selected_indices])\n",
    "    \n",
    "#     # Concatenate along the first axis\n",
    "#     balanced_x = np.concatenate(balanced_x, axis=0)\n",
    "#     balanced_y = np.concatenate(balanced_y, axis=0)\n",
    "    \n",
    "#     return balanced_x, balanced_y\n",
    "\n",
    "# # Alternative method to get balanced subset\n",
    "# x_train, y_train = get_balanced_subset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Beginning of Neural Network</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Initial layer\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer_dense:\n",
    "    def __init__(self , n_inputs , n_neurons):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "\n",
    "        self.weights = np.random.randn(n_inputs , n_neurons)*0.01\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        #calculating the dot product basically X.W + b\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "#dvalues = dl_dz\n",
    "    def backward(self , dvalues):\n",
    "        self.dvalues = dvalues\n",
    "        \n",
    "        #gradient on weight and bias parameter\n",
    "        self.dweights = np.dot(self.inputs.T , dvalues)\n",
    "        self.dbiases = np.sum(dvalues , axis=0 , keepdims=True)\n",
    "\n",
    "        #gradient on input values\n",
    "        self.dinputs = np.dot(dvalues , self.weights.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "ReLU activation \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relu:\n",
    "    def forward(self , inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.maximum(0 , inputs)\n",
    "\n",
    "    def backward(self , dvalues):\n",
    "        #making the copies\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # zero gradient where input values are negative\n",
    "        self.dinputs[self.inputs <=0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Softmax activation\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation_softmax:\n",
    "    def forward(self , inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs , axis = 1 ,keepdims=True))\n",
    "        # keepdims = True for broadcasting\n",
    "        probabilities = exp_values/(np.sum(exp_values , axis=1 , keepdims=True))\n",
    "\n",
    "        self.outputs = probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Loss Class\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common loss class\n",
    "class loss:\n",
    "    #calculates the data and regularization losses , given the ground truth and predicted values\n",
    "    def calculate(self , outputs , y):\n",
    "        sample_loss = self.forward(outputs , y)\n",
    "        data_loss = np.mean(sample_loss)\n",
    "\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Categorical Cross Entropy Loss\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_categorical_cross_entropy(loss):\n",
    "\n",
    "    def forward(self , y_pred , y_true ):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        #clipping the data to prevent division by 0 \n",
    "        # and preventing giant values\n",
    "\n",
    "        y_pred_clipped = np.clip(y_pred , 1e-7 , 1-1e-7)\n",
    "\n",
    "        #probabilities for target values - \n",
    "        #only if categorical labels\n",
    "        #applying advanced indexing\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1 #making it to multiply just along the rows\n",
    "            )\n",
    "\n",
    "        negative_log_likelihood = -np.log(correct_confidences)\n",
    "        return negative_log_likelihood\n",
    "    \n",
    "    #backward pass\n",
    "    def backward(self , dvalues , y_true):\n",
    "\n",
    "        #number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        #number of labels in every sample will be calculated using the 1st sample\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        #if labels are sparse , turning them into one hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        #calculating the gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "combined softmax activation and categorical cross entropy for last layer : Forward and Backward pass\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax and cross entropy loss are brought together\n",
    "# in the single following class for faster backward step\n",
    "\n",
    "class activation_softmax_loss_categorical_cross_entropy:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = activation_softmax()\n",
    "        self.loss = loss_categorical_cross_entropy()\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self , inputs , y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.outputs = self.activation.outputs\n",
    "        return self.loss.calculate(self.outputs , y_true)\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self , dvalues , y_true):\n",
    "        samples = len(dvalues)\n",
    "        #if lables are one-hot encoded,\n",
    "        #turn them to discrete values\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true , axis = 1)\n",
    "\n",
    "        #copying in order to modify safely\n",
    "        self.dinputs = dvalues.copy()\n",
    "        #calculate gradients\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        #normalize the gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "using optimizer as adam\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays, create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Vanilla SGD\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=0.01, decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - \\\n",
    "                             self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - \\\n",
    "                           self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Droput method\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, rate=0.3):\n",
    "\n",
    "        # Dropout rate\n",
    "        self.rate = rate\n",
    "        \n",
    "        # Mask to track which neurons are kept/dropped\n",
    "        self.mask = None\n",
    "        \n",
    "        # Binary scale for maintaining expected output during inference\n",
    "        self.scale = 1 / (1 - rate)\n",
    "        \n",
    "    def forward(self, inputs, training=True):\n",
    "\n",
    "        # Store inputs for backward pass\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # During training, randomly drop neurons\n",
    "        if training:\n",
    "            # Create a mask of same shape as inputs\n",
    "            self.mask = np.random.rand(*inputs.shape) > self.rate\n",
    "            \n",
    "            # Scale and apply mask\n",
    "            self.outputs = inputs * self.mask * self.scale\n",
    "        else:\n",
    "            # During inference, no dropout\n",
    "            self.outputs = inputs\n",
    "        \n",
    "        return self.outputs\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # During backward pass, apply the same mask\n",
    "        self.dinputs = dvalues * self.mask * self.scale\n",
    "        \n",
    "        return self.dinputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Batch Normalization\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, size, momentum=0.9, epsilon=1e-5):\n",
    "        self.gamma = np.ones((1, size))\n",
    "        self.beta = np.zeros((1, size))\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.running_mean = np.zeros((1, size))\n",
    "        self.running_var = np.ones((1, size))\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        self.X = X  # Store input for backpropagation\n",
    "        if training:\n",
    "            batch_mean = np.mean(X, axis=0, keepdims=True)\n",
    "            batch_var = np.var(X, axis=0, keepdims=True)\n",
    "            self.X_normalized = (X - batch_mean) / np.sqrt(batch_var + self.epsilon)\n",
    "            self.outputs = self.gamma * self.X_normalized + self.beta\n",
    "\n",
    "            # Update running stats\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n",
    "        else:\n",
    "            self.X_normalized = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            self.outputs = self.gamma * self.X_normalized + self.beta\n",
    "        \n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self, dinputs):\n",
    "        batch_size = self.X.shape[0]\n",
    "\n",
    "        dgamma = np.sum(dinputs * self.X_normalized, axis=0, keepdims=True)\n",
    "        dbeta = np.sum(dinputs, axis=0, keepdims=True)\n",
    "\n",
    "        dX_normalized = dinputs * self.gamma\n",
    "        dvar = np.sum(dX_normalized * (self.X - self.running_mean) * -0.5 * np.power(self.running_var + self.epsilon, -1.5), axis=0, keepdims=True)\n",
    "        dmean = np.sum(dX_normalized * -1 / np.sqrt(self.running_var + self.epsilon), axis=0, keepdims=True) + dvar * np.mean(-2 * (self.X - self.running_mean), axis=0, keepdims=True)\n",
    "\n",
    "        self.dinputs = dX_normalized / np.sqrt(self.running_var + self.epsilon) + dvar * 2 * (self.X - self.running_mean) / batch_size + dmean / batch_size\n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Training the neural network\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.098, loss: 2.350, lr: 0.001\n",
      "epoch: 1, acc: 0.493, loss: 1.964, lr: 0.001\n",
      "epoch: 2, acc: 0.634, loss: 1.520, lr: 0.0009999900000999989\n",
      "epoch: 3, acc: 0.715, loss: 1.065, lr: 0.0009999800003999922\n",
      "epoch: 4, acc: 0.737, loss: 0.835, lr: 0.000999970000899973\n",
      "epoch: 5, acc: 0.774, loss: 0.690, lr: 0.000999960001599936\n",
      "epoch: 6, acc: 0.781, loss: 0.675, lr: 0.000999950002499875\n",
      "epoch: 7, acc: 0.824, loss: 0.579, lr: 0.000999940003599784\n",
      "epoch: 8, acc: 0.823, loss: 0.558, lr: 0.000999930004899657\n",
      "epoch: 9, acc: 0.847, loss: 0.507, lr: 0.000999920006399488\n",
      "epoch: 10, acc: 0.856, loss: 0.472, lr: 0.0009999100080992712\n",
      "epoch: 11, acc: 0.872, loss: 0.434, lr: 0.000999900009999\n",
      "epoch: 12, acc: 0.879, loss: 0.410, lr: 0.000999890012098669\n",
      "epoch: 13, acc: 0.883, loss: 0.391, lr: 0.0009998800143982724\n",
      "epoch: 14, acc: 0.891, loss: 0.362, lr: 0.0009998700168978034\n",
      "epoch: 15, acc: 0.898, loss: 0.345, lr: 0.0009998600195972563\n",
      "epoch: 16, acc: 0.902, loss: 0.335, lr: 0.0009998500224966255\n",
      "epoch: 17, acc: 0.904, loss: 0.322, lr: 0.0009998400255959046\n",
      "epoch: 18, acc: 0.910, loss: 0.307, lr: 0.0009998300288950879\n",
      "epoch: 19, acc: 0.914, loss: 0.292, lr: 0.000999820032394169\n",
      "epoch: 20, acc: 0.916, loss: 0.283, lr: 0.0009998100360931424\n",
      "epoch: 21, acc: 0.918, loss: 0.276, lr: 0.0009998000399920016\n",
      "epoch: 22, acc: 0.921, loss: 0.265, lr: 0.000999790044090741\n",
      "epoch: 23, acc: 0.924, loss: 0.254, lr: 0.0009997800483893542\n",
      "epoch: 24, acc: 0.927, loss: 0.245, lr: 0.000999770052887836\n",
      "epoch: 25, acc: 0.930, loss: 0.239, lr: 0.0009997600575861792\n",
      "epoch: 26, acc: 0.932, loss: 0.231, lr: 0.0009997500624843788\n",
      "epoch: 27, acc: 0.934, loss: 0.223, lr: 0.0009997400675824286\n",
      "epoch: 28, acc: 0.935, loss: 0.216, lr: 0.0009997300728803223\n",
      "epoch: 29, acc: 0.936, loss: 0.213, lr: 0.0009997200783780542\n",
      "epoch: 30, acc: 0.938, loss: 0.207, lr: 0.0009997100840756182\n",
      "epoch: 31, acc: 0.940, loss: 0.200, lr: 0.000999700089973008\n",
      "epoch: 32, acc: 0.942, loss: 0.194, lr: 0.0009996900960702183\n",
      "epoch: 33, acc: 0.943, loss: 0.190, lr: 0.0009996801023672425\n",
      "epoch: 34, acc: 0.944, loss: 0.186, lr: 0.000999670108864075\n",
      "epoch: 35, acc: 0.945, loss: 0.181, lr: 0.0009996601155607093\n",
      "epoch: 36, acc: 0.947, loss: 0.176, lr: 0.0009996501224571398\n",
      "epoch: 37, acc: 0.948, loss: 0.175, lr: 0.0009996401295533609\n",
      "epoch: 38, acc: 0.949, loss: 0.169, lr: 0.0009996301368493659\n",
      "epoch: 39, acc: 0.951, loss: 0.166, lr: 0.0009996201443451488\n",
      "epoch: 40, acc: 0.952, loss: 0.162, lr: 0.0009996101520407042\n",
      "epoch: 41, acc: 0.952, loss: 0.158, lr: 0.0009996001599360256\n",
      "epoch: 42, acc: 0.954, loss: 0.156, lr: 0.0009995901680311073\n",
      "epoch: 43, acc: 0.955, loss: 0.151, lr: 0.000999580176325943\n",
      "epoch: 44, acc: 0.956, loss: 0.148, lr: 0.0009995701848205273\n",
      "epoch: 45, acc: 0.956, loss: 0.146, lr: 0.0009995601935148535\n",
      "epoch: 46, acc: 0.957, loss: 0.142, lr: 0.0009995502024089159\n",
      "epoch: 47, acc: 0.958, loss: 0.140, lr: 0.000999540211502709\n",
      "epoch: 48, acc: 0.958, loss: 0.137, lr: 0.0009995302207962257\n",
      "epoch: 49, acc: 0.960, loss: 0.134, lr: 0.000999520230289461\n",
      "epoch: 50, acc: 0.960, loss: 0.132, lr: 0.0009995102399824086\n",
      "epoch: 51, acc: 0.962, loss: 0.128, lr: 0.0009995002498750624\n",
      "epoch: 52, acc: 0.963, loss: 0.125, lr: 0.0009994902599674165\n",
      "epoch: 53, acc: 0.963, loss: 0.123, lr: 0.000999480270259465\n",
      "epoch: 54, acc: 0.964, loss: 0.120, lr: 0.000999470280751202\n",
      "epoch: 55, acc: 0.964, loss: 0.118, lr: 0.000999460291442621\n",
      "epoch: 56, acc: 0.966, loss: 0.115, lr: 0.0009994503023337165\n",
      "epoch: 57, acc: 0.965, loss: 0.114, lr: 0.0009994403134244824\n",
      "epoch: 58, acc: 0.965, loss: 0.112, lr: 0.0009994303247149127\n",
      "epoch: 59, acc: 0.967, loss: 0.110, lr: 0.0009994203362050011\n",
      "epoch: 60, acc: 0.968, loss: 0.107, lr: 0.0009994103478947421\n",
      "epoch: 61, acc: 0.968, loss: 0.106, lr: 0.0009994003597841295\n",
      "epoch: 62, acc: 0.968, loss: 0.106, lr: 0.0009993903718731574\n",
      "epoch: 63, acc: 0.969, loss: 0.104, lr: 0.0009993803841618196\n",
      "epoch: 64, acc: 0.970, loss: 0.100, lr: 0.0009993703966501106\n",
      "epoch: 65, acc: 0.971, loss: 0.098, lr: 0.0009993604093380237\n",
      "epoch: 66, acc: 0.971, loss: 0.097, lr: 0.0009993504222255533\n",
      "epoch: 67, acc: 0.972, loss: 0.095, lr: 0.0009993404353126935\n",
      "epoch: 68, acc: 0.971, loss: 0.096, lr: 0.0009993304485994385\n",
      "epoch: 69, acc: 0.973, loss: 0.092, lr: 0.0009993204620857817\n",
      "epoch: 70, acc: 0.972, loss: 0.092, lr: 0.0009993104757717174\n",
      "epoch: 71, acc: 0.973, loss: 0.090, lr: 0.0009993004896572402\n",
      "epoch: 72, acc: 0.974, loss: 0.088, lr: 0.0009992905037423429\n",
      "epoch: 73, acc: 0.974, loss: 0.087, lr: 0.0009992805180270205\n",
      "epoch: 74, acc: 0.974, loss: 0.086, lr: 0.0009992705325112669\n",
      "epoch: 75, acc: 0.975, loss: 0.084, lr: 0.0009992605471950758\n",
      "epoch: 76, acc: 0.975, loss: 0.082, lr: 0.0009992505620784412\n",
      "epoch: 77, acc: 0.975, loss: 0.081, lr: 0.0009992405771613573\n",
      "epoch: 78, acc: 0.976, loss: 0.080, lr: 0.0009992305924438182\n",
      "epoch: 79, acc: 0.976, loss: 0.080, lr: 0.0009992206079258179\n",
      "epoch: 80, acc: 0.976, loss: 0.077, lr: 0.0009992106236073502\n",
      "epoch: 81, acc: 0.977, loss: 0.077, lr: 0.0009992006394884093\n",
      "epoch: 82, acc: 0.977, loss: 0.076, lr: 0.0009991906555689891\n",
      "epoch: 83, acc: 0.977, loss: 0.075, lr: 0.0009991806718490836\n",
      "epoch: 84, acc: 0.978, loss: 0.072, lr: 0.000999170688328687\n",
      "epoch: 85, acc: 0.978, loss: 0.071, lr: 0.0009991607050077935\n",
      "epoch: 86, acc: 0.978, loss: 0.070, lr: 0.0009991507218863964\n",
      "epoch: 87, acc: 0.978, loss: 0.071, lr: 0.0009991407389644904\n",
      "epoch: 88, acc: 0.979, loss: 0.068, lr: 0.0009991307562420696\n",
      "epoch: 89, acc: 0.979, loss: 0.069, lr: 0.0009991207737191272\n",
      "epoch: 90, acc: 0.980, loss: 0.065, lr: 0.0009991107913956579\n",
      "epoch: 91, acc: 0.980, loss: 0.064, lr: 0.0009991008092716555\n",
      "epoch: 92, acc: 0.980, loss: 0.065, lr: 0.0009990908273471142\n",
      "epoch: 93, acc: 0.981, loss: 0.064, lr: 0.0009990808456220278\n",
      "epoch: 94, acc: 0.980, loss: 0.065, lr: 0.0009990708640963903\n",
      "epoch: 95, acc: 0.981, loss: 0.063, lr: 0.000999060882770196\n",
      "epoch: 96, acc: 0.982, loss: 0.059, lr: 0.0009990509016434388\n",
      "epoch: 97, acc: 0.981, loss: 0.060, lr: 0.0009990409207161126\n",
      "epoch: 98, acc: 0.981, loss: 0.060, lr: 0.0009990309399882115\n",
      "epoch: 99, acc: 0.982, loss: 0.059, lr: 0.0009990209594597295\n",
      "epoch: 100, acc: 0.982, loss: 0.057, lr: 0.0009990109791306607\n",
      "epoch: 101, acc: 0.982, loss: 0.056, lr: 0.0009990009990009992\n",
      "epoch: 102, acc: 0.983, loss: 0.056, lr: 0.0009989910190707385\n",
      "epoch: 103, acc: 0.983, loss: 0.056, lr: 0.0009989810393398733\n",
      "epoch: 104, acc: 0.983, loss: 0.055, lr: 0.0009989710598083974\n",
      "epoch: 105, acc: 0.984, loss: 0.054, lr: 0.0009989610804763047\n",
      "epoch: 106, acc: 0.984, loss: 0.054, lr: 0.0009989511013435892\n",
      "epoch: 107, acc: 0.983, loss: 0.053, lr: 0.000998941122410245\n",
      "epoch: 108, acc: 0.984, loss: 0.052, lr: 0.0009989311436762664\n",
      "epoch: 109, acc: 0.985, loss: 0.051, lr: 0.0009989211651416472\n",
      "epoch: 110, acc: 0.984, loss: 0.052, lr: 0.000998911186806381\n",
      "epoch: 111, acc: 0.985, loss: 0.049, lr: 0.0009989012086704624\n",
      "epoch: 112, acc: 0.985, loss: 0.049, lr: 0.0009988912307338854\n",
      "epoch: 113, acc: 0.985, loss: 0.048, lr: 0.0009988812529966439\n",
      "epoch: 114, acc: 0.985, loss: 0.046, lr: 0.0009988712754587315\n",
      "epoch: 115, acc: 0.985, loss: 0.047, lr: 0.0009988612981201431\n",
      "epoch: 116, acc: 0.985, loss: 0.047, lr: 0.0009988513209808721\n",
      "epoch: 117, acc: 0.985, loss: 0.047, lr: 0.0009988413440409126\n",
      "epoch: 118, acc: 0.986, loss: 0.046, lr: 0.0009988313673002587\n",
      "epoch: 119, acc: 0.985, loss: 0.046, lr: 0.0009988213907589046\n",
      "epoch: 120, acc: 0.986, loss: 0.044, lr: 0.000998811414416844\n",
      "epoch: 121, acc: 0.986, loss: 0.044, lr: 0.000998801438274071\n",
      "epoch: 122, acc: 0.987, loss: 0.043, lr: 0.00099879146233058\n",
      "epoch: 123, acc: 0.987, loss: 0.043, lr: 0.0009987814865863648\n",
      "epoch: 124, acc: 0.986, loss: 0.043, lr: 0.0009987715110414191\n",
      "epoch: 125, acc: 0.987, loss: 0.041, lr: 0.0009987615356957373\n",
      "epoch: 126, acc: 0.986, loss: 0.042, lr: 0.0009987515605493133\n",
      "epoch: 127, acc: 0.988, loss: 0.040, lr: 0.0009987415856021413\n",
      "epoch: 128, acc: 0.988, loss: 0.039, lr: 0.000998731610854215\n",
      "epoch: 129, acc: 0.988, loss: 0.038, lr: 0.000998721636305529\n",
      "epoch: 130, acc: 0.988, loss: 0.040, lr: 0.0009987116619560768\n",
      "epoch: 131, acc: 0.988, loss: 0.039, lr: 0.0009987016878058523\n",
      "epoch: 132, acc: 0.988, loss: 0.038, lr: 0.0009986917138548504\n",
      "epoch: 133, acc: 0.988, loss: 0.038, lr: 0.000998681740103064\n",
      "epoch: 134, acc: 0.989, loss: 0.037, lr: 0.0009986717665504878\n",
      "epoch: 135, acc: 0.989, loss: 0.036, lr: 0.000998661793197116\n",
      "epoch: 136, acc: 0.989, loss: 0.036, lr: 0.0009986518200429421\n",
      "epoch: 137, acc: 0.989, loss: 0.036, lr: 0.0009986418470879603\n",
      "epoch: 138, acc: 0.989, loss: 0.035, lr: 0.0009986318743321649\n",
      "epoch: 139, acc: 0.989, loss: 0.035, lr: 0.0009986219017755497\n",
      "epoch: 140, acc: 0.989, loss: 0.035, lr: 0.0009986119294181088\n",
      "epoch: 141, acc: 0.990, loss: 0.033, lr: 0.000998601957259836\n",
      "epoch: 142, acc: 0.989, loss: 0.034, lr: 0.000998591985300726\n",
      "epoch: 143, acc: 0.990, loss: 0.033, lr: 0.0009985820135407723\n",
      "epoch: 144, acc: 0.990, loss: 0.033, lr: 0.0009985720419799686\n",
      "epoch: 145, acc: 0.990, loss: 0.033, lr: 0.0009985620706183095\n",
      "epoch: 146, acc: 0.991, loss: 0.031, lr: 0.000998552099455789\n",
      "epoch: 147, acc: 0.990, loss: 0.032, lr: 0.000998542128492401\n",
      "epoch: 148, acc: 0.990, loss: 0.032, lr: 0.0009985321577281395\n",
      "epoch: 149, acc: 0.990, loss: 0.032, lr: 0.0009985221871629988\n",
      "epoch: 150, acc: 0.991, loss: 0.030, lr: 0.0009985122167969725\n",
      "epoch: 151, acc: 0.990, loss: 0.030, lr: 0.0009985022466300548\n",
      "epoch: 152, acc: 0.991, loss: 0.030, lr: 0.0009984922766622401\n",
      "epoch: 153, acc: 0.991, loss: 0.030, lr: 0.0009984823068935219\n",
      "epoch: 154, acc: 0.990, loss: 0.031, lr: 0.0009984723373238943\n",
      "epoch: 155, acc: 0.991, loss: 0.029, lr: 0.0009984623679533517\n",
      "epoch: 156, acc: 0.992, loss: 0.027, lr: 0.0009984523987818883\n",
      "epoch: 157, acc: 0.991, loss: 0.028, lr: 0.0009984424298094972\n",
      "epoch: 158, acc: 0.991, loss: 0.027, lr: 0.000998432461036173\n",
      "epoch: 159, acc: 0.991, loss: 0.027, lr: 0.0009984224924619103\n",
      "epoch: 160, acc: 0.991, loss: 0.027, lr: 0.0009984125240867022\n",
      "epoch: 161, acc: 0.992, loss: 0.026, lr: 0.000998402555910543\n",
      "epoch: 162, acc: 0.992, loss: 0.026, lr: 0.0009983925879334273\n",
      "epoch: 163, acc: 0.992, loss: 0.026, lr: 0.0009983826201553484\n",
      "epoch: 164, acc: 0.992, loss: 0.026, lr: 0.0009983726525763007\n",
      "epoch: 165, acc: 0.992, loss: 0.026, lr: 0.000998362685196278\n",
      "epoch: 166, acc: 0.992, loss: 0.025, lr: 0.000998352718015275\n",
      "epoch: 167, acc: 0.992, loss: 0.025, lr: 0.0009983427510332848\n",
      "epoch: 168, acc: 0.992, loss: 0.025, lr: 0.0009983327842503018\n",
      "epoch: 169, acc: 0.992, loss: 0.024, lr: 0.0009983228176663208\n",
      "epoch: 170, acc: 0.993, loss: 0.024, lr: 0.0009983128512813346\n",
      "epoch: 171, acc: 0.993, loss: 0.023, lr: 0.000998302885095338\n",
      "epoch: 172, acc: 0.993, loss: 0.024, lr: 0.0009982929191083246\n",
      "epoch: 173, acc: 0.993, loss: 0.023, lr: 0.000998282953320289\n",
      "epoch: 174, acc: 0.993, loss: 0.023, lr: 0.000998272987731225\n",
      "epoch: 175, acc: 0.993, loss: 0.023, lr: 0.0009982630223411264\n",
      "epoch: 176, acc: 0.993, loss: 0.023, lr: 0.0009982530571499876\n",
      "epoch: 177, acc: 0.993, loss: 0.023, lr: 0.0009982430921578022\n",
      "epoch: 178, acc: 0.993, loss: 0.023, lr: 0.0009982331273645647\n",
      "epoch: 179, acc: 0.993, loss: 0.022, lr: 0.000998223162770269\n",
      "epoch: 180, acc: 0.993, loss: 0.022, lr: 0.000998213198374909\n",
      "epoch: 181, acc: 0.993, loss: 0.021, lr: 0.0009982032341784787\n",
      "epoch: 182, acc: 0.993, loss: 0.022, lr: 0.0009981932701809723\n",
      "epoch: 183, acc: 0.993, loss: 0.022, lr: 0.0009981833063823842\n",
      "epoch: 184, acc: 0.994, loss: 0.021, lr: 0.0009981733427827076\n",
      "epoch: 185, acc: 0.993, loss: 0.021, lr: 0.0009981633793819372\n",
      "epoch: 186, acc: 0.994, loss: 0.021, lr: 0.0009981534161800669\n",
      "epoch: 187, acc: 0.994, loss: 0.021, lr: 0.0009981434531770906\n",
      "epoch: 188, acc: 0.994, loss: 0.021, lr: 0.0009981334903730024\n",
      "epoch: 189, acc: 0.994, loss: 0.020, lr: 0.0009981235277677965\n",
      "epoch: 190, acc: 0.994, loss: 0.020, lr: 0.000998113565361467\n",
      "epoch: 191, acc: 0.994, loss: 0.020, lr: 0.0009981036031540074\n",
      "epoch: 192, acc: 0.994, loss: 0.019, lr: 0.0009980936411454122\n",
      "epoch: 193, acc: 0.993, loss: 0.020, lr: 0.0009980836793356755\n",
      "epoch: 194, acc: 0.994, loss: 0.019, lr: 0.0009980737177247912\n",
      "epoch: 195, acc: 0.994, loss: 0.020, lr: 0.0009980637563127533\n",
      "epoch: 196, acc: 0.994, loss: 0.019, lr: 0.000998053795099556\n",
      "epoch: 197, acc: 0.994, loss: 0.019, lr: 0.0009980438340851932\n",
      "epoch: 198, acc: 0.995, loss: 0.018, lr: 0.0009980338732696588\n",
      "epoch: 199, acc: 0.994, loss: 0.019, lr: 0.0009980239126529471\n",
      "epoch: 200, acc: 0.995, loss: 0.018, lr: 0.0009980139522350524\n",
      "epoch: 201, acc: 0.995, loss: 0.018, lr: 0.000998003992015968\n",
      "epoch: 202, acc: 0.995, loss: 0.018, lr: 0.0009979940319956885\n",
      "epoch: 203, acc: 0.995, loss: 0.018, lr: 0.0009979840721742082\n",
      "epoch: 204, acc: 0.995, loss: 0.017, lr: 0.0009979741125515204\n",
      "epoch: 205, acc: 0.995, loss: 0.017, lr: 0.0009979641531276196\n",
      "epoch: 206, acc: 0.995, loss: 0.017, lr: 0.0009979541939024999\n",
      "epoch: 207, acc: 0.995, loss: 0.017, lr: 0.0009979442348761552\n",
      "epoch: 208, acc: 0.995, loss: 0.017, lr: 0.0009979342760485794\n",
      "epoch: 209, acc: 0.995, loss: 0.017, lr: 0.0009979243174197668\n",
      "epoch: 210, acc: 0.995, loss: 0.016, lr: 0.0009979143589897116\n",
      "epoch: 211, acc: 0.995, loss: 0.016, lr: 0.0009979044007584073\n",
      "epoch: 212, acc: 0.994, loss: 0.017, lr: 0.0009978944427258484\n",
      "epoch: 213, acc: 0.995, loss: 0.016, lr: 0.000997884484892029\n",
      "epoch: 214, acc: 0.996, loss: 0.016, lr: 0.0009978745272569427\n",
      "epoch: 215, acc: 0.995, loss: 0.017, lr: 0.000997864569820584\n",
      "epoch: 216, acc: 0.995, loss: 0.016, lr: 0.0009978546125829467\n",
      "epoch: 217, acc: 0.995, loss: 0.015, lr: 0.000997844655544025\n",
      "epoch: 218, acc: 0.995, loss: 0.016, lr: 0.0009978346987038126\n",
      "epoch: 219, acc: 0.995, loss: 0.016, lr: 0.0009978247420623042\n",
      "epoch: 220, acc: 0.995, loss: 0.015, lr: 0.0009978147856194934\n",
      "epoch: 221, acc: 0.995, loss: 0.016, lr: 0.0009978048293753743\n",
      "epoch: 222, acc: 0.996, loss: 0.015, lr: 0.0009977948733299407\n",
      "epoch: 223, acc: 0.995, loss: 0.015, lr: 0.0009977849174831871\n",
      "epoch: 224, acc: 0.996, loss: 0.015, lr: 0.0009977749618351078\n",
      "epoch: 225, acc: 0.995, loss: 0.015, lr: 0.000997765006385696\n",
      "epoch: 226, acc: 0.996, loss: 0.014, lr: 0.0009977550511349464\n",
      "epoch: 227, acc: 0.995, loss: 0.015, lr: 0.0009977450960828528\n",
      "epoch: 228, acc: 0.996, loss: 0.014, lr: 0.0009977351412294093\n",
      "epoch: 229, acc: 0.996, loss: 0.014, lr: 0.0009977251865746098\n",
      "epoch: 230, acc: 0.996, loss: 0.014, lr: 0.0009977152321184487\n",
      "epoch: 231, acc: 0.996, loss: 0.014, lr: 0.0009977052778609198\n",
      "epoch: 232, acc: 0.996, loss: 0.014, lr: 0.0009976953238020174\n",
      "epoch: 233, acc: 0.996, loss: 0.014, lr: 0.0009976853699417351\n",
      "epoch: 234, acc: 0.996, loss: 0.014, lr: 0.0009976754162800674\n",
      "epoch: 235, acc: 0.996, loss: 0.014, lr: 0.0009976654628170083\n",
      "epoch: 236, acc: 0.996, loss: 0.013, lr: 0.0009976555095525515\n",
      "epoch: 237, acc: 0.996, loss: 0.013, lr: 0.0009976455564866915\n",
      "epoch: 238, acc: 0.996, loss: 0.013, lr: 0.000997635603619422\n",
      "epoch: 239, acc: 0.996, loss: 0.013, lr: 0.0009976256509507371\n",
      "epoch: 240, acc: 0.996, loss: 0.012, lr: 0.0009976156984806313\n",
      "epoch: 241, acc: 0.996, loss: 0.014, lr: 0.0009976057462090984\n",
      "epoch: 242, acc: 0.996, loss: 0.013, lr: 0.0009975957941361318\n",
      "epoch: 243, acc: 0.996, loss: 0.014, lr: 0.0009975858422617266\n",
      "epoch: 244, acc: 0.996, loss: 0.012, lr: 0.0009975758905858764\n",
      "epoch: 245, acc: 0.996, loss: 0.013, lr: 0.0009975659391085751\n",
      "epoch: 246, acc: 0.996, loss: 0.012, lr: 0.0009975559878298169\n",
      "epoch: 247, acc: 0.997, loss: 0.012, lr: 0.0009975460367495962\n",
      "epoch: 248, acc: 0.996, loss: 0.013, lr: 0.0009975360858679064\n",
      "epoch: 249, acc: 0.996, loss: 0.014, lr: 0.0009975261351847418\n"
     ]
    }
   ],
   "source": [
    "# Creating the dense layer with 28*28 inputs\n",
    "dense1 = layer_dense(784, 256)\n",
    "batchnorm1 = BatchNormalization(256)  # BatchNorm after first dense\n",
    "activation1 = relu()\n",
    "dropout1 = Dropout(rate=0.2)\n",
    "\n",
    "dense2 = layer_dense(256, 128)\n",
    "batchnorm2 = BatchNormalization(128)  # BatchNorm after second dense\n",
    "activation2 = relu()\n",
    "dropout2 = Dropout(rate=0.2)\n",
    "\n",
    "dense3 = layer_dense(128, 10)\n",
    "\n",
    "# Softmax loss activation\n",
    "loss_activation = activation_softmax_loss_categorical_cross_entropy()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.001, decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(250):\n",
    "    # Forward pass\n",
    "    dense1.forward(x_train.reshape(x_train.shape[0], -1))  # Flatten the input\n",
    "    # batchnorm1.forward(dense1.outputs, training=True)  # Apply Batch Norm\n",
    "    # activation1.forward(batchnorm1.outputs)\n",
    "    activation1.forward(dense1.outputs)\n",
    "    dropout1.forward(activation1.outputs, training=True)\n",
    "\n",
    "    dense2.forward(dropout1.outputs)\n",
    "    # batchnorm2.forward(dense2.outputs, training=True)  # Apply Batch Norm\n",
    "    # activation2.forward(batchnorm2.outputs)\n",
    "    activation2.forward(dense2.outputs)\n",
    "    dropout2.forward(activation2.outputs, training=True)\n",
    "\n",
    "    dense3.forward(dropout2.outputs)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_activation.forward(dense3.outputs, y_train)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    predictions = np.argmax(loss_activation.outputs, axis=1)\n",
    "    if len(y_train.shape) == 2:\n",
    "        y_train = np.argmax(y_train, axis=1)\n",
    "    accuracy = np.mean(predictions == y_train)\n",
    "\n",
    "    print(f'epoch: {epoch}, ' +\n",
    "          f'acc: {accuracy:.3f}, ' +\n",
    "          f'loss: {loss:.3f}, ' +\n",
    "          f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.outputs, y_train)\n",
    "    dense3.backward(loss_activation.dinputs)\n",
    "    dropout2.backward(dense3.dinputs)\n",
    "    activation2.backward(dropout2.dinputs)\n",
    "    # batchnorm2.backward(activation2.dinputs)  # Backpropagate through BatchNorm\n",
    "    # dense2.backward(batchnorm2.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "\n",
    "    dropout1.backward(dense2.dinputs)\n",
    "    activation1.backward(dropout1.dinputs)\n",
    "    # batchnorm1.backward(activation1.dinputs)  # Backpropagate through BatchNorm\n",
    "    # dense1.backward(batchnorm1.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    # optimizer.update_params(batchnorm1)  # Update BatchNorm parameters\n",
    "    optimizer.update_params(dense2)\n",
    "    # optimizer.update_params(batchnorm2)  # Update BatchNorm parameters\n",
    "    optimizer.update_params(dense3)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    " Metric analysis\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.08%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 971    0    1    1    1    1    2    0    1    2]\n",
      " [   0 1128    2    0    0    1    2    0    2    0]\n",
      " [   4    0 1014    2    1    0    1    4    5    1]\n",
      " [   0    0    5  990    0    4    0    4    3    4]\n",
      " [   1    1    4    0  960    0    4    2    1    9]\n",
      " [   2    0    0    5    1  873    6    0    4    1]\n",
      " [   3    2    2    0    2    6  943    0    0    0]\n",
      " [   1    6    8    2    0    0    0 1004    2    5]\n",
      " [   4    1    3    4    3    5    3    4  943    4]\n",
      " [   2    2    0    5   10    2    0    6    0  982]]\n",
      "\n",
      "Per-class Accuracy:\n",
      "Digit 0: 99.08%\n",
      "Digit 1: 99.38%\n",
      "Digit 2: 98.26%\n",
      "Digit 3: 98.02%\n",
      "Digit 4: 97.76%\n",
      "Digit 5: 97.87%\n",
      "Digit 6: 98.43%\n",
      "Digit 7: 97.67%\n",
      "Digit 8: 96.82%\n",
      "Digit 9: 97.32%\n",
      "\n",
      "Total Misclassified Samples: 192\n"
     ]
    }
   ],
   "source": [
    "import cupy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare test data (ensure it's in the right format)\n",
    "x_test = np.asarray(x_test).reshape(x_test.shape[0], -1)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "# Perform forward pass through the network\n",
    "def predict(X):\n",
    "    # Forward pass through the first layer and ReLU\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.outputs)\n",
    "    dropout1.forward(activation1.outputs, training=False)  # Inference mode\n",
    "\n",
    "    # Second layer and ReLU\n",
    "    dense2.forward(dropout1.outputs)\n",
    "    activation2.forward(dense2.outputs)\n",
    "    dropout2.forward(activation2.outputs, training=False)  # Inference mode\n",
    "\n",
    "    # Final layer with Softmax\n",
    "    dense3.forward(dropout2.outputs)\n",
    "    \n",
    "    # Get predictions (class with highest probability)\n",
    "    return np.argmax(dense3.outputs, axis=1)\n",
    "\n",
    "# Perform prediction\n",
    "predictions = predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Optional: Detailed classification report\n",
    "from collections import Counter\n",
    "\n",
    "# Confusion matrix\n",
    "def confusion_matrix(true, pred):\n",
    "    conf_matrix = np.zeros((10, 10), dtype=int)\n",
    "    for t, p in zip(true, pred):\n",
    "        conf_matrix[t, p] += 1\n",
    "    return conf_matrix\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_mat)\n",
    "\n",
    "# Per-class accuracy\n",
    "per_class_accuracy = conf_mat.diagonal() / conf_mat.sum(axis=1)\n",
    "print(\"\\nPer-class Accuracy:\")\n",
    "for digit, acc in enumerate(per_class_accuracy):\n",
    "    print(f\"Digit {digit}: {acc * 100:.2f}%\")\n",
    "\n",
    "# Misclassification analysis\n",
    "misclassified_indices = np.where(predictions != y_test)[0]\n",
    "print(f\"\\nTotal Misclassified Samples: {len(misclassified_indices)}\")\n",
    "\n",
    "# Optional: Visualize some misclassified samples\n",
    "def visualize_misclassifications(x_test, y_test, predictions, num_to_show=5):\n",
    "    misclassified_indices = np.where(predictions != y_test)[0]\n",
    "    \n",
    "    # Randomly select some misclassified samples\n",
    "    show_indices = np.random.choice(misclassified_indices, \n",
    "                                    min(num_to_show, len(misclassified_indices)), \n",
    "                                    replace=False)\n",
    "    \n",
    "    for idx in show_indices:\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')\n",
    "        plt.title(f\"True: {y_test[idx]}, Predicted: {predictions[idx]}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Uncomment to visualize misclassifications\n",
    "# visualize_misclassifications(x_test, y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
