{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_digit(image, label=None):\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if label is not None:\n",
    "        plt.title(f'Label: {label}')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 76\n",
    "image = x_train[index].reshape(28, 28)\n",
    "visualize_digit(image, y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train)  # Convert to CuPy array\n",
    "y_train = np.asarray(y_train)  # Convert to CuPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_x_train = x_train\n",
    "original_y_train = y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Data augmentation\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as np\n",
    "import scipy.ndimage\n",
    "\n",
    "def augment_data(images):\n",
    "    augmented_images = []\n",
    "    for img in images:\n",
    "        # Convert CuPy array to NumPy and reshape to 28x28\n",
    "        img_np = img.get().reshape(28, 28)\n",
    "        \n",
    "        # Generate augmentation parameters as Python floats\n",
    "        angle = float(np.random.uniform(-10, 10))\n",
    "        shift_x = float(np.random.uniform(-2, 2))\n",
    "        shift_y = float(np.random.uniform(-2, 2))\n",
    "        \n",
    "        # Apply rotation and shifting using SciPy (which works with NumPy arrays)\n",
    "        rotated = scipy.ndimage.rotate(img_np, angle, reshape=False, mode='constant', cval=0)\n",
    "        shifted = scipy.ndimage.shift(rotated, shift=[shift_x, shift_y], mode='constant', cval=0)\n",
    "        \n",
    "        # Convert back to CuPy array and add noise\n",
    "        augmented = np.asarray(shifted) + np.random.normal(0, 0.02, (28, 28))\n",
    "        augmented = np.clip(augmented, 0, 1)\n",
    "        \n",
    "        # Flatten to original shape (e.g., 784)\n",
    "        augmented_images.append(augmented.flatten())\n",
    "    \n",
    "    return np.array(augmented_images)\n",
    "\n",
    "# Example usage:\n",
    "x_train_augmented = augment_data(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Data manipulation\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For NumPy/CuPy arrays, use slicing instead of .iloc\n",
    "# x_train = x_train[:60000]  # First 5,000 entries\n",
    "# y_train = y_train[:60000]\n",
    "\n",
    "# # If you want a balanced subset, here's a CuPy/NumPy compatible version:\n",
    "# def get_balanced_subset(x, y, samples_per_class=500):\n",
    "#     balanced_x = []\n",
    "#     balanced_y = []\n",
    "#     for digit in range(10):\n",
    "#         # Find indices of current digit\n",
    "#         digit_indices = np.where(y == digit)[0]\n",
    "        \n",
    "#         # Select first 'samples_per_class' indices for this digit\n",
    "#         selected_indices = digit_indices[:samples_per_class]\n",
    "        \n",
    "#         # Append selected samples\n",
    "#         balanced_x.append(x[selected_indices])\n",
    "#         balanced_y.append(y[selected_indices])\n",
    "    \n",
    "#     # Concatenate along the first axis\n",
    "#     balanced_x = np.concatenate(balanced_x, axis=0)\n",
    "#     balanced_y = np.concatenate(balanced_y, axis=0)\n",
    "    \n",
    "#     return balanced_x, balanced_y\n",
    "\n",
    "# # Alternative method to get balanced subset\n",
    "# x_train, y_train = get_balanced_subset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Beginning of Neural Network</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Initial layer\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer_dense:\n",
    "    def __init__(self , n_inputs , n_neurons):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "\n",
    "        self.weights = np.random.randn(n_inputs , n_neurons)*0.01\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        #calculating the dot product basically X.W + b\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "#dvalues = dl_dz\n",
    "    def backward(self , dvalues):\n",
    "        self.dvalues = dvalues\n",
    "        \n",
    "        #gradient on weight and bias parameter\n",
    "        self.dweights = np.dot(self.inputs.T , dvalues)\n",
    "        self.dbiases = np.sum(dvalues , axis=0 , keepdims=True)\n",
    "\n",
    "        #gradient on input values\n",
    "        self.dinputs = np.dot(dvalues , self.weights.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "ReLU activation \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relu:\n",
    "    def forward(self , inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.maximum(0 , inputs)\n",
    "\n",
    "    def backward(self , dvalues):\n",
    "        #making the copies\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # zero gradient where input values are negative\n",
    "        self.dinputs[self.inputs <=0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Softmax activation\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation_softmax:\n",
    "    def forward(self , inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs , axis = 1 ,keepdims=True))\n",
    "        # keepdims = True for broadcasting\n",
    "        probabilities = exp_values/(np.sum(exp_values , axis=1 , keepdims=True))\n",
    "\n",
    "        self.outputs = probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Loss Class\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common loss class\n",
    "class loss:\n",
    "    #calculates the data and regularization losses , given the ground truth and predicted values\n",
    "    def calculate(self , outputs , y):\n",
    "        sample_loss = self.forward(outputs , y)\n",
    "        data_loss = np.mean(sample_loss)\n",
    "\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Categorical Cross Entropy Loss\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_categorical_cross_entropy(loss):\n",
    "\n",
    "    def forward(self , y_pred , y_true ):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        #clipping the data to prevent division by 0 \n",
    "        # and preventing giant values\n",
    "\n",
    "        y_pred_clipped = np.clip(y_pred , 1e-7 , 1-1e-7)\n",
    "\n",
    "        #probabilities for target values - \n",
    "        #only if categorical labels\n",
    "        #applying advanced indexing\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1 #making it to multiply just along the rows\n",
    "            )\n",
    "\n",
    "        negative_log_likelihood = -np.log(correct_confidences)\n",
    "        return negative_log_likelihood\n",
    "    \n",
    "    #backward pass\n",
    "    def backward(self , dvalues , y_true):\n",
    "\n",
    "        #number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        #number of labels in every sample will be calculated using the 1st sample\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        #if labels are sparse , turning them into one hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        #calculating the gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "combined softmax activation and categorical cross entropy for last layer : Forward and Backward pass\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax and cross entropy loss are brought together\n",
    "# in the single following class for faster backward step\n",
    "\n",
    "class activation_softmax_loss_categorical_cross_entropy:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = activation_softmax()\n",
    "        self.loss = loss_categorical_cross_entropy()\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self , inputs , y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.outputs = self.activation.outputs\n",
    "        return self.loss.calculate(self.outputs , y_true)\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self , dvalues , y_true):\n",
    "        samples = len(dvalues)\n",
    "        #if lables are one-hot encoded,\n",
    "        #turn them to discrete values\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true , axis = 1)\n",
    "\n",
    "        #copying in order to modify safely\n",
    "        self.dinputs = dvalues.copy()\n",
    "        #calculate gradients\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        #normalize the gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "using optimizer as adam\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays, create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Vanilla SGD\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=0.01, decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - \\\n",
    "                             self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - \\\n",
    "                           self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Droput method\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, rate=0.3):\n",
    "\n",
    "        # Dropout rate\n",
    "        self.rate = rate\n",
    "        \n",
    "        # Mask to track which neurons are kept/dropped\n",
    "        self.mask = None\n",
    "        \n",
    "        # Binary scale for maintaining expected output during inference\n",
    "        self.scale = 1 / (1 - rate)\n",
    "        \n",
    "    def forward(self, inputs, training=True):\n",
    "\n",
    "        # Store inputs for backward pass\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # During training, randomly drop neurons\n",
    "        if training:\n",
    "            # Create a mask of same shape as inputs\n",
    "            self.mask = np.random.rand(*inputs.shape) > self.rate\n",
    "            \n",
    "            # Scale and apply mask\n",
    "            self.outputs = inputs * self.mask * self.scale\n",
    "        else:\n",
    "            # During inference, no dropout\n",
    "            self.outputs = inputs\n",
    "        \n",
    "        return self.outputs\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # During backward pass, apply the same mask\n",
    "        self.dinputs = dvalues * self.mask * self.scale\n",
    "        \n",
    "        return self.dinputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Batch Normalization\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, size, momentum=0.9, epsilon=1e-5):\n",
    "        self.gamma = np.ones((1, size))\n",
    "        self.beta = np.zeros((1, size))\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.running_mean = np.zeros((1, size))\n",
    "        self.running_var = np.ones((1, size))\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        self.X = X  # Store input for backpropagation\n",
    "        if training:\n",
    "            batch_mean = np.mean(X, axis=0, keepdims=True)\n",
    "            batch_var = np.var(X, axis=0, keepdims=True)\n",
    "            self.X_normalized = (X - batch_mean) / np.sqrt(batch_var + self.epsilon)\n",
    "            self.outputs = self.gamma * self.X_normalized + self.beta\n",
    "\n",
    "            # Update running stats\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n",
    "        else:\n",
    "            self.X_normalized = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            self.outputs = self.gamma * self.X_normalized + self.beta\n",
    "        \n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self, dinputs):\n",
    "        batch_size = self.X.shape[0]\n",
    "\n",
    "        dgamma = np.sum(dinputs * self.X_normalized, axis=0, keepdims=True)\n",
    "        dbeta = np.sum(dinputs, axis=0, keepdims=True)\n",
    "\n",
    "        dX_normalized = dinputs * self.gamma\n",
    "        dvar = np.sum(dX_normalized * (self.X - self.running_mean) * -0.5 * np.power(self.running_var + self.epsilon, -1.5), axis=0, keepdims=True)\n",
    "        dmean = np.sum(dX_normalized * -1 / np.sqrt(self.running_var + self.epsilon), axis=0, keepdims=True) + dvar * np.mean(-2 * (self.X - self.running_mean), axis=0, keepdims=True)\n",
    "\n",
    "        self.dinputs = dX_normalized / np.sqrt(self.running_var + self.epsilon) + dvar * 2 * (self.X - self.running_mean) / batch_size + dmean / batch_size\n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Training the neural network\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.088, loss: 2.479, lr: 0.01\n",
      "epoch: 1, acc: 0.127, loss: 13.859, lr: 0.01\n",
      "epoch: 2, acc: 0.237, loss: 10.707, lr: 0.009999000099990002\n",
      "epoch: 3, acc: 0.259, loss: 8.497, lr: 0.009998000399920017\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 47\u001b[0m\n\u001b[0;32m     43\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_train, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     44\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(predictions \u001b[38;5;241m==\u001b[39m y_train)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m---> 47\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     48\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     49\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer\u001b[38;5;241m.\u001b[39mcurrent_learning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     52\u001b[0m loss_activation\u001b[38;5;241m.\u001b[39mbackward(loss_activation\u001b[38;5;241m.\u001b[39moutputs, y_train)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creating the dense layer with 28*28 inputs\n",
    "dense1 = layer_dense(784, 512)\n",
    "batchnorm1 = BatchNormalization(512)  # BatchNorm after first dense\n",
    "activation1 = relu()\n",
    "dropout1 = Dropout(rate=0.3)\n",
    "\n",
    "dense2 = layer_dense(512, 256)\n",
    "batchnorm2 = BatchNormalization(256)  # BatchNorm after second dense\n",
    "activation2 = relu()\n",
    "dropout2 = Dropout(rate=0.3)\n",
    "\n",
    "dense3 = layer_dense(256, 10)\n",
    "\n",
    "# Softmax loss activation\n",
    "loss_activation = activation_softmax_loss_categorical_cross_entropy()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.01, decay=1e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(250):\n",
    "    # Forward pass\n",
    "    dense1.forward(x_train.reshape(x_train.shape[0], -1))  # Flatten the input\n",
    "    # batchnorm1.forward(dense1.outputs, training=True)  # Apply Batch Norm\n",
    "    # activation1.forward(batchnorm1.outputs)\n",
    "    activation1.forward(dense1.outputs)\n",
    "    dropout1.forward(activation1.outputs, training=True)\n",
    "\n",
    "    dense2.forward(dropout1.outputs)\n",
    "    # batchnorm2.forward(dense2.outputs, training=True)  # Apply Batch Norm\n",
    "    # activation2.forward(batchnorm2.outputs)\n",
    "    activation2.forward(dense2.outputs)\n",
    "    dropout2.forward(activation2.outputs, training=True)\n",
    "\n",
    "    dense3.forward(dropout2.outputs)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_activation.forward(dense3.outputs, y_train)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    predictions = np.argmax(loss_activation.outputs, axis=1)\n",
    "    if len(y_train.shape) == 2:\n",
    "        y_train = np.argmax(y_train, axis=1)\n",
    "    accuracy = np.mean(predictions == y_train)\n",
    "\n",
    "    print(f'epoch: {epoch}, ' +\n",
    "          f'acc: {accuracy:.3f}, ' +\n",
    "          f'loss: {loss:.3f}, ' +\n",
    "          f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.outputs, y_train)\n",
    "    dense3.backward(loss_activation.dinputs)\n",
    "    dropout2.backward(dense3.dinputs)\n",
    "    activation2.backward(dropout2.dinputs)\n",
    "    # batchnorm2.backward(activation2.dinputs)  # Backpropagate through BatchNorm\n",
    "    # dense2.backward(batchnorm2.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "\n",
    "    dropout1.backward(dense2.dinputs)\n",
    "    activation1.backward(dropout1.dinputs)\n",
    "    # batchnorm1.backward(activation1.dinputs)  # Backpropagate through BatchNorm\n",
    "    # dense1.backward(batchnorm1.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    # optimizer.update_params(batchnorm1)  # Update BatchNorm parameters\n",
    "    optimizer.update_params(dense2)\n",
    "    # optimizer.update_params(batchnorm2)  # Update BatchNorm parameters\n",
    "    optimizer.update_params(dense3)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    " Metric analysis\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 76.48%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[915   0   0  31   0   0   1   0  21  12]\n",
      " [  0 958   1   8  19   4   5   0 140   0]\n",
      " [ 26   1 787  76   1   7  36  13  82   3]\n",
      " [  4   0   9 945   0   6   1   8  35   2]\n",
      " [  3   1   2   2 523   5  11   1 191 243]\n",
      " [ 15   0   0 450   0 250   5   3 155  14]\n",
      " [ 71   2   1   5  22   5 793   0  52   7]\n",
      " [  1   2  15   9   2   2   0 885  81  30]\n",
      " [  3   0   3 137   1  19   9   3 788  11]\n",
      " [ 10   0   0  24   2   3   0  34 133 803]]\n",
      "\n",
      "Per-class Accuracy:\n",
      "Digit 0: 93.37%\n",
      "Digit 1: 84.41%\n",
      "Digit 2: 76.26%\n",
      "Digit 3: 93.56%\n",
      "Digit 4: 53.26%\n",
      "Digit 5: 28.03%\n",
      "Digit 6: 82.78%\n",
      "Digit 7: 86.17%\n",
      "Digit 8: 80.90%\n",
      "Digit 9: 79.58%\n",
      "\n",
      "Total Misclassified Samples: 2352\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data (ensure it's in the right format)\n",
    "x_test = np.asarray(x_test)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "# Perform forward pass through the network\n",
    "def predict(X):\n",
    "    # Forward pass through the first layer and ReLU\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.outputs)\n",
    "    dropout1.forward(activation1.outputs, training=False)  # Inference mode\n",
    "\n",
    "    # Second layer and ReLU\n",
    "    dense2.forward(dropout1.outputs)\n",
    "    activation2.forward(dense2.outputs)\n",
    "    dropout2.forward(activation2.outputs, training=False)  # Inference mode\n",
    "\n",
    "    # Final layer with Softmax\n",
    "    dense3.forward(dropout2.outputs)\n",
    "    \n",
    "    # Get predictions (class with highest probability)\n",
    "    return np.argmax(dense3.outputs, axis=1)\n",
    "\n",
    "# Perform prediction\n",
    "predictions = predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Optional: Detailed classification report\n",
    "from collections import Counter\n",
    "\n",
    "# Confusion matrix\n",
    "def confusion_matrix(true, pred):\n",
    "    conf_matrix = np.zeros((10, 10), dtype=int)\n",
    "    for t, p in zip(true, pred):\n",
    "        conf_matrix[t, p] += 1\n",
    "    return conf_matrix\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_mat)\n",
    "\n",
    "# Per-class accuracy\n",
    "per_class_accuracy = conf_mat.diagonal() / conf_mat.sum(axis=1)\n",
    "print(\"\\nPer-class Accuracy:\")\n",
    "for digit, acc in enumerate(per_class_accuracy):\n",
    "    print(f\"Digit {digit}: {acc * 100:.2f}%\")\n",
    "\n",
    "# Misclassification analysis\n",
    "misclassified_indices = np.where(predictions != y_test)[0]\n",
    "print(f\"\\nTotal Misclassified Samples: {len(misclassified_indices)}\")\n",
    "\n",
    "# Optional: Visualize some misclassified samples\n",
    "def visualize_misclassifications(x_test, y_test, predictions, num_to_show=5):\n",
    "    misclassified_indices = np.where(predictions != y_test)[0]\n",
    "    \n",
    "    # Randomly select some misclassified samples\n",
    "    show_indices = np.random.choice(misclassified_indices, \n",
    "                                    min(num_to_show, len(misclassified_indices)), \n",
    "                                    replace=False)\n",
    "    \n",
    "    for idx in show_indices:\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')\n",
    "        plt.title(f\"True: {y_test[idx]}, Predicted: {predictions[idx]}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Uncomment to visualize misclassifications\n",
    "# visualize_misclassifications(x_test, y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
