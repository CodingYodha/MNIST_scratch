{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_digit(image, label=None):\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if label is not None:\n",
    "        plt.title(f'Label: {label}')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 76\n",
    "image = x_train[index].reshape(28, 28)\n",
    "visualize_digit(image, y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train)  # Convert to CuPy array\n",
    "y_train = np.asarray(y_train)  # Convert to CuPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_x_train = x_train\n",
    "original_y_train = y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Data augmentation\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cupy as np\n",
    "# import scipy.ndimage\n",
    "\n",
    "# def augment_data(images):\n",
    "#     augmented_images = []\n",
    "#     for img in images:\n",
    "#         # Convert CuPy array to NumPy and reshape to 28x28\n",
    "#         img_np = img.get().reshape(28, 28)\n",
    "        \n",
    "#         # Generate augmentation parameters as Python floats\n",
    "#         angle = float(np.random.uniform(-10, 10))\n",
    "#         shift_x = float(np.random.uniform(-2, 2))\n",
    "#         shift_y = float(np.random.uniform(-2, 2))\n",
    "        \n",
    "#         # Apply rotation and shifting using SciPy (which works with NumPy arrays)\n",
    "#         rotated = scipy.ndimage.rotate(img_np, angle, reshape=False, mode='constant', cval=0)\n",
    "#         shifted = scipy.ndimage.shift(rotated, shift=[shift_x, shift_y], mode='constant', cval=0)\n",
    "        \n",
    "#         # Convert back to CuPy array and add noise\n",
    "#         augmented = np.asarray(shifted) + np.random.normal(0, 0.02, (28, 28))\n",
    "#         augmented = np.clip(augmented, 0, 1)\n",
    "        \n",
    "#         # Flatten to original shape (e.g., 784)\n",
    "#         augmented_images.append(augmented.flatten())\n",
    "    \n",
    "#     return np.array(augmented_images)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Data manipulation\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For NumPy/CuPy arrays, use slicing instead of .iloc\n",
    "# x_train = x_train[:60000]  # First 5,000 entries\n",
    "# y_train = y_train[:60000]\n",
    "\n",
    "# # If you want a balanced subset, here's a CuPy/NumPy compatible version:\n",
    "# def get_balanced_subset(x, y, samples_per_class=500):\n",
    "#     balanced_x = []\n",
    "#     balanced_y = []\n",
    "#     for digit in range(10):\n",
    "#         # Find indices of current digit\n",
    "#         digit_indices = np.where(y == digit)[0]\n",
    "        \n",
    "#         # Select first 'samples_per_class' indices for this digit\n",
    "#         selected_indices = digit_indices[:samples_per_class]\n",
    "        \n",
    "#         # Append selected samples\n",
    "#         balanced_x.append(x[selected_indices])\n",
    "#         balanced_y.append(y[selected_indices])\n",
    "    \n",
    "#     # Concatenate along the first axis\n",
    "#     balanced_x = np.concatenate(balanced_x, axis=0)\n",
    "#     balanced_y = np.concatenate(balanced_y, axis=0)\n",
    "    \n",
    "#     return balanced_x, balanced_y\n",
    "\n",
    "# # Alternative method to get balanced subset\n",
    "# x_train, y_train = get_balanced_subset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Beginning of Neural Network</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Initial layer\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer_dense:\n",
    "    def __init__(self , n_inputs , n_neurons):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "\n",
    "        self.weights = np.random.randn(n_inputs , n_neurons)*0.01\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        #calculating the dot product basically X.W + b\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "#dvalues = dl_dz\n",
    "    def backward(self , dvalues):\n",
    "        self.dvalues = dvalues\n",
    "        \n",
    "        #gradient on weight and bias parameter\n",
    "        self.dweights = np.dot(self.inputs.T , dvalues)\n",
    "        self.dbiases = np.sum(dvalues , axis=0 , keepdims=True)\n",
    "\n",
    "        #gradient on input values\n",
    "        self.dinputs = np.dot(dvalues , self.weights.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "ReLU activation \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relu:\n",
    "    def forward(self , inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.maximum(0 , inputs)\n",
    "\n",
    "    def backward(self , dvalues):\n",
    "        #making the copies\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # zero gradient where input values are negative\n",
    "        self.dinputs[self.inputs <=0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Softmax activation\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation_softmax:\n",
    "    def forward(self , inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs , axis = 1 ,keepdims=True))\n",
    "        # keepdims = True for broadcasting\n",
    "        probabilities = exp_values/(np.sum(exp_values , axis=1 , keepdims=True))\n",
    "\n",
    "        self.outputs = probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Loss Class\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common loss class\n",
    "class loss:\n",
    "    #calculates the data and regularization losses , given the ground truth and predicted values\n",
    "    def calculate(self , outputs , y):\n",
    "        sample_loss = self.forward(outputs , y)\n",
    "        data_loss = np.mean(sample_loss)\n",
    "\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Categorical Cross Entropy Loss\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_categorical_cross_entropy(loss):\n",
    "\n",
    "    def forward(self , y_pred , y_true ):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        #clipping the data to prevent division by 0 \n",
    "        # and preventing giant values\n",
    "\n",
    "        y_pred_clipped = np.clip(y_pred , 1e-7 , 1-1e-7)\n",
    "\n",
    "        #probabilities for target values - \n",
    "        #only if categorical labels\n",
    "        #applying advanced indexing\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1 #making it to multiply just along the rows\n",
    "            )\n",
    "\n",
    "        negative_log_likelihood = -np.log(correct_confidences)\n",
    "        return negative_log_likelihood\n",
    "    \n",
    "    #backward pass\n",
    "    def backward(self , dvalues , y_true):\n",
    "\n",
    "        #number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        #number of labels in every sample will be calculated using the 1st sample\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        #if labels are sparse , turning them into one hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        #calculating the gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "combined softmax activation and categorical cross entropy for last layer : Forward and Backward pass\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax and cross entropy loss are brought together\n",
    "# in the single following class for faster backward step\n",
    "\n",
    "class activation_softmax_loss_categorical_cross_entropy:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = activation_softmax()\n",
    "        self.loss = loss_categorical_cross_entropy()\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self , inputs , y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.outputs = self.activation.outputs\n",
    "        return self.loss.calculate(self.outputs , y_true)\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self , dvalues , y_true):\n",
    "        samples = len(dvalues)\n",
    "        #if lables are one-hot encoded,\n",
    "        #turn them to discrete values\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true , axis = 1)\n",
    "\n",
    "        #copying in order to modify safely\n",
    "        self.dinputs = dvalues.copy()\n",
    "        #calculate gradients\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        #normalize the gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "using optimizer as adam\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays, create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Vanilla SGD\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=0.01, decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - \\\n",
    "                             self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - \\\n",
    "                           self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Droput method\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, rate=0.3):\n",
    "\n",
    "        # Dropout rate\n",
    "        self.rate = rate\n",
    "        \n",
    "        # Mask to track which neurons are kept/dropped\n",
    "        self.mask = None\n",
    "        \n",
    "        # Binary scale for maintaining expected output during inference\n",
    "        self.scale = 1 / (1 - rate)\n",
    "        \n",
    "    def forward(self, inputs, training=True):\n",
    "\n",
    "        # Store inputs for backward pass\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # During training, randomly drop neurons\n",
    "        if training:\n",
    "            # Create a mask of same shape as inputs\n",
    "            self.mask = np.random.rand(*inputs.shape) > self.rate\n",
    "            \n",
    "            # Scale and apply mask\n",
    "            self.outputs = inputs * self.mask * self.scale\n",
    "        else:\n",
    "            # During inference, no dropout\n",
    "            self.outputs = inputs\n",
    "        \n",
    "        return self.outputs\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # During backward pass, apply the same mask\n",
    "        self.dinputs = dvalues * self.mask * self.scale\n",
    "        \n",
    "        return self.dinputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Batch Normalization\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, size, momentum=0.9, epsilon=1e-5):\n",
    "        self.gamma = np.ones((1, size))\n",
    "        self.beta = np.zeros((1, size))\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.running_mean = np.zeros((1, size))\n",
    "        self.running_var = np.ones((1, size))\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        self.X = X  # Store input for backpropagation\n",
    "        if training:\n",
    "            batch_mean = np.mean(X, axis=0, keepdims=True)\n",
    "            batch_var = np.var(X, axis=0, keepdims=True)\n",
    "            self.X_normalized = (X - batch_mean) / np.sqrt(batch_var + self.epsilon)\n",
    "            self.outputs = self.gamma * self.X_normalized + self.beta\n",
    "\n",
    "            # Update running stats\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n",
    "        else:\n",
    "            self.X_normalized = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            self.outputs = self.gamma * self.X_normalized + self.beta\n",
    "        \n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self, dinputs):\n",
    "        batch_size = self.X.shape[0]\n",
    "\n",
    "        dgamma = np.sum(dinputs * self.X_normalized, axis=0, keepdims=True)\n",
    "        dbeta = np.sum(dinputs, axis=0, keepdims=True)\n",
    "\n",
    "        dX_normalized = dinputs * self.gamma\n",
    "        dvar = np.sum(dX_normalized * (self.X - self.running_mean) * -0.5 * np.power(self.running_var + self.epsilon, -1.5), axis=0, keepdims=True)\n",
    "        dmean = np.sum(dX_normalized * -1 / np.sqrt(self.running_var + self.epsilon), axis=0, keepdims=True) + dvar * np.mean(-2 * (self.X - self.running_mean), axis=0, keepdims=True)\n",
    "\n",
    "        self.dinputs = dX_normalized / np.sqrt(self.running_var + self.epsilon) + dvar * 2 * (self.X - self.running_mean) / batch_size + dmean / batch_size\n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block \">\n",
    "Training the neural network\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.078, loss: 2.351, lr: 0.001\n",
      "epoch: 1, acc: 0.418, loss: 1.922, lr: 0.001\n",
      "epoch: 2, acc: 0.645, loss: 1.420, lr: 0.0009999900000999989\n",
      "epoch: 3, acc: 0.720, loss: 0.999, lr: 0.0009999800003999922\n",
      "epoch: 4, acc: 0.750, loss: 0.774, lr: 0.000999970000899973\n",
      "epoch: 5, acc: 0.755, loss: 0.732, lr: 0.000999960001599936\n",
      "epoch: 6, acc: 0.806, loss: 0.614, lr: 0.000999950002499875\n",
      "epoch: 7, acc: 0.804, loss: 0.607, lr: 0.000999940003599784\n",
      "epoch: 8, acc: 0.840, loss: 0.516, lr: 0.000999930004899657\n",
      "epoch: 9, acc: 0.842, loss: 0.515, lr: 0.000999920006399488\n",
      "epoch: 10, acc: 0.869, loss: 0.451, lr: 0.0009999100080992712\n",
      "epoch: 11, acc: 0.876, loss: 0.416, lr: 0.000999900009999\n",
      "epoch: 12, acc: 0.878, loss: 0.400, lr: 0.000999890012098669\n",
      "epoch: 13, acc: 0.890, loss: 0.362, lr: 0.0009998800143982724\n",
      "epoch: 14, acc: 0.898, loss: 0.339, lr: 0.0009998700168978034\n",
      "epoch: 15, acc: 0.902, loss: 0.326, lr: 0.0009998600195972563\n",
      "epoch: 16, acc: 0.907, loss: 0.314, lr: 0.0009998500224966255\n",
      "epoch: 17, acc: 0.910, loss: 0.304, lr: 0.0009998400255959046\n",
      "epoch: 18, acc: 0.915, loss: 0.292, lr: 0.0009998300288950879\n",
      "epoch: 19, acc: 0.917, loss: 0.282, lr: 0.000999820032394169\n",
      "epoch: 20, acc: 0.920, loss: 0.274, lr: 0.0009998100360931424\n",
      "epoch: 21, acc: 0.922, loss: 0.265, lr: 0.0009998000399920016\n",
      "epoch: 22, acc: 0.924, loss: 0.257, lr: 0.000999790044090741\n",
      "epoch: 23, acc: 0.927, loss: 0.247, lr: 0.0009997800483893542\n",
      "epoch: 24, acc: 0.930, loss: 0.236, lr: 0.000999770052887836\n",
      "epoch: 25, acc: 0.931, loss: 0.231, lr: 0.0009997600575861792\n",
      "epoch: 26, acc: 0.932, loss: 0.225, lr: 0.0009997500624843788\n",
      "epoch: 27, acc: 0.934, loss: 0.219, lr: 0.0009997400675824286\n",
      "epoch: 28, acc: 0.936, loss: 0.213, lr: 0.0009997300728803223\n",
      "epoch: 29, acc: 0.938, loss: 0.204, lr: 0.0009997200783780542\n",
      "epoch: 30, acc: 0.939, loss: 0.202, lr: 0.0009997100840756182\n",
      "epoch: 31, acc: 0.940, loss: 0.197, lr: 0.000999700089973008\n",
      "epoch: 32, acc: 0.941, loss: 0.195, lr: 0.0009996900960702183\n",
      "epoch: 33, acc: 0.943, loss: 0.188, lr: 0.0009996801023672425\n",
      "epoch: 34, acc: 0.945, loss: 0.182, lr: 0.000999670108864075\n",
      "epoch: 35, acc: 0.946, loss: 0.178, lr: 0.0009996601155607093\n",
      "epoch: 36, acc: 0.948, loss: 0.174, lr: 0.0009996501224571398\n",
      "epoch: 37, acc: 0.949, loss: 0.170, lr: 0.0009996401295533609\n",
      "epoch: 38, acc: 0.951, loss: 0.163, lr: 0.0009996301368493659\n",
      "epoch: 39, acc: 0.952, loss: 0.161, lr: 0.0009996201443451488\n",
      "epoch: 40, acc: 0.952, loss: 0.158, lr: 0.0009996101520407042\n",
      "epoch: 41, acc: 0.954, loss: 0.153, lr: 0.0009996001599360256\n",
      "epoch: 42, acc: 0.954, loss: 0.152, lr: 0.0009995901680311073\n",
      "epoch: 43, acc: 0.955, loss: 0.148, lr: 0.000999580176325943\n",
      "epoch: 44, acc: 0.957, loss: 0.144, lr: 0.0009995701848205273\n",
      "epoch: 45, acc: 0.958, loss: 0.141, lr: 0.0009995601935148535\n",
      "epoch: 46, acc: 0.958, loss: 0.138, lr: 0.0009995502024089159\n",
      "epoch: 47, acc: 0.960, loss: 0.135, lr: 0.000999540211502709\n",
      "epoch: 48, acc: 0.960, loss: 0.133, lr: 0.0009995302207962257\n",
      "epoch: 49, acc: 0.961, loss: 0.130, lr: 0.000999520230289461\n",
      "epoch: 50, acc: 0.961, loss: 0.129, lr: 0.0009995102399824086\n",
      "epoch: 51, acc: 0.963, loss: 0.126, lr: 0.0009995002498750624\n",
      "epoch: 52, acc: 0.963, loss: 0.123, lr: 0.0009994902599674165\n",
      "epoch: 53, acc: 0.964, loss: 0.119, lr: 0.000999480270259465\n",
      "epoch: 54, acc: 0.964, loss: 0.117, lr: 0.000999470280751202\n",
      "epoch: 55, acc: 0.965, loss: 0.115, lr: 0.000999460291442621\n",
      "epoch: 56, acc: 0.966, loss: 0.114, lr: 0.0009994503023337165\n",
      "epoch: 57, acc: 0.967, loss: 0.112, lr: 0.0009994403134244824\n",
      "epoch: 58, acc: 0.967, loss: 0.110, lr: 0.0009994303247149127\n",
      "epoch: 59, acc: 0.966, loss: 0.108, lr: 0.0009994203362050011\n",
      "epoch: 60, acc: 0.968, loss: 0.107, lr: 0.0009994103478947421\n",
      "epoch: 61, acc: 0.968, loss: 0.105, lr: 0.0009994003597841295\n",
      "epoch: 62, acc: 0.969, loss: 0.100, lr: 0.0009993903718731574\n",
      "epoch: 63, acc: 0.970, loss: 0.100, lr: 0.0009993803841618196\n",
      "epoch: 64, acc: 0.970, loss: 0.099, lr: 0.0009993703966501106\n",
      "epoch: 65, acc: 0.971, loss: 0.096, lr: 0.0009993604093380237\n",
      "epoch: 66, acc: 0.971, loss: 0.095, lr: 0.0009993504222255533\n",
      "epoch: 67, acc: 0.972, loss: 0.093, lr: 0.0009993404353126935\n",
      "epoch: 68, acc: 0.972, loss: 0.092, lr: 0.0009993304485994385\n",
      "epoch: 69, acc: 0.973, loss: 0.089, lr: 0.0009993204620857817\n",
      "epoch: 70, acc: 0.974, loss: 0.087, lr: 0.0009993104757717174\n",
      "epoch: 71, acc: 0.973, loss: 0.088, lr: 0.0009993004896572402\n",
      "epoch: 72, acc: 0.974, loss: 0.085, lr: 0.0009992905037423429\n",
      "epoch: 73, acc: 0.974, loss: 0.086, lr: 0.0009992805180270205\n",
      "epoch: 74, acc: 0.975, loss: 0.084, lr: 0.0009992705325112669\n",
      "epoch: 75, acc: 0.975, loss: 0.081, lr: 0.0009992605471950758\n",
      "epoch: 76, acc: 0.976, loss: 0.080, lr: 0.0009992505620784412\n",
      "epoch: 77, acc: 0.977, loss: 0.077, lr: 0.0009992405771613573\n",
      "epoch: 78, acc: 0.976, loss: 0.079, lr: 0.0009992305924438182\n",
      "epoch: 79, acc: 0.976, loss: 0.078, lr: 0.0009992206079258179\n",
      "epoch: 80, acc: 0.977, loss: 0.076, lr: 0.0009992106236073502\n",
      "epoch: 81, acc: 0.977, loss: 0.074, lr: 0.0009992006394884093\n",
      "epoch: 82, acc: 0.978, loss: 0.072, lr: 0.0009991906555689891\n",
      "epoch: 83, acc: 0.978, loss: 0.072, lr: 0.0009991806718490836\n",
      "epoch: 84, acc: 0.978, loss: 0.072, lr: 0.000999170688328687\n",
      "epoch: 85, acc: 0.979, loss: 0.069, lr: 0.0009991607050077935\n",
      "epoch: 86, acc: 0.979, loss: 0.069, lr: 0.0009991507218863964\n",
      "epoch: 87, acc: 0.979, loss: 0.068, lr: 0.0009991407389644904\n",
      "epoch: 88, acc: 0.979, loss: 0.067, lr: 0.0009991307562420696\n",
      "epoch: 89, acc: 0.980, loss: 0.066, lr: 0.0009991207737191272\n",
      "epoch: 90, acc: 0.980, loss: 0.066, lr: 0.0009991107913956579\n",
      "epoch: 91, acc: 0.981, loss: 0.064, lr: 0.0009991008092716555\n",
      "epoch: 92, acc: 0.981, loss: 0.062, lr: 0.0009990908273471142\n",
      "epoch: 93, acc: 0.981, loss: 0.063, lr: 0.0009990808456220278\n",
      "epoch: 94, acc: 0.981, loss: 0.061, lr: 0.0009990708640963903\n",
      "epoch: 95, acc: 0.981, loss: 0.060, lr: 0.000999060882770196\n",
      "epoch: 96, acc: 0.981, loss: 0.060, lr: 0.0009990509016434388\n",
      "epoch: 97, acc: 0.983, loss: 0.058, lr: 0.0009990409207161126\n",
      "epoch: 98, acc: 0.982, loss: 0.058, lr: 0.0009990309399882115\n",
      "epoch: 99, acc: 0.983, loss: 0.056, lr: 0.0009990209594597295\n",
      "epoch: 100, acc: 0.982, loss: 0.057, lr: 0.0009990109791306607\n",
      "epoch: 101, acc: 0.983, loss: 0.054, lr: 0.0009990009990009992\n",
      "epoch: 102, acc: 0.983, loss: 0.055, lr: 0.0009989910190707385\n",
      "epoch: 103, acc: 0.983, loss: 0.054, lr: 0.0009989810393398733\n",
      "epoch: 104, acc: 0.984, loss: 0.054, lr: 0.0009989710598083974\n",
      "epoch: 105, acc: 0.984, loss: 0.052, lr: 0.0009989610804763047\n",
      "epoch: 106, acc: 0.984, loss: 0.052, lr: 0.0009989511013435892\n",
      "epoch: 107, acc: 0.985, loss: 0.050, lr: 0.000998941122410245\n",
      "epoch: 108, acc: 0.984, loss: 0.050, lr: 0.0009989311436762664\n",
      "epoch: 109, acc: 0.985, loss: 0.049, lr: 0.0009989211651416472\n",
      "epoch: 110, acc: 0.985, loss: 0.050, lr: 0.000998911186806381\n",
      "epoch: 111, acc: 0.986, loss: 0.046, lr: 0.0009989012086704624\n",
      "epoch: 112, acc: 0.985, loss: 0.047, lr: 0.0009988912307338854\n",
      "epoch: 113, acc: 0.985, loss: 0.047, lr: 0.0009988812529966439\n",
      "epoch: 114, acc: 0.986, loss: 0.045, lr: 0.0009988712754587315\n",
      "epoch: 115, acc: 0.986, loss: 0.046, lr: 0.0009988612981201431\n",
      "epoch: 116, acc: 0.987, loss: 0.044, lr: 0.0009988513209808721\n",
      "epoch: 117, acc: 0.986, loss: 0.045, lr: 0.0009988413440409126\n",
      "epoch: 118, acc: 0.986, loss: 0.044, lr: 0.0009988313673002587\n",
      "epoch: 119, acc: 0.986, loss: 0.044, lr: 0.0009988213907589046\n",
      "epoch: 120, acc: 0.987, loss: 0.042, lr: 0.000998811414416844\n",
      "epoch: 121, acc: 0.987, loss: 0.043, lr: 0.000998801438274071\n",
      "epoch: 122, acc: 0.987, loss: 0.042, lr: 0.00099879146233058\n",
      "epoch: 123, acc: 0.987, loss: 0.041, lr: 0.0009987814865863648\n",
      "epoch: 124, acc: 0.987, loss: 0.042, lr: 0.0009987715110414191\n",
      "epoch: 125, acc: 0.988, loss: 0.041, lr: 0.0009987615356957373\n",
      "epoch: 126, acc: 0.988, loss: 0.040, lr: 0.0009987515605493133\n",
      "epoch: 127, acc: 0.988, loss: 0.039, lr: 0.0009987415856021413\n",
      "epoch: 128, acc: 0.988, loss: 0.039, lr: 0.000998731610854215\n",
      "epoch: 129, acc: 0.988, loss: 0.038, lr: 0.000998721636305529\n",
      "epoch: 130, acc: 0.988, loss: 0.038, lr: 0.0009987116619560768\n",
      "epoch: 131, acc: 0.988, loss: 0.038, lr: 0.0009987016878058523\n",
      "epoch: 132, acc: 0.989, loss: 0.036, lr: 0.0009986917138548504\n",
      "epoch: 133, acc: 0.989, loss: 0.037, lr: 0.000998681740103064\n",
      "epoch: 134, acc: 0.989, loss: 0.036, lr: 0.0009986717665504878\n",
      "epoch: 135, acc: 0.989, loss: 0.035, lr: 0.000998661793197116\n",
      "epoch: 136, acc: 0.989, loss: 0.035, lr: 0.0009986518200429421\n",
      "epoch: 137, acc: 0.989, loss: 0.036, lr: 0.0009986418470879603\n",
      "epoch: 138, acc: 0.990, loss: 0.035, lr: 0.0009986318743321649\n",
      "epoch: 139, acc: 0.990, loss: 0.033, lr: 0.0009986219017755497\n",
      "epoch: 140, acc: 0.990, loss: 0.033, lr: 0.0009986119294181088\n",
      "epoch: 141, acc: 0.990, loss: 0.033, lr: 0.000998601957259836\n",
      "epoch: 142, acc: 0.990, loss: 0.033, lr: 0.000998591985300726\n",
      "epoch: 143, acc: 0.990, loss: 0.032, lr: 0.0009985820135407723\n",
      "epoch: 144, acc: 0.990, loss: 0.032, lr: 0.0009985720419799686\n",
      "epoch: 145, acc: 0.990, loss: 0.031, lr: 0.0009985620706183095\n",
      "epoch: 146, acc: 0.990, loss: 0.031, lr: 0.000998552099455789\n",
      "epoch: 147, acc: 0.991, loss: 0.031, lr: 0.000998542128492401\n",
      "epoch: 148, acc: 0.990, loss: 0.031, lr: 0.0009985321577281395\n",
      "epoch: 149, acc: 0.991, loss: 0.029, lr: 0.0009985221871629988\n",
      "epoch: 150, acc: 0.990, loss: 0.030, lr: 0.0009985122167969725\n",
      "epoch: 151, acc: 0.991, loss: 0.029, lr: 0.0009985022466300548\n",
      "epoch: 152, acc: 0.992, loss: 0.028, lr: 0.0009984922766622401\n",
      "epoch: 153, acc: 0.991, loss: 0.029, lr: 0.0009984823068935219\n",
      "epoch: 154, acc: 0.991, loss: 0.028, lr: 0.0009984723373238943\n",
      "epoch: 155, acc: 0.991, loss: 0.029, lr: 0.0009984623679533517\n",
      "epoch: 156, acc: 0.991, loss: 0.029, lr: 0.0009984523987818883\n",
      "epoch: 157, acc: 0.991, loss: 0.029, lr: 0.0009984424298094972\n",
      "epoch: 158, acc: 0.991, loss: 0.028, lr: 0.000998432461036173\n",
      "epoch: 159, acc: 0.991, loss: 0.027, lr: 0.0009984224924619103\n",
      "epoch: 160, acc: 0.991, loss: 0.028, lr: 0.0009984125240867022\n",
      "epoch: 161, acc: 0.991, loss: 0.027, lr: 0.000998402555910543\n",
      "epoch: 162, acc: 0.991, loss: 0.027, lr: 0.0009983925879334273\n",
      "epoch: 163, acc: 0.992, loss: 0.027, lr: 0.0009983826201553484\n",
      "epoch: 164, acc: 0.992, loss: 0.026, lr: 0.0009983726525763007\n",
      "epoch: 165, acc: 0.992, loss: 0.026, lr: 0.000998362685196278\n",
      "epoch: 166, acc: 0.992, loss: 0.024, lr: 0.000998352718015275\n",
      "epoch: 167, acc: 0.993, loss: 0.025, lr: 0.0009983427510332848\n",
      "epoch: 168, acc: 0.992, loss: 0.025, lr: 0.0009983327842503018\n",
      "epoch: 169, acc: 0.993, loss: 0.024, lr: 0.0009983228176663208\n",
      "epoch: 170, acc: 0.992, loss: 0.024, lr: 0.0009983128512813346\n",
      "epoch: 171, acc: 0.993, loss: 0.024, lr: 0.000998302885095338\n",
      "epoch: 172, acc: 0.992, loss: 0.025, lr: 0.0009982929191083246\n",
      "epoch: 173, acc: 0.993, loss: 0.025, lr: 0.000998282953320289\n",
      "epoch: 174, acc: 0.993, loss: 0.024, lr: 0.000998272987731225\n",
      "epoch: 175, acc: 0.993, loss: 0.022, lr: 0.0009982630223411264\n",
      "epoch: 176, acc: 0.993, loss: 0.023, lr: 0.0009982530571499876\n",
      "epoch: 177, acc: 0.993, loss: 0.023, lr: 0.0009982430921578022\n",
      "epoch: 178, acc: 0.993, loss: 0.023, lr: 0.0009982331273645647\n",
      "epoch: 179, acc: 0.994, loss: 0.021, lr: 0.000998223162770269\n",
      "epoch: 180, acc: 0.994, loss: 0.022, lr: 0.000998213198374909\n",
      "epoch: 181, acc: 0.994, loss: 0.021, lr: 0.0009982032341784787\n",
      "epoch: 182, acc: 0.993, loss: 0.021, lr: 0.0009981932701809723\n",
      "epoch: 183, acc: 0.993, loss: 0.022, lr: 0.0009981833063823842\n",
      "epoch: 184, acc: 0.993, loss: 0.021, lr: 0.0009981733427827076\n",
      "epoch: 185, acc: 0.993, loss: 0.021, lr: 0.0009981633793819372\n",
      "epoch: 186, acc: 0.993, loss: 0.021, lr: 0.0009981534161800669\n",
      "epoch: 187, acc: 0.994, loss: 0.020, lr: 0.0009981434531770906\n",
      "epoch: 188, acc: 0.994, loss: 0.020, lr: 0.0009981334903730024\n",
      "epoch: 189, acc: 0.994, loss: 0.020, lr: 0.0009981235277677965\n",
      "epoch: 190, acc: 0.994, loss: 0.020, lr: 0.000998113565361467\n",
      "epoch: 191, acc: 0.994, loss: 0.019, lr: 0.0009981036031540074\n",
      "epoch: 192, acc: 0.994, loss: 0.019, lr: 0.0009980936411454122\n",
      "epoch: 193, acc: 0.994, loss: 0.020, lr: 0.0009980836793356755\n",
      "epoch: 194, acc: 0.994, loss: 0.019, lr: 0.0009980737177247912\n",
      "epoch: 195, acc: 0.994, loss: 0.019, lr: 0.0009980637563127533\n",
      "epoch: 196, acc: 0.994, loss: 0.019, lr: 0.000998053795099556\n",
      "epoch: 197, acc: 0.995, loss: 0.018, lr: 0.0009980438340851932\n",
      "epoch: 198, acc: 0.994, loss: 0.019, lr: 0.0009980338732696588\n",
      "epoch: 199, acc: 0.994, loss: 0.019, lr: 0.0009980239126529471\n",
      "epoch: 200, acc: 0.994, loss: 0.018, lr: 0.0009980139522350524\n",
      "epoch: 201, acc: 0.994, loss: 0.019, lr: 0.000998003992015968\n",
      "epoch: 202, acc: 0.994, loss: 0.018, lr: 0.0009979940319956885\n",
      "epoch: 203, acc: 0.994, loss: 0.018, lr: 0.0009979840721742082\n",
      "epoch: 204, acc: 0.995, loss: 0.017, lr: 0.0009979741125515204\n",
      "epoch: 205, acc: 0.995, loss: 0.017, lr: 0.0009979641531276196\n",
      "epoch: 206, acc: 0.995, loss: 0.017, lr: 0.0009979541939024999\n",
      "epoch: 207, acc: 0.994, loss: 0.018, lr: 0.0009979442348761552\n",
      "epoch: 208, acc: 0.995, loss: 0.017, lr: 0.0009979342760485794\n",
      "epoch: 209, acc: 0.994, loss: 0.018, lr: 0.0009979243174197668\n",
      "epoch: 210, acc: 0.996, loss: 0.016, lr: 0.0009979143589897116\n",
      "epoch: 211, acc: 0.995, loss: 0.016, lr: 0.0009979044007584073\n",
      "epoch: 212, acc: 0.995, loss: 0.017, lr: 0.0009978944427258484\n",
      "epoch: 213, acc: 0.995, loss: 0.016, lr: 0.000997884484892029\n",
      "epoch: 214, acc: 0.995, loss: 0.016, lr: 0.0009978745272569427\n",
      "epoch: 215, acc: 0.995, loss: 0.016, lr: 0.000997864569820584\n",
      "epoch: 216, acc: 0.995, loss: 0.016, lr: 0.0009978546125829467\n",
      "epoch: 217, acc: 0.995, loss: 0.015, lr: 0.000997844655544025\n",
      "epoch: 218, acc: 0.995, loss: 0.016, lr: 0.0009978346987038126\n",
      "epoch: 219, acc: 0.996, loss: 0.015, lr: 0.0009978247420623042\n",
      "epoch: 220, acc: 0.996, loss: 0.015, lr: 0.0009978147856194934\n",
      "epoch: 221, acc: 0.995, loss: 0.016, lr: 0.0009978048293753743\n",
      "epoch: 222, acc: 0.995, loss: 0.016, lr: 0.0009977948733299407\n",
      "epoch: 223, acc: 0.996, loss: 0.015, lr: 0.0009977849174831871\n",
      "epoch: 224, acc: 0.996, loss: 0.015, lr: 0.0009977749618351078\n",
      "epoch: 225, acc: 0.996, loss: 0.014, lr: 0.000997765006385696\n",
      "epoch: 226, acc: 0.996, loss: 0.015, lr: 0.0009977550511349464\n",
      "epoch: 227, acc: 0.996, loss: 0.014, lr: 0.0009977450960828528\n",
      "epoch: 228, acc: 0.996, loss: 0.014, lr: 0.0009977351412294093\n",
      "epoch: 229, acc: 0.995, loss: 0.015, lr: 0.0009977251865746098\n",
      "epoch: 230, acc: 0.996, loss: 0.015, lr: 0.0009977152321184487\n",
      "epoch: 231, acc: 0.996, loss: 0.014, lr: 0.0009977052778609198\n",
      "epoch: 232, acc: 0.996, loss: 0.013, lr: 0.0009976953238020174\n",
      "epoch: 233, acc: 0.996, loss: 0.013, lr: 0.0009976853699417351\n",
      "epoch: 234, acc: 0.996, loss: 0.014, lr: 0.0009976754162800674\n",
      "epoch: 235, acc: 0.996, loss: 0.014, lr: 0.0009976654628170083\n",
      "epoch: 236, acc: 0.996, loss: 0.013, lr: 0.0009976555095525515\n",
      "epoch: 237, acc: 0.996, loss: 0.014, lr: 0.0009976455564866915\n",
      "epoch: 238, acc: 0.996, loss: 0.013, lr: 0.000997635603619422\n",
      "epoch: 239, acc: 0.996, loss: 0.013, lr: 0.0009976256509507371\n",
      "epoch: 240, acc: 0.996, loss: 0.013, lr: 0.0009976156984806313\n",
      "epoch: 241, acc: 0.996, loss: 0.013, lr: 0.0009976057462090984\n",
      "epoch: 242, acc: 0.996, loss: 0.013, lr: 0.0009975957941361318\n",
      "epoch: 243, acc: 0.996, loss: 0.013, lr: 0.0009975858422617266\n",
      "epoch: 244, acc: 0.996, loss: 0.013, lr: 0.0009975758905858764\n",
      "epoch: 245, acc: 0.997, loss: 0.012, lr: 0.0009975659391085751\n",
      "epoch: 246, acc: 0.996, loss: 0.013, lr: 0.0009975559878298169\n",
      "epoch: 247, acc: 0.996, loss: 0.012, lr: 0.0009975460367495962\n",
      "epoch: 248, acc: 0.996, loss: 0.013, lr: 0.0009975360858679064\n",
      "epoch: 249, acc: 0.997, loss: 0.012, lr: 0.0009975261351847418\n"
     ]
    }
   ],
   "source": [
    "# to store the logs of loss and accuracy\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "\n",
    "# Creating the dense layer with 28*28 inputs\n",
    "dense1 = layer_dense(784, 256)\n",
    "batchnorm1 = BatchNormalization(256)  # BatchNorm after first dense\n",
    "activation1 = relu()\n",
    "dropout1 = Dropout(rate=0.2)\n",
    "\n",
    "dense2 = layer_dense(256, 128)\n",
    "batchnorm2 = BatchNormalization(128)  # BatchNorm after second dense\n",
    "activation2 = relu()\n",
    "dropout2 = Dropout(rate=0.2)\n",
    "\n",
    "dense3 = layer_dense(128, 10)\n",
    "\n",
    "# Softmax loss activation\n",
    "loss_activation = activation_softmax_loss_categorical_cross_entropy()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.001, decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(250):\n",
    "    # Forward pass\n",
    "    dense1.forward(x_train.reshape(x_train.shape[0], -1))  # Flatten the input\n",
    "    # batchnorm1.forward(dense1.outputs, training=True)  # Apply Batch Norm\n",
    "    # activation1.forward(batchnorm1.outputs)\n",
    "    activation1.forward(dense1.outputs)\n",
    "    dropout1.forward(activation1.outputs, training=True)\n",
    "\n",
    "    dense2.forward(dropout1.outputs)\n",
    "    # batchnorm2.forward(dense2.outputs, training=True)  # Apply Batch Norm\n",
    "    # activation2.forward(batchnorm2.outputs)\n",
    "    activation2.forward(dense2.outputs)\n",
    "    dropout2.forward(activation2.outputs, training=True)\n",
    "\n",
    "    dense3.forward(dropout2.outputs)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_activation.forward(dense3.outputs, y_train)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    predictions = np.argmax(loss_activation.outputs, axis=1)\n",
    "    if len(y_train.shape) == 2:\n",
    "        y_train = np.argmax(y_train, axis=1)\n",
    "    accuracy = np.mean(predictions == y_train)\n",
    "\n",
    "    train_losses.append(loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "\n",
    "    print(f'epoch: {epoch}, ' +\n",
    "          f'acc: {accuracy:.3f}, ' +\n",
    "          f'loss: {loss:.3f}, ' +\n",
    "          f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.outputs, y_train)\n",
    "    dense3.backward(loss_activation.dinputs)\n",
    "    dropout2.backward(dense3.dinputs)\n",
    "    activation2.backward(dropout2.dinputs)\n",
    "    # batchnorm2.backward(activation2.dinputs)  # Backpropagate through BatchNorm\n",
    "    # dense2.backward(batchnorm2.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "\n",
    "    dropout1.backward(dense2.dinputs)\n",
    "    activation1.backward(dropout1.dinputs)\n",
    "    # batchnorm1.backward(activation1.dinputs)  # Backpropagate through BatchNorm\n",
    "    # dense1.backward(batchnorm1.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    # optimizer.update_params(batchnorm1)  # Update BatchNorm parameters\n",
    "    optimizer.update_params(dense2)\n",
    "    # optimizer.update_params(batchnorm2)  # Update BatchNorm parameters\n",
    "    optimizer.update_params(dense3)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    " Metric analysis\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.17%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 974    1    0    0    0    0    2    1    2    0]\n",
      " [   0 1125    3    1    0    1    2    0    3    0]\n",
      " [   2    0 1014    2    1    0    2    6    5    0]\n",
      " [   0    0    5  993    0    5    0    3    3    1]\n",
      " [   2    1    3    0  962    0    5    4    1    4]\n",
      " [   2    0    0   11    1  873    1    0    4    0]\n",
      " [   3    2    3    1    4    4  939    0    2    0]\n",
      " [   1    3    9    1    1    0    0 1008    0    5]\n",
      " [   2    1    3    6    3    3    2    4  947    3]\n",
      " [   2    3    0    4    9    4    1    4    0  982]]\n",
      "\n",
      "Per-class Accuracy:\n",
      "Digit 0: 99.39%\n",
      "Digit 1: 99.12%\n",
      "Digit 2: 98.26%\n",
      "Digit 3: 98.32%\n",
      "Digit 4: 97.96%\n",
      "Digit 5: 97.87%\n",
      "Digit 6: 98.02%\n",
      "Digit 7: 98.05%\n",
      "Digit 8: 97.23%\n",
      "Digit 9: 97.32%\n",
      "\n",
      "Total Misclassified Samples: 183\n"
     ]
    }
   ],
   "source": [
    "import cupy as np\n",
    "\n",
    "\n",
    "# Prepare test data (ensure it's in the right format)\n",
    "x_test = np.asarray(x_test).reshape(x_test.shape[0], -1)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "# Perform forward pass through the network\n",
    "def predict(X):\n",
    "    # Forward pass through the first layer and ReLU\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.outputs)\n",
    "    dropout1.forward(activation1.outputs, training=False)  # Inference mode\n",
    "\n",
    "    # Second layer and ReLU\n",
    "    dense2.forward(dropout1.outputs)\n",
    "    activation2.forward(dense2.outputs)\n",
    "    dropout2.forward(activation2.outputs, training=False)  # Inference mode\n",
    "\n",
    "    # Final layer with Softmax\n",
    "    dense3.forward(dropout2.outputs)\n",
    "    \n",
    "    # Get predictions (class with highest probability)\n",
    "    return np.argmax(dense3.outputs, axis=1)\n",
    "\n",
    "# Perform prediction\n",
    "predictions = predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Optional: Detailed classification report\n",
    "from collections import Counter\n",
    "\n",
    "# Confusion matrix\n",
    "def confusion_matrix(true, pred):\n",
    "    conf_matrix = np.zeros((10, 10), dtype=int)\n",
    "    for t, p in zip(true, pred):\n",
    "        conf_matrix[t, p] += 1\n",
    "    return conf_matrix\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_mat)\n",
    "\n",
    "# Per-class accuracy\n",
    "per_class_accuracy = conf_mat.diagonal() / conf_mat.sum(axis=1)\n",
    "print(\"\\nPer-class Accuracy:\")\n",
    "for digit, acc in enumerate(per_class_accuracy):\n",
    "    print(f\"Digit {digit}: {acc * 100:.2f}%\")\n",
    "\n",
    "# Misclassification analysis\n",
    "misclassified_indices = np.where(predictions != y_test)[0]\n",
    "print(f\"\\nTotal Misclassified Samples: {len(misclassified_indices)}\")\n",
    "\n",
    "# Optional: Visualize some misclassified samples\n",
    "def visualize_misclassifications(x_test, y_test, predictions, num_to_show=5):\n",
    "    misclassified_indices = np.where(predictions != y_test)[0]\n",
    "    \n",
    "    # Randomly select some misclassified samples\n",
    "    show_indices = np.random.choice(misclassified_indices, \n",
    "                                    min(num_to_show, len(misclassified_indices)), \n",
    "                                    replace=False)\n",
    "    \n",
    "    for idx in show_indices:\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')\n",
    "        plt.title(f\"True: {y_test[idx]}, Predicted: {predictions[idx]}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Uncomment to visualize misclassifications\n",
    "# visualize_misclassifications(x_test, y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Learning curve\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAGJCAYAAAC90mOkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABlDklEQVR4nO3dCbxM9f/H8Y99yy77HhEKEdEqopKilRZS8UsppZVCaVEqtPjl14J2qn+0SVmShIS0p5R9V9l3d/6P9/d0bnOve91t5s72ej4ex9w5c2bmzLnX+c7nfL7fzzdPIBAIGAAAAAAgXXnTfwgAAAAAIAROAAAAAJABAicAAAAAyACBEwAAAABkgMAJAAAAADJA4AQAAAAAGSBwAgAAAIAMEDgBAAAAQAYInAAAAAAgAwROAAAAAJABAickvPHjx1uePHls4cKFFguWLFliV111lVWrVs0KFSpkZcqUsXbt2tm4cePs0KFDkd49AEh4//3vf1270rJly0jvSkzauHGj3XHHHVa/fn0rWrSoFStWzJo1a2YPPfSQbd26NdK7hwSWP9I7ACDzXnzxRbvhhhusQoUKdvXVV1vdunVtx44dNmPGDLvuuuts/fr1NnDgwEjvJgAktNdff91q1qxpCxYssGXLllmdOnUivUsx4+uvv7bzzjvPdu7c6S4SKmASXdx89NFHbfbs2fbpp59GejeRoAicgBgxf/58FzS1atXKpkyZYsWLF09+7NZbb3WNyg8//BCS99q1a5e7wgcAyJrly5fb3Llz7d1337X//Oc/LogaMmSIRaNoO9crm9SlSxfLly+fffPNNy7jFOzhhx+2F154IS4/O2IDXfWATNJJ/Nxzz7USJUrYUUcdZW3btnXBTLADBw7YAw884DJBhQsXtrJly9qpp55q06ZNS95mw4YN1rNnT6tatarralepUiW78MILbcWKFUd8f72uun6oEQ4OmnzNmze3a665xv08a9Yst61ug+k9tF7dE316jj7P77//7q7y6bWvvPJK69u3r1u/e/fuw96rW7duVrFixRRdAz/++GM77bTTXEOk1+jYsaP9+OOPmTq2ABAvdI4uXbq0Owdecskl7n56QcJtt93mMlNqC9QmdO/e3bZs2ZK8zd69e+3++++3Y4891rUpai8uuugid74O1blevvjiC7v00kutevXqbl/UFVz7tmfPnsP2+5dffrHLLrvMjj76aCtSpIjVq1fP7r33XvfYZ5995t530qRJhz3vjTfecI/Nmzcv3WP3v//9z9auXWsjRow4LGgS9ba47777ku/r9XR8UtMx9dvD4C75n3/+ud14441Wvnx5d7zfeeed5PVp7YseC74gqc+u36m6yOv3oXb3/fffT/fzIP6QcQIyQQGAggIFTXfddZcVKFDAnVTPPPNMd8L1+7HrBD5s2DC7/vrrrUWLFrZ9+3aXCVq8eLGdffbZbpuLL77Yvd7NN9/sTu6bNm1ygdWqVavc/bQoeFF3vNNPP901bKF28OBB69ChgwvynnjiCdenXPsyevRo++ijj1yDGrwvH3zwgWuUdFVQXn31VevRo4d7jccee8xt89xzz7nXU8CZ3ucCgHijQEnBTcGCBd1FJp0L1f3spJNOSt5G3dDUpvz888927bXX2oknnugCJn0JX7NmjZUrV85dmDr//PPdub9r167Wr18/1zVb7YW+zB9zzDEhOdfL22+/7c7bffr0cRf81MXwmWeecfuix3zfffed22+1gb1793bndgViahOUDVKbqKBLx0CZo9THRfusXhPp0edXMKbgJBwUNCngGzx4sMs4KbhVMPnWW2/ZGWeckWLbiRMnWsOGDa1Ro0buvtrtU045xapUqWL33HOPu0io53Xu3Nn+7//+77DPizgVABLcuHHjAvqv8PXXX6e7TefOnQMFCxYM/P7778nr1q1bFyhevHjg9NNPT17XuHHjQMeOHdN9nb///tu91+OPP56lffz222/d8/r165ep7T/77DO3vW6DLV++3K3XZ/b16NHDrbvnnntSbJuUlBSoUqVK4OKLL06x/q233nLbz549293fsWNHoFSpUoFevXql2G7Dhg2BkiVLHrYeAOLVwoUL3flx2rRpyefRqlWrHnbuHjx4sNvu3XffPew19BwZO3as22bEiBHpbhOKc73s3r37sHXDhg0L5MmTJ7By5crkdWrv1O4FrwveHxkwYECgUKFCga1btyav27RpUyB//vyBIUOGBI6kdOnSrh3NLH2etF6zRo0a7vOmbudPPfXUwMGDB1Ns261bt0D58uVTrF+/fn0gb968gaFDhyava9u2beD4448P7N27N8Xnbt26daBu3bqZ3mfENrrqARnQVT8NRNVVpdq1ayevV5eJK664wubMmeMyS1KqVCl3Veq3335L87V0JU1XIdWt4u+//870Pvivn1YXvVDRlcZg6qKgTJPGU+nqaPBVOF1x0xVL0dVPdTnRlVVdMfUXZaOUiVPXDQBIBMqqqDtZmzZtks+jl19+uU2YMCFF12ZlKBo3bpxmlkLP8bdR5km9E9LbJhTner9t8ikTo3N469atdXHd9RqQzZs3u8IMypCl7vkQvD/qbrhv3z7XDS643VC2S8UeMmrrwtnO9erVK7mnhE+/H/X8CO7uqH1PSkpyj8lff/1lM2fOdF0UlfXz27k///zTZfDU5quLIeIfgROQATUW6sKgftypHXfcce7kunr1and/6NChLohQf/Tjjz/e7rzzTte1wae+4+rKpvFAalzV9W748OFu3NORqIug6IQdDvnz53f9vVNTo6E+7n4fbgVQCqQUUPkNpR8knnXWWa4LRPCigFMNEgDEOwVGCpAUNKlAhKrpadEFJJXXVpc7n7q3+V3A0qNt1O7o/Bzuc726iqv7tcbuqOuazt9+17Vt27a52z/++MPdZrTfGpukbonBY7v088knn5xhdUG1deFq56RWrVqHrTvnnHOsZMmSLrjz6ecmTZq4tlz0e1QQOWjQoMPaOb/wB21dYmCMExBCCoTU2L333nsuaFD58JEjR9qYMWPcuCe/Al6nTp1s8uTJ9sknn7gTscZF6WpW06ZN03xdNTZq8L7//vtM7Ud6VyPTm+dJAV3evIdfR1FDpz7s6set7Jr6sSuQ8q/CiQJHf5yTCkakFspGHwCilc7hmhJCwZOW1BQ8tG/fPqTvGYpzvbbVGFxlVe6++24X+Gj8jjIoCqb8c3xWKOukMVkaI6XskwopPfvssxk+T++tuQr379/vemdkV3qfPzizFnxM1KNEBS00/5aC3C+//NIeeeSR5G38Y6C5pZRhSgsl5xMD32iADOiKkgbQLl269LDHVGFHjZAGw/p0xU5V87QoQ6NgSkUj/MBJNED29ttvd4syNrqy9eSTT9prr72W5j7o/ZXRUcOs7Fbw+6VFFZ0k9USBK1euzPLnV9eEp556ynWh0FU4BVIKqII/i6hKkSbiBYBEpMBI50EV1UlNpcn1xVwX0fTlXefNjKaP0DZfffWVq9aqYgzhOtfrgtyvv/5qL7/8sgt4fMHVYMXvqp6ZaS9UzKJ///725ptvuott2v/gC27p0UVFVd1TN0V1/86IPn/qz66gSwFsVmjf9PmVFVTBDmWXgvfX/+z6HLRziY2uekAG1B9aVwmVRQouGa6rUiqvqrE+flc69XcOpi4PugqlK26iLn8qL5u6cVSfbn+b9Kg7gE7mmvg2eMyRb9GiRe7ELzVq1HD7rf7owXQ1LavUeGjf9NpTp051gVQwXX3T59fVOTXwaXV1BIB4puBAwZGq4KkiXOpF0zuoC5rf7VnVVb/99ts0y3Z7NQ+8bTSOJq1Mjb9NKM71/pgf/zX9n3XBLPVFRF0IHDt2rOval9b++DQ2S9N36GKgAkp1h9O6jGiuQo0f1kVFBXOpqTvcQw89lKL9TP3Zn3/++XQzTulRMKSLnro4qEVVcYO79SkgVsVAVdNNKyijnUscZJyAf6gxUGCQmrob6EStq28KklTOVN3PdAJVQKExSr4GDRq4k6tmOtdJWKXINchUjaaoIdD8Two+tK1eRw2ngjBdoTsSDdTVlUy9v7ozKIDSfFFqjDWoVQ2y36Cov7bGIamcrLpyqHH58MMPs9UHW2VyFfxpng593tRXDRU0qdyu9kfb6nOogVXDqlLmKt+amS4aABCrdP7VufiCCy5I83Fl6XVeVBChc6jGv6pt0HlaxRbUZqirnF5HWSkVjlD255VXXnGZG5UHVxlwFW6YPn26awc0/18ozvVqT/Q8dUNT9zyd05XxSauA0dNPP+3aQZ3rVY5cwYUuKOpcry52wbT/flnxBx98MFP7ogyS2kTNM6WeGComoWMjmtZDGazgcubqyaFgS0GmuhsqGFUX+MwEacGUSVIJeXWx1DFWqfbU1P7qs2v8sopMKAultlsZMnVJ1HsjAUS6rB8QaX6Z0vSW1atXu+0WL14c6NChQ+Coo44KFC1aNNCmTZvA3LlzU7zWQw89FGjRooUrz12kSJFA/fr1Aw8//HBg//797vEtW7YEbrrpJre+WLFirlx3y5YtXYnvzFq0aFHgiiuuCFSuXDlQoEABV75VZVJffvnlwKFDh5K327x5syslrn3VNv/5z38CP/zwQ5olarUvR3Lvvfe659WpUyfdbVQOV8dHn6lw4cKBY445JnDNNde48rwAEM86derkznu7du1KdxudD3XOVjsgf/75Z6Bv375u2gdNd6Gy5Tof+4/7ZcJ1/q1Vq5Z7bsWKFQOXXHJJiqkxQnGu/+mnnwLt2rVz7Vu5cuXcNBL+NBjBryF67S5durh2Tp+5Xr16gUGDBh32mvv27XP7ozZhz549WTqemu7jtttuCxx77LHuPfTZmjVr5trTbdu2JW+nNu/uu+92+6xt1AYtW7Ys3XLkR5p2RCXktY1KsPvtfmo67t27d3e/B/0+9Ls7//zzA++8806WPh9iVx79E+ngDQAAAPFD5ccrV67sxi299NJLkd4dICQY4wQAAICQUuVYjf0JLjgBxDoyTgAAAAgJVQLU/IUa16SxRhqbBMQLMk4AAAAICRUL6tOnj6tEp+IWQDwh4wQAAAAAGSDjBAAAAAAZIHACAAAAgAwk3AS4SUlJtm7dOitevLibLA4AkHvUO1wThapMcd68XLvz0TYBQPS3SwkXOKlhqlatWqR3AwAS2urVq61q1aqR3o2oQdsEANHfLiVc4KSref7BKVGiRKR3BwASyvbt212A4J+L4aFtAoDob5cSLnDyu0CoYaJxAoDIoDtaSrRNABD97RIdzAEAAAAgAwROAAAAAJABAicAAAAAyEDCjXECkLOSnQcPHrRDhw5FelcQpfLly2f58+dnDBMAIO4QOAHIlP3799v69ett9+7dkd4VRLmiRYtapUqVrGDBgpHeFQAAQobACUCmJudcvny5yyZogjh9ISajgLQykgqwN2/e7P5e6tatyyS3AIC4QeAEIEP6MqzgSfMcKJsApKdIkSJWoEABW7lypfu7KVy4sMWa2bNn2+OPP26LFi1yWdZJkyZZ586dj/icWbNmWf/+/e3HH390/0/uu+8+u+aaa3JtnwEA4celQACZRvYAifB3smvXLmvcuLGNHj06U9sru9axY0dr06aNLVmyxG699Va7/vrr7ZNPPgn7vgIAcg8ZJwAAgpx77rluyawxY8ZYrVq17Mknn3T3jzvuOJszZ46NHDnSOnToEMY9BQDkJgKnrPriC7ONG81OPtmsatVI7w0AIMLmzZtn7dq1S7FOAZMyT+nZt2+fW3zbt28P6z4CQE4FAjp3meXP7y2iIrsa8qzbP//01qnTwYEDZqoPdPTR3jrd37JF5zoVEDI76ijvNfS8gwf/XYLvH+mxtO63bm1WuXJ4jwGBU1YNHGg2Z47ZO+8QOAEJqGbNmu4L8ZG+FKce+6IuXH///beVKlUq7PuH3LdhwwarUKFCinW6r2Boz549btxXasOGDbMHHnggF/cSQHYChW3bzAoVMtuxw2zlSrMaNbzAYOdOb9m169+fCxQwq1JFBZW8QGHNGp0fvOdrOwULagb0mBZ92fdv9+4127TJe99ixbzX2rrV7K+/vOeVL+8FHho2quBF+6Vb7Y/e7++/vddQUKJ91X0te/bofOTd6lRUooT3OfR6S5earV3r7ZMWPxjS5w6+FT1/+fJ/Aye9pm71fH+GkuDtfdpOz9FnCbf33jO74ILwvgeBU1bpr1+CrhQCiD4ZVf0bMmSI3X///Vl+3a+//tqKqVXLpNatW7sCAyVLlrRwIkCLLQMGDHDFJHwKslRUAkg0+tKv07WCh/XrvS/o+/f/u+i+AgIFEspgKJDQovXBgYeCghUrvGBAQUrp0l62QwGCTtkKYPTFXgGOvsTrPf1F76N1CjQUnGjWDe2X1uv1452CMAVTmXXwoBcwpUXHU8c5Xz7vGKqTlk8BZ/Hi//6OUz/Pz2TpuWn9fKTH9LN+5+FG4JRV/rwkqX/jAKKKghXfxIkTbfDgwbZULeo/jtIlvKAy2prUVxO3ZuRov99BJql0e8WKFbP0HMQW/X43Bn87MH1Z2GglSpRIM9skhQoVcgsQbfSlV4GJsgsKMmrX9r6U6svyTz95i67N+FkPBRbKguiLtB/IKAjR1yX9iWudXkfbrl79b1ZCWREtCnL8AEZftKOV9k9BmP9fXf+1FZCpKfEXBVxqetSUKNBTtkWdk/SVUduqm5o+sx7T4m+nW79bm4ILvY6eo+OsYEDHU93g9LjWa1tdi9PvRQGP7ms7/R50PBWc6L6/TvusTJT/e6he3ctuqVtbo0beOr2Hf/z9647Bt9rPmjXNypb1PoMCUf2u9fn0/tpvPaZb/Y71HL3fzz97n13HTvujfRZ9DmWq/KAnVmoKEThlld/QETgh0enMGInJcHX2z8QcUsHBirI9ykD56/zszJQpU1zZ6O+//94+/fRTd8VfWYD58+e7ymoa5K8uVcHjV1J31dPrvvDCC/bRRx+5KmpVqlRxRQIu+Ke/QOpM0Pjx491zFczpdvXq1XbqqafauHHj3KSxcvDgQbcfr7zyips7SxXa1B1s27ZtNnny5GwdNr1/v3797IMPPnBja8444wx7+umn3VxLovLhffv2dUUNVEZcn1Mluc877zz3XD2mY7Rz506rWrWqDRw40Hr27JmtfYk3rVq1cn9LwaZNm+bWA7lBX2D15VNBjYIe/xqQsie//OJ9MVbWZv78f7/oKruiL8pffeVtry+1Cob0xT+3O9WoOdGiL+f6kq398xcFKPryr69d2q8yZbxFTYEfgOizK2BQYNG0qRc4KBBQcKBFX+B1+tfn1PP0fNHn999Xn1/BSLly3vP1RV7b677eV++h9X5XNT8ASEQlSx55tIrfRCuYPOmktLeJ1fnRCZyyyv9N01UPiU5BU1DWJteoBcxCV7kjueeee+yJJ56w2rVrW+nSpV0Qo0Dh4YcfdtkABS6dOnVymarqukSXDo1VGT58uAs0nnnmGbvyyitdIFLGb51T2b17t3vfV1991ZXuvuqqq+yOO+6w119/3T3+2GOPuZ8VTCl4e+qpp1zApAAsuzSn0G+//Wbvv/++y4Tcfffd7rP+9NNPbt6lm266yQVMmsNIXRG13s/KDRo0yN3/+OOPrVy5crZs2TI3dideKTjUZwwuN64y4/p96u9A3ezWrl3r/j7khhtusGeffdbuuusuu/baa23mzJn21ltvuWAayGoApOyAvtD/+KPX7UxX9/WFXRkdBUV+oDBxovcFXl9SdfVfX09yck3XH1/jU7ZEpwBlNPxuVAo+mjf3ghQFM/oCrf3R6UDZC+2DH4TpsyhY0329jj6DeqPqvoIV3fcXP3BSViIaMw/BiWMSxYmNwCmr6KoHxI2hQ4fa2WefnXxfX4w1f4/vwQcfdJOfKthQxuVIQUm3bt3cz4888ojL5CxYsMDOOeecNLc/cOCAK2F9zDHHuPt6be2LT8GXvpx36dLF3deX8tQZjazwA6Yvv/zSjbkSBWbKsCkgu/TSS23VqlV28cUX2/HHH+8eVzDp02NNmza15vrG9E/WLZ4tXLgwRZDqj0Xq0aOHyxiqG6iOiU+lyBUk3XbbbS7IVUbuxRdfpBR5glKWR0GDggwFI7//7gUWulVtKQURCkS++cbshx+87IcCJV2LUpbEH2ifVfpaomyKghN1j9Nr6rV1mlEmRvdbtvQCMwUpyqRoP0880Qt+/C5kSnxr32Nw7mog7AicsoriEIDH7zAdifcNET8QCM40qGCEvgTry7G6zCmzEvwlOS0nnHBC8s/K1iijsyn15dsgRYsWTQ6aRF30/O3VHU/jY1q0aJH8uLrrNWvWzJKyOQDg559/duO3Wupb0z/Kli1r9erVc4/JLbfcYn369HHd8dQ1UUGU/7m0XvcXL15s7du3t86dOycHYPHozDPPdOPe0qPgKa3nfKNvwohbGp+ibm0KUJRpmTHj3+pp6vamrM/ixbrQ4G2f2QyQApZgfhcwXbto0sQb56KAShkdZWT86mvnn+9lhfSnWquWt38KmmK1CxQQCwicsoqME+DRJdMQdZmLlNTV8dRdTmNT1I2uTp06bmD/JZdc4rqwHYm6ugXTuKcjBTlpbX+kL+q5QeOolCFR0KjgSWO7NFbr5ptvdpPBquuhsl46Pm3btnVd+3ScgHig/34qXPDHH17go+sJyhIpQNE6LZs3Z/711N1Mpw2dJhXMqAS0uqQpwe0PnNd6TQmp60/qvqbuYAqSlPHJTne1LNatAZANBE5ZRXEIIG6pK5u63fld5JSBWqH6trlIhSw0B5DKnp9++ulunSr+KdvTRJefs0HjpJQ9++qrr5IzRX/++acbu9WgQYPk7dR1T+N1tKiroIpeKHDyqwmqq5qW0047ze68804CJ8QUdV9TUOSXrtYQNBVLUNe6hQtTlk1Oj3qp6rnKLrVv73WF8wsLqKKYerqqOIHWaRvN6cOYGCB+EDhlFcUhgLilCnPvvvuuKwihLJCKImS3e1xOKFhRxkdZr/r167sxT6psl9HcVKIKgcV1afsfeo7GbV144YXWq1cv+9///uceV2EMVQDUelGFP2WWjj32WPden332mQu4RKXc1VWwYcOGriLfhx9+mPwYEG30X/a77/6tJqfskcYSLVhweLe41F3k1INWwY6uJ+i/kT9GSN3m1B0uK9OxBQ0TBBAnCJyyiowTELdGjBjhqqIpK6Pqcao8p4lJc5veV+XHu3fv7sY39e7d23Wj088Z8bNUPj1H2SZV6FM58vPPP991PdR26nrndxtUVkvd79asWePGaKmwxciRI5PnolIGStk3dV9UxmnChAlh+vRA5ihz5Hev05xDyh6phopfnCEtKnig65/qHqdK/KrponFDqgmjhG4Ih1ACiEN5ApHuWJ/L9CVIXWE0AFtfDrLsoYdUm9esVy+z558Pxy4CUWfv3r2uJLOqhxWm1FKuU9ZLGZ7LLrvMVfqL5b+XHJ+D4xTH5cjUyUNd69as8SYCVZGGmTPTD5A0fFHV4pQhOvZYs4YNvS50qnei6w/+OCMA2J6F8y8Zp6yiOASAMFMhBhVo0CS16hqncuQKRK644opI7xqQK1SS+8svva52msvouee8zFJanUA07kjd6OrVM+vUycskqcBCqhosKRA0AcgOAqesohw5gDDTpLgqea0qf+oU0KhRI5s+fTrjihDXliwxGzfOK9SgIEnFFYJpbqHTTvPmH1KwdMEFXjYpGidMBRCfCJyyiowTgDBTdTtV+APinaa+ev11s+nTzb79NuVjKsxQp443BkllvK+91ps8FgAihcApqygOAQBAtqnr3aOPmv34o9nXX6e8Lqkijxdd5E3sqsr5lPIGEE0InLKKcuRIYAlWSwbZxN8JUlMRh8WLzcaO9brj+UUdNNbossvMzjvPrGNHby4kAIhWBE5ZRVc9JCC/ZPXu3btdOWrgSPR3Evx3g8SkuZTeestszhxvDqU9e/59TEFSz55epTvmOwIQKwicsoriEEhAmguoVKlStmnTJne/aNGimZqMFYmXaVLQpL8T/b1kZt4pxBclG1Um/MknzT7++PAxS2edZXbLLV6RBwCINQROWUXGCQmqokpamSUHT0B6FDT5fy9IjIloVQlvxgyzt9/+t8iDqt117mx2zjlmp57qlQunAh6AWEbglFUUh0CCUoapUqVKVr58eTuQ3qyTSHjqnkemKXG8/75Znz5m69b9u65oUa8C3q23mh1zTCT3DgBCi8ApqygOgQSnL8V8MQYS29atZv36mb3yine/dGmzNm3M2rY169rV65YHAPGGwCmryDgBABKUks3KMiloWrvWq4p3xx1mQ4eaFS4c6b0DgPAicMoqMk4AgAQs+vDqq2Z33qlxjt66unXNxo/35lsCgERA4JRVFIcAACSQlSvNevc2+/RT774mp9UYpvvu88YzAUCiIHDKKsqRAwASxI8/eiXElWVS83f//Wa3364iIJHeMwDIfQROWUXGCQCQAD75xOyqq8y2bDFr0sRs4kSzY4+N9F4BQOQQOGU346QRsur0zSSgAIA4o7FLPXt6Pzdr5nXTo1IegETHVHTZzTgJWScAQJz5+muzG27wfu7Vy+yLLwiaAEAInLKKwAkAEKdmzzZr394bxnvBBWZjxpgVKRLpvQKA6EDglJPAiQIRAIA4sXGjWceO3uS2p5zilR/Py7cEAEjGGKesypfPWw4dIuMEAIgbDz9stnOnN6Zp2jQyTQCQGteSclIggsAJABAHVq3yuuXJY48RNAFAWgicctJdj656AIA48MorXrHY0083a9s20nsDANGJwCk7yDgBAOKI5miSa66J9J4AQPQicMoOMk4AgDjx009mP/xgVqCAWefOkd4bAIheBE45CZzIOAEAYtjUqWaXXur9rDLkpUtHeo8AIHpRVS8nXfXIOAEAYtS6dWYXX2y2e7dXDOKeeyK9RwAQ3cg4ZQcZJwBAjLv/fi9oatnSbPVqs1NPjfQeAUB0i2jgNGzYMDvppJOsePHiVr58eevcubMtXbo0w+e9/fbbVr9+fStcuLAdf/zxNmXKFMtVFIcAAMR4tumll7yfR4wwK1s20nsEANEvooHT559/bjfddJPNnz/fpk2bZgcOHLD27dvbrl270n3O3LlzrVu3bnbdddfZN99844ItLT9oZGtuoTgEACCGLVlilpRk1rChWevWkd4bAIgNER3jNFWjUoOMHz/eZZ4WLVpkp2syiTQ89dRTds4559idd97p7j/44IMu6Hr22WdtjD97X7jRVQ8AEMN++cW7bdAg0nsCALEjqsY4bdu2zd2WKVMm3W3mzZtn7dq1S7GuQ4cObn1a9u3bZ9u3b0+x5BjFIQAAcRA41a8f6T0BgNgRNYFTUlKS3XrrrXbKKadYo0aN0t1uw4YNVqFChRTrdF/r0xtHVbJkyeSlWrVqOd9ZMk4AgBj288/e7XHHRXpPACB2RE3gpLFOGqc0YcKEkL7ugAEDXCbLX1ardFBOURwCABDDyDgBQIzO49S3b1/78MMPbfbs2Va1atUjbluxYkXbuHFjinW6r/VpKVSokFtCiuIQAIAYtWWLt8ixx0Z6bwAgdkQ04xQIBFzQNGnSJJs5c6bVqlUrw+e0atXKZsyYkWKdikNofa4h4wQAiFH+rB/Vq5sVKxbpvQGA2JE/0t3z3njjDXvvvffcXE7+OCWNRSqiaczNrHv37lalShU3Vkn69etnZ5xxhj355JPWsWNH17Vv4cKF9vzzz+fejpNxAgDEoPnzzW691fuZbnoAEEMZp+eee86NOzrzzDOtUqVKycvEiROTt1m1apWtX78++X7r1q1dsKVAqXHjxvbOO+/Y5MmTj1hQIuQoDgEAiEH33mu2cKH3c5cukd4bAIgt+SPdVS8js2bNOmzdpZde6paIoRw5ACAG/fGHd/vRR2bnnRfpvQGA2BI1VfViChknAECMOXTIbO1a7+fjj4/03gBA7CFwyg6KQwAAYowK0h44YJYvn1mlSpHeGwCIPQRO2UFxCABAjFm1yrutUsUsf1RMRgIAsYXAKTsKFPBudekOAIAYCpxUhhwAkHUETjnJOBE4AQBiBIETAOQMgVNOMk6McQIAxAgCJwDIGQKn7KCrHgAgxhA4AUDOEDhlB+XIASCujR492mrWrGmFCxe2li1b2oIFC464/ahRo6xevXpWpEgRq1atmt122222d+9eixbff2/27bfezwROAJA91NXJDjJOABC3Jk6caP3797cxY8a4oElBUYcOHWzp0qVWvnz5w7Z/44037J577rGxY8da69at7ddff7VrrrnG8uTJYyNGjLBIU8zXsuW/9wmcACB7yDhlBxknAIhbCnZ69eplPXv2tAYNGrgAqmjRoi4wSsvcuXPtlFNOsSuuuMJlqdq3b2/dunXLMEuVW376KeV1vxo1Irk3ABC7CJyyg4wTAMSl/fv326JFi6xdu3bJ6/Lmzevuz5s3L83nKMuk5/iB0h9//GFTpkyx8847L9332bdvn23fvj3FEi6bN3u3JUuaffCBWYkSYXsrAIhrdNXLDjJOABCXtmzZYocOHbIKFSqkWK/7v/zyS5rPUaZJzzv11FMtEAjYwYMH7YYbbrCBAwem+z7Dhg2zBx54wHKDHzhde61Zhw658pYAEJfIOGUHGScAwD9mzZpljzzyiP33v/+1xYsX27vvvmsfffSRPfjgg+k+Z8CAAbZt27bkZfXq1WEPnMqVC9tbAEBCIOOUHWScACAulStXzvLly2cbN25MsV73K1asmOZzBg0aZFdffbVdf/317v7xxx9vu3btst69e9u9997ruvqlVqhQIbfkBj9wOvroXHk7AIhbZJyyg4wTAMSlggULWrNmzWzGjBnJ65KSktz9Vq1apfmc3bt3HxYcKfgSdd2LNAInAAgNMk45yTgROAFA3FEp8h49eljz5s2tRYsWrhy5Mkiqsifdu3e3KlWquHFK0qlTJ1eJr2nTpq58+bJly1wWSuv9ACqStmzxbgmcACBnCJxyknGiqx4AxJ3LL7/cNm/ebIMHD7YNGzZYkyZNbOrUqckFI1atWpUiw3Tfffe5OZt0u3btWjv66KNd0PTwww9bNCDjBAChkScQDf0IcpFKvpYsWdINxi2R3Zqsv/9uVqeOWfHiesFQ7yIAxK2QnIPjULiOy759ZoULez//9ZdZ6dIhe2kASLjzL2OcsoOMEwAgBvjZpvz5zUqVivTeAEBsI3DKDsY4AQBiQHAp8jx5Ir03ABDbCJxyknFKSjI7dCjSewMAQJoY3wQAoUPglB1+xknIOgEAohST3wJA6BA45STjJIxzAgBEKTJOABA6BE45DZzIOAEAohSBEwCEDoFTdmhCQ38ODzJOAIAo9McfZq++6v1crVqk9wYAYh+BU06zTmScAABR6KqrzFavNqtf3+y66yK9NwAQ+wiccloggowTACDKHDxo9vXX3s+TJ1McAgBCgcApu8g4AQCi1KpVXvBUqJBZ3bqR3hsAiA8ETtlFxgkAEKWWLfNujznm3yG5AICc4XSaXWScAABRHjjVqRPpPQGA+EHglF1knAAAUYrACQBCj8Apu8g4AQCiFIETAIQegVNOM04ETgCAKEPgBAChR+CU04wTXfUAAFEkKcmb/FYInAAgdAicsouMEwAgCq1da7Zvn3d9r1q1SO8NAMQPAqfsIuMEAIhCmzZ5t+XLm+XPH+m9AYD4QeCUXWScAABRaNs277ZkyUjvCQDEFwKn7CLjBACIQtu3e7cETgAQWgRO2UXGCQAQxRmnEiUivScAEF8InLKLjBMAIAqRcQKA8CBwyi4mwAUARHHgRMYJAEKLwCmnXfXIOAEAoghd9QAgPAicsouMEwAgCtFVDwDCg8Apu8g4AQCiEBknAAgPAqfsIuMEAIhCZJwAIDwInLKLjBMAIAqRcQKA8CBwyi4yTgCAKETGCQDCg8Apu8g4AQCiEBknAAgPAqfsIuMEAIhCzOMEAHEYOM2ePds6depklStXtjx58tjkyZOPuP2sWbPcdqmXDRs2WMQyTgROAIAoEQjQVQ8A4jJw2rVrlzVu3NhGjx6dpectXbrU1q9fn7yUL1/eIpZxoqseACBK7NpllpTk/UzGCQBCK79F0LnnnuuWrFKgVKpUKYsoMk4AgCjjZ5vy5TMrWjTSewMA8SUmxzg1adLEKlWqZGeffbZ9+eWXR9x23759tn379hRLSJBxAgBEcWGIPHkivTcAEF9iKnBSsDRmzBj7v//7P7dUq1bNzjzzTFu8eHG6zxk2bJiVLFkyedFzQoKMEwAgyjC+CQDitKteVtWrV88tvtatW9vvv/9uI0eOtFdffTXN5wwYMMD69++ffF8Zp5AET2ScAABRhlLkABA+MRU4paVFixY2Z86cdB8vVKiQW0KOjBMAIMpQihwAwiemuuqlZcmSJa4LX64j4wQAiDJ01QOAOM047dy505YtW5Z8f/ny5S4QKlOmjFWvXt11s1u7dq298sor7vFRo0ZZrVq1rGHDhrZ371578cUXbebMmfbpp5/m/s4zAS4AIMrQVQ8A4jRwWrhwobVp0yb5vj8WqUePHjZ+/Hg3R9OqVauSH9+/f7/dfvvtLpgqWrSonXDCCTZ9+vQUr5Fr/K56ZJwAAFFi3z7vtnDhSO8JAMSfiAZOqogX0DTn6VDwFOyuu+5yS1Qg4wQAiDJ+k+Q3UQCA0In5MU4RQ8YJABBlCJwAIHwInLKLjBMAIMr4TVL+mK+ZCwDRh8Apu8g4AUDUqFmzpg0dOjTFuNhEdPCgd0vGCQBCj8Apu/y5ofyRuACAiLn11lvt3Xfftdq1a9vZZ59tEyZMsH0JeH6mqx4AhA+BU3aRcQKAqAqcNJ3FggUL7LjjjrObb77ZzfHXt29fW7x4sSUKAicACB8Cp+wi4wQAUefEE0+0p59+2tatW2dDhgxx8/2ddNJJ1qRJExs7duwRK7nGU1c9xjgBQOhxas1pxikpyezQIbN8+SK9RwCQ8A4cOGCTJk2ycePG2bRp0+zkk0+26667ztasWWMDBw50c/+98cYbFq/IOAFA+BA45TTj5GedihaN5N4AQEJTdzwFS2+++ablzZvXunfvbiNHjrT69esnb9OlSxeXfYpnBE4AED501ctpxkkY5wQAEaWA6LfffrPnnnvO1q5da0888USKoElq1aplXbt2zdTrjR492lXqK1y4sLVs2dKNnTqSrVu32k033eTGVRUqVMiOPfZYmzJliuU2AicACB8yTtkV3CoxzgkAIuqPP/6wGjVqHHGbYsWKuaxURiZOnGj9+/e3MWPGuKBp1KhR1qFDB1u6dKmVL1/+sO3379/vKvnpsXfeeceqVKliK1eutFKlSlluY4wTAIQPp9bsypPHyzop20TGCQAiatOmTbZhwwYX6AT76quvLF++fNa8efNMv9aIESOsV69e1rNnT3dfAdRHH33kikvcc889h22v9X/99ZfNnTvXCvxzUU3Zqkgg4wQA4UNXvZygsh4ARAV1k1u9evVh69VtT49llrJHixYtsnbt2iWv05gp3Z83b16az3n//fetVatW7n0qVKhgjRo1skceecQOqXBQOjTH1Pbt21MsoUDgBADhQ+AUisCJjBMARNRPP/3kSpGn1rRpU/dYZm3ZssUFPAqAgum+MlrpdRNUFz09T+OaBg0aZE8++aQ99NBD6b7PsGHDrGTJkslLtWrVLBQInAAgfAicQlEggowTAESUCjJs3LjxsPXr16+3/GEe8JOUlOTGNz3//PPWrFkzu/zyy+3ee+91XfzSM2DAANu2bVvykla2LDsY4wQA4UPglBN01QOAqNC+ffvkYCS40p3mblLhhswqV66cGxOVOgjT/YoVK6b5HFXSUxU9Pc933HHHuQyVuv6lF+iVKFEixRIKZJwAIHwInEKRcaKrHgBElMqPK2ujynpt2rRxi8qPK3hRt7nMKliwoMsazZgxI0VGSfc1jiktp5xyii1btsxt5/v1119dQKXXy00ETgAQPgROOUHGCQCigkqAf/fddzZ8+HBr0KCBC36eeuop+/7777M8fkilyF944QV7+eWX7eeff7Y+ffrYrl27kqvsaXJdZbd8elxV9fr16+cCJlXgU3GIrBSlCHXgRFc9AAg9Tq05QcYJAKKG5mnq3bt3jl9HY5Q2b95sgwcPdhmrJk2a2NSpU5MLRqxatcpV2vMpMPvkk0/stttusxNOOMEFcQqi7r77bstt/hgnMk4AEHoETjlBxgkAoooq6CmwST226IILLsjS6/Tt29ctaZk1a9Zh69SNb/78+RZpdNUDgCgLnNSPPE+ePFa1alV3f8GCBfbGG2+47hGhuNoXM8g4AUBUUEnwLl26uK55ap8CgYBbr5/lSHMqxRMCJwCIsjFOV1xxhX322WfuZ3VjUMUiBU8qvzp06FBLGGScACAqqGucikFs2rTJihYtaj/++KPNnj3bmjdvnmaGKF5RjhwAoixw+uGHH6xFixbu57feesvNkj537lx7/fXXbfz48ZYwyDgBQFSYN2+eu3CncuIaf6Tl1FNPdRPN3nLLLZYoyDgBQJQFTgcOHHBzUMj06dOT+47Xr1/fTTaYMMg4AUBUUFe84sWLu58VPK1bt879rPLkS5cutURB4AQAURY4NWzY0M2I/sUXX9i0adPsnHPOcevVUJUtW9YSBoETAEQF9Xz49ttv3c8tW7Z0Zcm//PJLl4WqXbu2JQoCJwCIssDpscces//973925plnWrdu3axx48Zu/fvvv5/chS8h0FUPAKLCfffdlzwBrYKl5cuX22mnnWZTpkyxp59+2hIFY5wAIHyydWpVwLRlyxbbvn27lS5dOnm9KuppUG7CIOMEAFGhQ4cOyT/XqVPHfvnlFzcprdoov7JeIiDjBABRlnHas2eP7du3LzloWrlypY0aNcr1Iy9fvrwlDDJOABBxGnebP39+V7goWJkyZRIqaBICJwCIssDpwgsvtFdeecX9vHXrVtef/Mknn7TOnTvbc889ZwmDjBMARFyBAgWsevXqCTNXU3rUU/Gf3ooETgAQLYHT4sWLXd9xeeedd6xChQou66RgKpH6kpNxAoDooHkEBw4c6LrnJSp/fJMwxgkAQi9bp9bdu3cnl3399NNP7aKLLnJzZpx88skugEoYZJwAICo8++yztmzZMqtcubIrQV6sWLHDLvglSjc9IeMEAFESOGng7eTJk61Lly72ySef2G233ebWa8b2EiVKWMIg4wQAUUFdxRMdgRMARGHgNHjwYLviiitcwHTWWWdZq1atkrNPTZs2tYRBxgkAosKQIUMs0QUHTnTVA4DQy9ap9ZJLLrFTTz3V1q9fnzyHk7Rt29ZloRIGGScAQJSNccqb11sAAKGV7WtSFStWdMuaNWvc/apVqybW5LdCxgkAooLG2R6p9HgiVNyjFDkARGHgpNnZH3roIVeCfOfOnW6dikXcfvvtrrKRGrCEQOAEAFFh0qRJh83t9M0339jLL79sDzzwgCUCAicAiMLAScHRSy+9ZI8++qidcsopbt2cOXPs/vvvt71799rDDz9sCYGuegAQFTS/YFrdyhs2bGgTJ0606667zhKlqx7jmwAgPLJ1etUVvBdffNEuuOCC5HUnnHCCValSxW688cbECZzIOAFAVNM0Gb1797ZEQMYJAMIrW33qNMFg/fr1D1uvdQk1+SAZJwCIWnv27HGTsuuiXiIgcAKAKMw4qZKeJhtUgxRM65R5ShhknAAgKpQuXTpFcYhAIGA7duywokWL2muvvRbRfcstBE4AEIWB0/Dhw61jx442ffr05Dmc5s2bZ6tXr7YpU6ZYwiDjBABRYeTIkSkCJxUpOvroo61ly5YuqEoEjHECgPDK1un1jDPOsF9//dVGjx5tv/zyi1t30UUXuX7kqrZ32mmnWUIg4wQAUeGaa66xREfGCQDCK9vXpSpXrnxYEYhvv/3WVdt7/vnnLSGQcQKAqDBu3Dg76qij7NJLL02x/u2337bdu3dbjx49LN4ROAFAeCXIhEthQsYJAKLCsGHDrFy5coetL1++vD3yyCOWCAicACC8CJxygsAJAKLCqlWrrFatWoetr1GjhnssETDGCQDCi8ApJ+iqBwBRQZml77777rD16kJetmxZSwRknAAgvLJ0XUoFII5k69atlpAZp0OHvCVfvkjvEQAkpG7dutktt9xixYsXt9NPP92t+/zzz61fv37WtWtXSwQETgAQRYFTyZIlM3y8e/fulnAZJz/rVKRIJPcGABLWgw8+aCtWrLC2bdta/n/6qiUlJbk2iTFOAIBcD5xUtQhpZJz8cU4ETgAQEQULFrSJEye6KTGWLFliRYoUseOPP96NcUoUjHECgPDi9JoTwZf1GOcEABFXt25dtyQiMk4AEMfFIWbPnm2dOnVyc0JpxvfJkydn+JxZs2bZiSeeaIUKFbI6derY+PHjLWI0S73fXY/KegAQMRdffLE99thjh60fPnz4YXM7xSsCJwCI48Bp165d1rhxYxs9enSmtl++fLl17NjR2rRp47pi3HrrrXb99dfbJ598YhFDZT0AiDhdiDvvvPMOW3/uuee6xxKpqx6BEwDEYVc9NWhaMmvMmDFuno4nn3zS3T/uuONszpw5NnLkSOvQoUOaz9m3b59bfNu3b7eQj3PauZOMEwBE0M6dO904p9QKFCgQ+vN+lGecGOMEAOERU/M4zZs3z9q1a5dinQImrT/SbPKq9ucv1apVC0+BiL17Q/u6AIBMUyEIFYdIbcKECdagQQNLBHTVA4DwiqnrUhs2bLAKFSqkWKf7upq4Z88eV0UptQEDBlj//v2T72vbkAZPxYp5t7t3h+41AQBZMmjQIDfX4O+//25nnXWWWzdjxgx744037J133rFEQOAEAOEVU4FTdqiIhJawOeoo71bd9QAAEaFCQyowpDmbFCjpQprG0M6cOdPKlCljiYBy5AAQXjF1eq1YsaJt3LgxxTrdL1GiRJrZplxRvLh3u2NHZN4fAOCoeJAWv3fBm2++aXfccYctWrTIDh06ZPGOjBMAhFdMjXFq1aqV63oRbNq0aW59xJBxAoCooQp6PXr0cNNcqJCQuu3Nnz/fEgGBEwDEccZJVZCWLVuWoty4yoyrW0X16tXd+KS1a9faK6+84h6/4YYb7Nlnn7W77rrLrr32WtcF46233rKPPvooch+CwAkAIj7+VXP6vfTSSy7TdNlll7lqquq6lyiFIYTACQDiOOO0cOFCa9q0qVtERRz08+DBg9399evX26pVq5K3VylyBUnKMqnvuq4mvvjii+mWIs8VBE4AENGxTfXq1bPvvvvORo0aZevWrbNnnnnGEhFjnAAgvCJ6ej3zzDMtEAik+7iuIKb1nG+++caihh84McYJAHLdxx9/bLfccov16dPH6tata4mMjBMAhFdMjXGKSn5xCDJOAJDrNAn6jh07rFmzZtayZUvXnXvLli2WiAicACC8CJxyiq56ABAxJ598sr3wwguua/d//vMfN+GtCkMkJSW5bt0KqhIFgRMAhBeBU04ROAFAxBUrVswVDVIG6vvvv7fbb7/dHn30UStfvrxdcMEFlggY4wQA4UXglFMETgAQVVQsYvjw4bZmzRo3l1OiIOMEAOFF4JRTTIALAFEpX7581rlzZ3v//fctERA4AUB4ETjlFBknAEAUddUjcAKA8CBwyikCJwBAFGWcGOMEAOFB4JRTBE4AgChAcQgACC8Cp5wicAIARAECJwAILwKnUE6Am5QU6b0BAITA6NGjrWbNmla4cGE3se6CBQsy9TzNI5UnTx5XlCK3HTrk3ebLl+tvDQAJgcApVBkn2b07knsCAAiBiRMnWv/+/W3IkCG2ePFia9y4sXXo0ME2bdp0xOetWLHC7rjjDjvttNMsEgicACC8CJxyqkgRszx5vJ/prgcAMW/EiBHWq1cv69mzpzVo0MDGjBljRYsWtbFjx6b7nEOHDtmVV15pDzzwgNWuXTtX9/ffffBuCZwAIDwInHJKQRPjnAAgLuzfv98WLVpk7dq1S16XN29ed3/evHnpPm/o0KFWvnx5u+666zL1Pvv27bPt27enWHKKMU4AEF4ETqEe5wQAiFlbtmxx2aMKFSqkWK/7GzZsSPM5c+bMsZdeesleeOGFTL/PsGHDrGTJkslLtWrVcrzvZJwAILwInELBzzjt2BHpPQEA5KIdO3bY1Vdf7YKmcuXKZfp5AwYMsG3btiUvq1evzvG+EDgBQHiR0A8FuuoBQFxQ8JMvXz7buHFjivW6X7FixcO2//33311RiE6dOiWvS/qnwmr+/Plt6dKldswxxxz2vEKFCrkllAicACC8yDiFAoETAMSFggULWrNmzWzGjBkpAiHdb9Wq1WHb169f377//ntbsmRJ8nLBBRdYmzZt3M+h6IKXWQROABBeZJxCgcAJAOKGSpH36NHDmjdvbi1atLBRo0bZrl27XJU96d69u1WpUsWNU9I8T40aNUrx/FKlSrnb1OvDjcAJAMKLwCkUKA4BAHHj8ssvt82bN9vgwYNdQYgmTZrY1KlTkwtGrFq1ylXaizZU1QOA8OL0GgolS3q369dHek8AACHQt29ft6Rl1qxZR3zu+PHjLRLIOAFAeEXfJbNY1LKld5tBYwoAQLgQOAFAeBE4hULbtt7tggWUJAcARASBEwCEF4FTKNSoYVa7ttdqzZ4d6b0BACQgAicACC8Cp1BnnWbOjPSeAAASOHCiOAQAhAeBU6iccYZ3+9VXkd4TAEAC8qvqkXECgPAgcApldz2hsh4AIALoqgcA4UXgFCoVK3q3GzdGek8AAAmIwAkAwovAKVT+mRjRdu3yFgAAchGBEwCEF4FTqBx1lFmRIt7PZJ0AALkoKcksEPB+JnACgPAgcAqVPHn+zTpt2BDpvQEAJGC2SaiqBwDhQeAUSn7gRMYJABChwImMEwCEB4FTKFEgAgAQAQROABB+BE6hRMYJABABBE4AEH4ETqFE4AQAiAACJwAIPwKnUKI4BAAgAgicACD8CJxCiYwTACACDh78t8BrXlp2AAgLTq+hRHEIAEAEMPktAIQfgVMokXECAEQAgRMAhB+BUyhVquS1Wjt3ml17bcpO5wAAhAmBEwCEH4FTKB11lNkjj3idzMeNM/vww0jvEQAgARA4AUD4ETiF2l13mXXt6v28dGmk9wYAkECBU/78kd4TAIhfBE7hULOmd7tqVaT3BACQQFX1yDgBQPgQOIVD9ereLYETACAX0FUPAMKPwCkcatTwbleujPSeAAASAIETAIQfgVM4kHECAOQiAicACD8Cp3AGTlu3mm3fHum9AQDEOQInAAg/AqdwKF7crHRp72eyTgCAXCoOQVU9AAgfAqdwobseACCXkHECgAQJnEaPHm01a9a0woULW8uWLW3BggXpbjt+/HjLkydPikXPizoUiAAA5BICJwBIgMBp4sSJ1r9/fxsyZIgtXrzYGjdubB06dLBNmzal+5wSJUrY+vXrk5eV0Ric+BmnG280mzMn0nsDAIhjBE4AkACB04gRI6xXr17Ws2dPa9CggY0ZM8aKFi1qY8eOTfc5yjJVrFgxealQoYJFnUaN/v35sssiuScAgDhH4AQAcR447d+/3xYtWmTt2rX7d4fy5nX3582bl+7zdu7caTVq1LBq1arZhRdeaD/++GO62+7bt8+2b9+eYskV115r9sYbivLM1q83O0IGDQCAUAROFIcAgDgNnLZs2WKHDh06LGOk+xs2bEjzOfXq1XPZqPfee89ee+01S0pKstatW9uaNWvS3H7YsGFWsmTJ5EXBVq4oUMCsWzez2rW9+z/8kDvvCwBI2Kp6ZJwAII676mVVq1atrHv37takSRM744wz7N1337Wjjz7a/ve//6W5/YABA2zbtm3Jy+rVqyPTZe8IWTEAAHKCrnoAEOeBU7ly5Sxfvny2cePGFOt1X2OXMqNAgQLWtGlTW7ZsWZqPFypUyBWTCF5yVcOG3i2BEwAgTAicACDOA6eCBQtas2bNbMaMGcnr1PVO95VZygx19fv++++tUqVKFpX8jBNd9QAAYULgBADhF/FhpCpF3qNHD2vevLm1aNHCRo0aZbt27XJV9kTd8qpUqeLGKsnQoUPt5JNPtjp16tjWrVvt8ccfd+XIr7/+eotKwRmnQMArFgEAQAgROAFAAgROl19+uW3evNkGDx7sCkJo7NLUqVOTC0asWrXKVdrz/f333658ubYtXbq0y1jNnTvXlTKPSvXqeS3Z1q1ma9eaVa0a6T0CAMRpcQiq6gFA+OQJBJQGSRwqR67qeioUkWvjnZo2NVuyxOyVV8yuvjp33hMAolBEzsEJcFzGjfNmwTjvPLOPPgrLLgKAJfr5N+aq6sWk88/3bt9/P9J7AgCIQ3TVA4DwI3DKDRdc4N1OnaoZeSO9NwCAOEPgBADhR+CUG5o1M1PVv507zWbOjPTeAADiDIETAIQfgVNuUHGLiy7yfh440Gz//kjvEQAgjhA4AUD4ETjllvvuMytb1isS8eijkd4bAEAcoaoeAIQfgVNuqVjRbNQo7+cXXvDmdAIAIATIOAFA+BE45aaLLzYrXNhszRqzn3+O9N4AAOIEgRMAhB+BU24qUsTs9NO9nz/5JNJ7AwCIEwROABB+BE65rUMH75bACQAQIgROABB+BE6RCpw+/9xs69ZI7w0AII4CJ4pDAED4EDjltgYNzBo1Mtu71+zZZyO9NwCAOKqqR8YJAMKHwCm35cnjzeUkI0d6k+ICAKLK6NGjrWbNmla4cGFr2bKlLViwIN1tX3jhBTvttNOsdOnSbmnXrt0Rtw8HuuoBQPgROEXCZZeZ1a1r9tdfZr16UZocAKLIxIkTrX///jZkyBBbvHixNW7c2Dp06GCbNm1Kc/tZs2ZZt27d7LPPPrN58+ZZtWrVrH379rZ27dpc22cCJwAIPwKnSFDLprmc1Bl9wgSzoUMjvUcAgH+MGDHCevXqZT179rQGDRrYmDFjrGjRojZ27Ng0t3/99dftxhtvtCZNmlj9+vXtxRdftKSkJJsxY0au7TOBEwCEH4FTpJxxhtlzz3k/33+/F0ABACJq//79tmjRItfdzpc3b153X9mkzNi9e7cdOHDAypQpk+42+/bts+3bt6dYcoLACQDCj8Apkq6/3qx/f+/nq682e+ONSO8RACS0LVu22KFDh6xChQop1uv+hg0bMvUad999t1WuXDlF8JXasGHDrGTJksmLuveFojgEVfUAIHwInCJt+HCzq67yWj3dvvJKpPcIAJBNjz76qE2YMMEmTZrkCkukZ8CAAbZt27bkZfXq1Tl6XzJOABB+XJuKNLVyL79sVqKE2X//a9azp1mhQmaXXx7pPQOAhFOuXDnLly+fbdy4McV63a9YseIRn/vEE0+4wGn69Ol2wgknHHHbQoUKuSVUCJwAIPzIOEWDvHnNnnnGq7CXlGR25ZUq6xTpvQKAhFOwYEFr1qxZisIOfqGHVq1apfu84cOH24MPPmhTp0615s2bW24jcAKA8CNwiqbgacwYb6yTWsCuXc1uvVUjlSO9ZwCQUFSKXHMzvfzyy/bzzz9bnz59bNeuXa7KnnTv3t11tfM99thjNmjQIFd1T3M/aSyUlp25OE8fgRMAhB9d9aIteFK5Ww1KfuIJs6eeMlMVJ2WfataM9N4BQEK4/PLLbfPmzTZ48GAXAKnMuDJJfsGIVatWuUp7vueee85V47vkkktSvI7mgbpfVVNzMXCiOAQAhA+n2GijVu/xx81OP92sRw8zzT7ftKkXPLVvH+m9A4CE0LdvX7ekN+FtsBUrVlik+VX1yDgBQPjQVS9adepk9s03Zi1bmm3d6t2fMiXSewUAiEJ01QOA8CNwimY1apjNnm120UXeWKfOnc0++CDSewUAiDIETgAQfgRO0a5gQbMJE8wuvdTswAEveLrmGvUNifSeAQCiBIETAIQfgVMsKFDA7I03zK691itXrnmfjj3WrF8/s82bI713AIAII3ACgPAjcIqlohEvvWT21Vdmbdt62aennzY75hizRx7x7gMAEhJV9QAg/AicYk2LFmbTp5t9+qlXbW/HDrN77zU791yzr79m3icASEBU1QOA8CNwilVnn222cKHZK6+YFStmplnuFVRVr67JQ7z7/iVIAEBco6seAIQfgVMs0wSMV19tNneu2XnnmZUpY7Zxo9nQoWbt2pmdeabZ0qWR3ksAQJgROAFA+BE4xYMTTjD76COzDRu8DNTll5sddZTZnDlmjRqZ/ec/XnYqEIj0ngIAwoDACQDCj8Ap3qrvKQOl8uXff2/WsaPX8f35581OOsns+OPNHn/cbNOmSO8pACCECJwAIPwInOJVzZpmH35oNmuWWbduZoULm/34o9ldd5nVqWM2bJjZL7+QhQKAOCoOQVU9AAgfTrHx7owzvGXbNrO33jJ77jmzb74xGzjQWypVMrvoIrOuXc0aNPDGSQEAYgoZJ0RCIBCwgwcP2iGKUSHKFShQwPKF4ARJ4JQoSpY069XL7LrrzMaPN3v9dbMvvzRbv95s9GhvyZPHC7KUoVIwVa5cpPcaAJAJBE7Ibfv377f169fb7t27I70rQIby5MljVatWtaNUAyAHCJwSsRLftdd6y969Xle+sWO9iXVXrfLua+nTx6x1a7MLLvCWY4/1AisAQNQhcEJuSkpKsuXLl7sr+JUrV7aCBQu6L6ZAtGZGN2/ebGvWrLG6devmKPNE4JTINO7pnHO8RVauNJs40Ssuoe58qsqnReOi1IWvSROzNm3MatUyO/lks2OOifQnAAAQOCEC2SYFT9WqVbOiRYtGeneADB199NG2YsUKO3DgAIETQqRGDS9I0qLsk4pLvP++2cyZZn/95d1q8Slw6tDBWxRQFS8eyb0HAEv0wIniEMhNedWLBYgBocqIcopF2qpXN7vxRm/Zs8ebSFfZpwULzH7//d/b//7XW9RaN2zoFZjwb7UouKIlB4BcqapHxgkAwodvtMhYkSJeNz0tvu3bvbFQn3ziLQqivv3WW4IVLGhWr96/AVX79mYtWjBeCgBCiK56ABB+BE7InhIl/i0cIStWeJPuaq6on37ylp9/NlO1Ha3XIoMHm1Ws6JVBr1DB7IQTvIl5leGqVs2sShUv2AIAZBqBExA5NWvWtFtvvdUtmTFr1ixr06aN/f3331aqVKmw7x9Ch8AJoZtwV0unTv+uS0ryCk4oiFJAtWiR2XvvmW3Y4C0ydWrK11EmShmpCy/0Kvlp/qn69b1Aq1Ah75ZsFQCkQOAE5Hycy5AhQ+z+++/P8ut+/fXXVqxYsUxv37p1a1fKvaSmiskl9evXd5UQV65caRV1ARvZQuCE8NGgUVXg09Kxo7dOgdBvv5lt2mS2Zo3ZwoVeNz8Vo1i92mzfPq80upa0aG4pdRls2tS7VYbqjz/MKlf2MlcEVgASEIETkDEFK76JEyfa4MGDbanGcP8jeI4flbDWxL75MzFOWxXbskLl23MzeJkzZ47t2bPHLrnkEnv55Zft7rvvtkg6cOCAm5A2FlEOBblLV1eaNzc77zyz3r3Nnn/ebMYML5hSEQplqJ55xuzSS81atjQ7+2yvS5/KneobwZYtZtOnmz3+uNmVV5qdeaY3J5VKqiuIUmCldX37mg0caKaTg64e6T0UpKn74NatOiNG+kgAQMiLQ1CLB5GiZnXXrsgsmW3SFaz4i7I9ykD593/55RcrXry4ffzxx9asWTMrVKiQCzh+//13u/DCC61ChQousDrppJNsur6HpOqqN2rUqOT7et0XX3zRunTp4sq1a+6g91WlOKirnrbZqu8jZjZ+/HjXZe+TTz6x4447zr3POeeckyLQO3jwoN1yyy1uu7Jly7rgp0ePHta5c+cMP/dLL71kV1xxhV199dU2VnN3pqL5jbp162ZlypRxmbPmzZvbV0EXsD/44AP3uQsXLmzlypVznyv4s06ePDnF62kf9ZlEJcC1jQLVM844w73G66+/bn/++ad7zypVqrhjdPzxx9ubb76Z4nVU8n748OFWp04d9/uoXr26Pfzww+6xs846y/rqu14QzdWkoHSGvvOFCadYRA9lijTWSf8RUv1ncDRh7w8/mC1Z4s0zpVudVJTRWrvWC75UNv3zz70lo4IXyk5pUbaqalWva6CCM81ZpWqApUt7gZ7Gc1FyFUAUI+OESNOQ5qCETa7audMsCz3ljuiee+6xJ554wmrXrm2lS5e21atX23nnnee+sOvL+yuvvGKdOnVymSp9kU/PAw884L70P/744/bMM8/YlVde6brJKThJy+7du937vvrqq67M+1VXXWV33HGHCzLksccecz+PGzfOBVdPPfWUC1g0VupIduzYYW+//bYLhNRdb9u2bfbFF1/Yaaed5h7fuXOnC2gUwCi4UxC5ePFiF7TIRx995AKle++91312zeE1ZcqUbB3XJ5980po2beqCp71797oAVQFgiRIl3PsosDvmmGOshYZsmNmAAQPshRdesJEjR9qpp57qAkkFuHL99de7wEmvqd+LvPbaa+5zKKgKFwInxNaEvcpWaUmLAitllFSIQuOqdF/fIjZu9Eqp65vFjh1ed0Flt9TFT0tmAjrNUaUBnAqkdKsqgSee6AVqOltrnb/opFi3LkUuAOQaAicgNIYOHWpnq7fLPxToNG7cOPn+gw8+aJMmTXJBRuqMR7BrrrnGZVTkkUcesaefftoWLFjgMknpdV8bM2aMCxxEr6198Sn4UiDhZ3ueffbZTAUwEyZMcBmvhqpsbGZdu3Z1GSg/cHrjjTdcpkbjtPygThkenwJGPUeBoC/4eGSWCmdcdNFFKdYpMPTdfPPNLuP21ltvucBJAZ+CQ31OZdZEx0YBlOi1dIzee+89u+yyy9w6Zbl03EM1Z1NaCJwQX4GVxj5pyeiymDJVwYuqAi5b5j2uwhXqMqgU+v79Xh8AlV/X4vvii4z3xc9U1a7tXQ5TMKUTl35WUKWuhX6/Gj2mgC44E6Zuh+p/o234NgTgCAicEGnqUa/mLVLvHSrqphZMGRkVjFBGRBkPdZnTeKFVGpt9BCeoavA/1P1NWZVNGt+dDnVX84MmqVSpUvL2yhJt3LgxORMj+fLlcxkbPzOUHnXNU/bKp5+VYVIgpq6JS5YscVmg9DJherxXr14W6uN66NAhF1AqUFq7dq3LZO3bt88dB/n555/d/bZt26b5espa+V0PFTgpS/bDDz+k6BIZDgROSDz6T6mTU9AJKl3KWimgURDl3yrLNHu2F1xpQKgCMa33F53otK2eK34Fwe++M/u//8v6/ipw0vtovxWQKbhSlkuBmRZdWVGa2u9WqAGXKu1evryXWdPJUM/XcxTI6ZuVXlPr+ZYFxAUCJ0SamqJQdZeLpNTV8ZQVmTZtmutGp0xMkSJFXJEFfdE/ktTFD5QFOVKQk9b2KlCREz/99JPNnz/fZbqCC0IoaFEmSgGRPs+RZPR4njT2U9mzjI6rujAqo6SxYRrfpMeVlfKPa0bv63fXa9KkiRujpS6M6qJXo0YNCycCJ+BIFKhoUYGKYF27pv8cnUBUKVABlYKn5cu9jt8LFnjdBhXgKKBRFUG1NKokqGyXAhltrwyYgi//RKSsU9AA0ZBRsKXgSidyLfrGVbasl+nSfigI030FWdpfdVf0g7XgRZ9Tz9fPf/7pvY6CPC06Uer1dBnSf019fr2mbjWOTN/46NYI5AiBExAeX375pev+5XeRUwZKBQ9ykwpZqDiFutOdfvrpycGPsiwKHNKjLnnafvTo0SnWK8jQYwqclBlTIYu//vorzayTHlexhZ49e6ZbUTC4iMVvv/3mxmtl5riq6IafDVNQ+euvv1oDDYVwHXHquuBJ760AKS0KuJTJ0jgodTlUt75wi4rASb9QRZ4bNmxw/SaVPgxOR6amQW6DBg1yf7g6sBowp4F7QFRQMBTUP9jNSyV+SfbMULCkzJaCG43LUiClwEMBjG4ViCirpceCM2O67wdi6lqo5//9t0rNeM/VtystCsq0nYK3YCoRHwkq0KHgSfuvz645wXRfV62CF2XO1PVRtyoI4l+hUyBWr54XwOkqlQJVPyDU59Wtfi9+MKcgUAGx3kuPaXuNT9M2OtZ6XT/40+vpOKqio7bXtn72TtvrvbWdgsJgOsZ6XJ9JP6d+HAgR/Xn5gRNV9YDQ0vfMd9991xWEUHZF3z8z6h4XDhoDNGzYMJf1UpEHfVfWBLrpjedR1keFJjROqlGjRikeUyAyYsQI+/HHH904LHWZU3U+vb66CH7zzTdWuXJla9WqlZvbSt3l1I2wa9eurquixlb5GSxleRSwaFsFc1qfmVLjOq7vvPOOzZ071xXh0P6oO6IfOKkrnl7rrrvucpXyTjnlFDcWS/t83XXXpfgsGuukjFVwtb9wifgpVuUJ+/fv7wbEtWzZ0qXsOnTo4KqVlNfV8FR0gPVL1i/3/PPPdxGmftmKulP/YQAxS99+/L9/ZWZU9S+U9C1LXQ39DJGCAK1TRkwBm4IFBV5+sKD7CtYUBPjjvfxFAYgfIChDpQZFV5sUqGmdXkOBhoILBXd6jl5TNWR969al3L9vv017v5W9U+YuGun3pGBUn1+/Px0DHRsFtcHBtN+FU4GhAjIt/s/6BqzATZk4BWh6PS3quqCGSMdRi7b1s6Far+OpLqEKLvVaqRdto99z6kW/d38Mnd+FMzOLurke4SonclfwdzgyTkBo6Qv9tdde6yatVSlufZnfHjzmOZfofZVg6N69uxvf1Lt3b/d9WT+nRWN9VPI7rWBCVfm0KOukz/fpp5/a7bff7pIQCowUvPhZqjPPPNMlLFQU49FHH3Vjtfysl6iqnbJRKjahYEvd7xYtWpTh57nvvvvsjz/+cJ9B45r0efR9XuO5fApSNY+W5ttat26dC+puuOGGFK+jmEBd/HSrYCvc8gRy2oEyhxQsqTa8n15TFF+tWjUXWat0YWqXX3657dq1yz788MPkdSeffLJLVSr4yoj+2JXy1C9Gv3wAEaJgQI2PTkGqbuh391OQoQBJ9/WFP/iLvwI7dYNUMKZgUsGetlcQpnL02lY/K2jzAwN/XJe2VTCjx/3MnIIAPV9BocanKQDRpIT+tv6iYEdBoQIWbavF/7aq5yTavGA33qiuAtl6Kufg0B8X/Vf6pxqvSzAreQqEk0pJL1++3GrVqpUrX1ZxOH1fVvCjwggKahLVihUrXDZM3RhPVLXjbPzNZuX8G9GMkwaAKSpVeUWfate3a9fO5s2bl+ZztF4ZqmCKVlNPvuVTRQ4tvkhcJQCQBgU5Gv8kqWdd/ydVHzMUjCmDp8ycgjEFU8oMab0/du3HH73P7J+w9W03OKOkWwVgOhbK+uk5fibK77ao1/MDOW2vWz1XwaECPr227qe1+N00Uy9+d0bts3+b0RLcFRVRQZWF9evLRA8ZADFIc0ApM6SKePpeq4SDAgFNbJuIDhw44DJqylwpgXKkoCmUIho4bdmyxfWH1IC3YLrvT3CVmtKUaW2v9WlRl77g2vMAEHIKko4U7KkbX+oCI0CIKK5WoU8A8UuJBc1TpCp/6iym4SnTp093WadE9OWXX7rJf4899lg3Viq3RHyMU7gpmxWcoVLGSV0BAQAAgFig764KFmDJY68iMdooooGTBtlpUJuqaATT/YrqdpIGrc/K9oUKFXILAAAAAGRXXosglRfUrMeq0R482E33VdYwLVofvL1oYrL0tgcAAEDoRbi+GJDrf6sR76qnbnQ9evRwE1hp7iaVI1fVPH+iLZVdrFKlihurJP369XMD41T+sGPHjm7m44ULF9rzzz8f4U8CAAAQ//x5ejTRqSYpBaKdCtJJeuXbYyZwUnlxTWilGu0q8KCy4lOnTk0uALFq1So3IM6nOvqau0lVNAYOHOgm0FJFPeZwAgAACD99+SxVqpRt0rxzpmn6iqY7ESsQaerNplhDf6eaFyqm53HKbcwhAgCRwzk4bRwXxBp9fdQF762aAw+IckrCaA4nDRNKLWbmcQIAAEDsUYapUqVKVr58eTenDhDNFDAF92DLLgInAAAAZLvbXk7HjQCxIqJV9QAAAAAgFhA4AQAAAEAGCJwAAAAAIAMJN8bJLyKoChoAgNzln3sTrKBrhmibACD626WEC5x27NjhbqtVqxbpXQGAhKVzscq/wkPbBADR3y4l3DxOmgRr3bp1Vrx48WxN1qaoVA3b6tWrE3aujUQ/Bnz+xP78kujHICefX02OGqfKlSuHpDRsvKBtyplE//yS6MeAz5/Ynz8nxyAr7VLCZZx0QKpWrZrj19EvJFH/MH2Jfgz4/In9+SXRj0F2Pz+ZpsPRNoVGon9+SfRjwOdP7M+f3WOQ2XaJy30AAAAAkAECJwAAAADIAIFTFhUqVMiGDBnibhNVoh8DPn9if35J9GOQ6J8/GiX67yTRP78k+jHg8yf258+tY5BwxSEAAAAAIKvIOAEAAABABgicAAAAACADBE4AAAAAkAECJwAAAADIAIFTFo0ePdpq1qxphQsXtpYtW9qCBQssHt1///1u9vrgpX79+smP792712666SYrW7asHXXUUXbxxRfbxo0bLVbNnj3bOnXq5GaN1medPHlyisdVQ2Xw4MFWqVIlK1KkiLVr185+++23FNv89ddfduWVV7pJ10qVKmXXXXed7dy50+LlGFxzzTWH/U2cc845cXEMhg0bZieddJIVL17cypcvb507d7alS5em2CYzf/OrVq2yjh07WtGiRd3r3HnnnXbw4EGLBZk5BmeeeeZhfwM33HBD3ByDWEbb5KFtiq+2KZHbJUn0tmlYFLZLBE5ZMHHiROvfv78rdbh48WJr3LixdejQwTZt2mTxqGHDhrZ+/frkZc6cOcmP3XbbbfbBBx/Y22+/bZ9//rmtW7fOLrroIotVu3btcr9PfflIy/Dhw+3pp5+2MWPG2FdffWXFihVzv3udsHw6Mf/44482bdo0+/DDD90Jv3fv3hYvx0DUIAX/Tbz55pspHo/VY6C/YTU88+fPd/t+4MABa9++vTsmmf2bP3TokDsx79+/3+bOnWsvv/yyjR8/3n2piQWZOQbSq1evFH8D+r8RL8cgVtE20TbFa9uUyO2SJHrb9Hk0tksqR47MadGiReCmm25Kvn/o0KFA5cqVA8OGDQvEmyFDhgQaN26c5mNbt24NFChQIPD2228nr/v5559V1j4wb968QKzT55g0aVLy/aSkpEDFihUDjz/+eIpjUKhQocCbb77p7v/000/ueV9//XXyNh9//HEgT548gbVr1wZi/RhIjx49AhdeeGG6z4mnY7Bp0yb3WT7//PNM/81PmTIlkDdv3sCGDRuSt3nuuecCJUqUCOzbty8Qa1IfAznjjDMC/fr1S/c58XYMYgVtk4e2Kb7bpkRvlyTR26ZNUdAukXHKJEWqixYtcmlwX968ed39efPmWTxSul/p8dq1a7srNkp1io6Dov7gY6GuEtWrV4/LY7F8+XLbsGFDis9bsmRJ1x3G/7y6VReA5s2bJ2+j7fU3oquA8WLWrFkuzV2vXj3r06eP/fnnn8mPxdMx2LZtm7stU6ZMpv/mdXv88cdbhQoVkrfRld/t27e7q52xJvUx8L3++utWrlw5a9SokQ0YMMB2796d/Fi8HYNYQNtE25TobVOitEuS6G3Ttihol/Ln6BMkkC1btrh0X/CBF93/5ZdfLN7oxKtUpk5ESns+8MADdtppp9kPP/zgTtQFCxZ0J6PUx0KPxRv/M6X1u/cf061O3MHy58/v/nPHyzFRdwil/2vVqmW///67DRw40M4991x3UsqXL1/cHIOkpCS79dZb7ZRTTnEnYcnM37xu0/ob8R+LJWkdA7niiiusRo0a7kvrd999Z3fffbfrb/7uu+/G3TGIFbRNtE2J3DYlSrskid42JUVJu0TghDTpxOM74YQTXGOlP8y33nrLDUBF4unatWvyz7p6o7+LY445xl3ta9u2rcUL9afWl7DgcROJJr1jEDwuQH8DGpCu372+sOhvAQg32iYkYrskid423RQl7RJd9TJJKUBdvUhdqUT3K1asaPFOVzOOPfZYW7Zsmfu86h6ydevWhDgW/mc60u9et6kHYqtii6r5xOMxEXWT0f8L/U3EyzHo27evGzz82WefWdWqVZPXZ+ZvXrdp/Y34j8WK9I5BWvSlVYL/BuLhGMQS2ibaJtqm+G6XJNHbpr5R1C4ROGWSUqHNmjWzGTNmpEgb6n6rVq0s3ql0p6J3RfI6DgUKFEhxLJQWVT/zeDwW6gKg/1zBn1d9Y9U/2v+8utWJS/2NfTNnznR/I/5/4nizZs0a15dcfxOxfgw07lgn5kmTJrl91u88WGb+5nX7/fffp2ikVQVIJXAbNGhg0S6jY5CWJUuWuNvgv4FYPgaxiLaJtom2KT7bJUn0tikQje1SNopaJKwJEya4ajXjx493lVp69+4dKFWqVIpKHfHi9ttvD8yaNSuwfPnywJdffhlo165doFy5cq6iidxwww2B6tWrB2bOnBlYuHBhoFWrVm6JVTt27Ah88803btF/ixEjRrifV65c6R5/9NFH3e/6vffeC3z33Xeuik+tWrUCe/bsSX6Nc845J9C0adPAV199FZgzZ06gbt26gW7dugXi4RjosTvuuMNV6dHfxPTp0wMnnnii+4x79+6N+WPQp0+fQMmSJd3f/Pr165OX3bt3J2+T0d/8wYMHA40aNQq0b98+sGTJksDUqVMDRx99dGDAgAGBWJDRMVi2bFlg6NCh7rPrb0D/F2rXrh04/fTT4+YYxCraJtqmeG2bErldkkRvm/pEYbtE4JRFzzzzjPsDLViwoCsBO3/+/EA8uvzyywOVKlVyn7NKlSruvv5AfTop33jjjYHSpUsHihYtGujSpYv7Y45Vn332mTspp15U6tQv+zpo0KBAhQoV3BeUtm3bBpYuXZriNf788093Mj7qqKNcmcuePXu6E3s8HAOdpHTS0clGpU9r1KgR6NWr12FfzGL1GKT1ubWMGzcuS3/zK1asCJx77rmBIkWKuC9z+pJ34MCBQCzI6BisWrXKNUZlypRx/wfq1KkTuPPOOwPbtm2Lm2MQy2ibPLRN8dU2JXK7JIneNlkUtkt5/tkxAAAAAEA6GOMEAAAAABkgcAIAAACADBA4AQAAAEAGCJwAAAAAIAMETgAAAACQAQInAAAAAMgAgRMAAAAAZIDACQAAAAAyQOAExKk8efLY5MmTI70bAAAko21CLCNwAsLgmmuucY1D6uWcc86J9K4BABIUbROQM/lz+HwA6VBDNG7cuBTrChUqFLH9AQCAtgnIPjJOQJioIapYsWKKpXTp0u4xXeF77rnn7Nxzz7UiRYpY7dq17Z133knx/O+//97OOuss93jZsmWtd+/etnPnzhTbjB071ho2bOjeq1KlSta3b98Uj2/ZssW6dOliRYsWtbp169r777+f/Njff/9tV155pR199NHuPfR46sYUABBfaJuA7CNwAiJk0KBBdvHFF9u3337rGomuXbvazz//7B7btWuXdejQwTVmX3/9tb399ts2ffr0FI2PGrebbrrJNVpqyNTw1KlTJ8V7PPDAA3bZZZfZd999Z+edd557n7/++iv5/X/66Sf7+OOP3fvq9cqVK5fLRwEAEE1om4AjCAAIuR49egTy5csXKFasWIrl4Ycfdo/rv94NN9yQ4jktW7YM9OnTx/38/PPPB0qXLh3YuXNn8uMfffRRIG/evIENGza4+5UrVw7ce++96e6D3uO+++5Lvq/X0rqPP/7Y3e/UqVOgZ8+eIf7kAIBoRdsE5AxjnIAwadOmjbtSFqxMmTLJP7dq1SrFY7q/ZMkS97OusjVu3NiKFSuW/Pgpp5xiSUlJtnTpUtedYt26dda2bdsj7sMJJ5yQ/LNeq0SJErZp0yZ3v0+fPu6q4uLFi619+/bWuXNna926dQ4/NQAgmtE2AdlH4ASEiRqD1N0TQkX9vjOjQIECKe6rUVMDJ+rDvnLlSpsyZYpNmzbNNXTqXvHEE0+EZZ8BAJFH2wRkH2OcgAiZP3/+YfePO+4497Nu1b9c/cl9X375peXNm9fq1atnxYsXt5o1a9qMGTNytA8afNujRw977bXXbNSoUfb888/n6PUAALGNtglIHxknIEz27dtnGzZsSLEuf/78yYNcNai2efPmduqpp9rrr79uCxYssJdeesk9poGyQ4YMcQ3H/fffb5s3b7abb77Zrr76aqtQoYLbRutvuOEGK1++vLtCt2PHDteAabvMGDx4sDVr1sxVPtK+fvjhh8mNIwAgPtE2AdlH4ASEydSpU10Z1mC6IvfLL78kVxWaMGGC3XjjjW67N9980xo0aOAeU4nWTz75xPr162cnnXSSu68+3yNGjEh+LTVce/futZEjR9odd9zhGr1LLrkk0/tXsGBBGzBggK1YscJ1rzjttNPc/gAA4hdtE5B9eVQhIgfPB5AN6s89adIkN+gVAIBoQNsEHBljnAAAAAAgAwROAAAAAJABuuoBAAAAQAbIOAEAAABABgicAAAAACADBE4AAAAAkAECJwAAAADIAIETAAAAAGSAwAkAAAAAMkDgBAAAAAAZIHACAAAAADuy/wdxkeR+ew39sgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # Ensure NumPy is imported\n",
    "\n",
    "# Convert CuPy arrays to NumPy before plotting\n",
    "train_losses_np = np.array([loss.get() for loss in train_losses])\n",
    "train_accuracies_np = np.array([acc.get() for acc in train_accuracies])\n",
    "\n",
    "# Plot loss curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses_np, label=\"Training Loss\", color='r')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies_np, label=\"Training Accuracy\", color='b')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Curve\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
